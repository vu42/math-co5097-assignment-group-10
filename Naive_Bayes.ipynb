{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Problem Statement\n",
    "\n",
    "In real-world scenarios, particularly in Computer Science and Machine Learning, we often encounter the need to classify data points into predefined categories based on their features. Probabilistic classification methods provide a mathematical framework to handle uncertainty inherent in these problems. Among them, the **Naive Bayes classifier** stands out due to its simplicity, interpretability, and efficiency [1][3].\n",
    "\n",
    "The core idea behind Naive Bayes is using probability theory, particularly **Bayes' theorem**, to calculate the probability of each class given the observed features, and then selecting the most probable class.\n",
    "\n",
    "### 1.2. Applications in Computer Science\n",
    "\n",
    "Naive Bayes has been successfully applied to numerous practical problems in computer science, including:\n",
    "\n",
    "- **Spam Email Filtering:** Classifying emails as \"spam\" or \"not spam\" based on the occurrence of certain words [3].\n",
    "- **Document Categorization:** Classifying text documents (e.g., news articles, academic papers) into various topics.\n",
    "- **Sentiment Analysis:** Determining sentiment (positive, negative, neutral) from product reviews or social media data.\n",
    "- **Optical Character Recognition (OCR):** Identifying handwritten characters from image data.\n",
    "- **Medical Diagnosis:** Classifying patients as healthy or having a disease based on symptoms or medical test results.\n",
    "\n",
    "### 1.3. Computer Science Problems Solved by Naive Bayes\n",
    "\n",
    "Naive Bayes has notably solved or significantly simplified various computer science challenges, such as:\n",
    "\n",
    "- **Text Classification (NLP):**\n",
    "  - Predicting the topic or sentiment of texts.\n",
    "- **Image Classification:**\n",
    "  - Recognizing digits or letters from handwritten documents (such as the famous MNIST dataset).\n",
    "- **Recommender Systems:**\n",
    "  - Predicting user preferences based on their historical interactions.\n",
    "\n",
    "### 1.4. Mathematical Foundation\n",
    "\n",
    "Naive Bayes leverages two critical mathematical concepts:\n",
    "\n",
    "- **Bayes' Theorem**, which allows the computation of conditional probabilities [1][2]:\n",
    "\n",
    "$$\n",
    "P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y)P(y)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "- The assumption of **conditional independence** among features, given a class $y$, which significantly simplifies calculations [1][3]:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\ldots, x_d|y) = \\prod_{i=1}^{d} P(x_i|y)\n",
    "$$\n",
    "\n",
    "These assumptions enable the Naive Bayes classifier to efficiently estimate class probabilities even with limited training data, making it especially powerful for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Foundations & Implementations\n",
    "\n",
    "This section presents the core mathematical formulas underlying the Naive Bayes classifier, organized from fundamental probability concepts to practical implementations. Each formula includes:\n",
    "- **Detailed Explanation:** Intuitive understanding of the concept\n",
    "- **Detailed Derivation:** Mathematical proof and reasoning\n",
    "- **Concrete Example:** Real-world application with numerical calculations\n",
    "- **Python Implementation:** Verification through code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. The Probabilistic Model for Classification\n",
    "\n",
    "In a classification task, given an input data example (denoted by the vector $\\mathbf{x}$), our goal is to assign it to one of the predefined classes (labels) $y$. The mathematical representation for classification, using probability theory, is to find the most likely label $y$ [1], given the observed features $\\mathbf{x}$:\n",
    "\n",
    "#### Formula 22.9.1:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y P(y \\mid \\mathbf{x})\n",
    "$$\n",
    "\n",
    "### Derivation & Explanation:\n",
    "\n",
    "- To classify a data point, we want to maximize the conditional probability $P(y \\mid \\mathbf{x})$, which represents the probability of a class $y$ given features $\\mathbf{x}$.\n",
    "- According to Bayes' theorem, this conditional probability can be rewritten as:\n",
    "\n",
    "$$\n",
    "P(y \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid y)P(y)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "- Hence, the optimal class can be found by maximizing:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y \\frac{P(\\mathbf{x} \\mid y)P(y)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "- Since $P(\\mathbf{x})$ (the denominator) is the same for every class, it does not affect the maximization. Therefore, we simplify the formula by ignoring this denominator, yielding:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y P(\\mathbf{x} \\mid y)P(y)\n",
    "$$\n",
    "\n",
    "This simplification significantly reduces the computational complexity and serves as the basis for the Naive Bayes classifier.\n",
    "\n",
    "### Concrete Example (detailed calculation):\n",
    "\n",
    "Suppose we have an email, and we want to classify it as \"Spam\" (S) or \"Not Spam\" (NS) based on a single keyword \"lottery\". We have:\n",
    "\n",
    "- $P(S) = 0.2$, $P(NS) = 0.8$\n",
    "- The word \"lottery\" occurs with:\n",
    "  - $P(\\text{\"lottery\"}|S) = 0.7$\n",
    "  - $P(\\text{\"lottery\"}|NS) = 0.05$\n",
    "\n",
    "Using the simplified formula:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y P(y)P(\\mathbf{x}|y)\n",
    "$$\n",
    "\n",
    "We compute probabilities for both classes clearly:\n",
    "\n",
    "- For class Spam (S):\n",
    "\n",
    "$$\n",
    "P(S)P(\\text{\"lottery\"}|S) = 0.2 \\times 0.7 = 0.14\n",
    "$$\n",
    "\n",
    "- For class Not Spam (NS):\n",
    "\n",
    "$$\n",
    "P(NS)P(\\text{\"lottery\"}|NS) = 0.8 \\times 0.05 = 0.04\n",
    "$$\n",
    "\n",
    "Thus, clearly, we see:\n",
    "\n",
    "$$\n",
    "0.14 > 0.04 \\quad \\Rightarrow \\quad \\hat{y} = \\text{Spam}\n",
    "$$\n",
    "\n",
    "Hence, the email is classified as Spam.\n",
    "\n",
    "### Python Verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "P_y_torch = torch.tensor([0.2, 0.8])\n",
    "P_x_given_y_torch = torch.tensor([0.7, 0.05])\n",
    "\n",
    "posteriors_torch = P_y_torch * P_x_given_y_torch\n",
    "\n",
    "log_posteriors = torch.log(posteriors_torch)\n",
    "normalized_posteriors = F.softmax(log_posteriors, dim=0)\n",
    "\n",
    "predicted_idx = torch.argmax(posteriors_torch)\n",
    "classes = ['Spam', 'Not Spam']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PyTorch Implementation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Unnormalized posteriors: {posteriors_torch}\")\n",
    "print(f\"Normalized posteriors: {normalized_posteriors}\")\n",
    "print(f\"Predicted: {classes[predicted_idx]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "P_y_tf = tf.constant([0.2, 0.8])\n",
    "P_x_given_y_tf = tf.constant([0.7, 0.05])\n",
    "\n",
    "posteriors_tf = P_y_tf * P_x_given_y_tf\n",
    "\n",
    "log_posteriors = tf.math.log(posteriors_tf)\n",
    "normalized_posteriors = tf.nn.softmax(log_posteriors)\n",
    "\n",
    "predicted_idx_tf = tf.argmax(posteriors_tf)\n",
    "classes = ['Spam', 'Not Spam']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TensorFlow Implementation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Unnormalized posteriors: {posteriors_tf.numpy()}\")\n",
    "print(f\"Normalized posteriors: {normalized_posteriors.numpy()}\")\n",
    "print(f\"Predicted: {classes[predicted_idx_tf]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np as mx_np, npx\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "P_y_mx = mx_np.array([0.2, 0.8])\n",
    "P_x_given_y_mx = mx_np.array([0.7, 0.05])\n",
    "\n",
    "posteriors_mx = P_y_mx * P_x_given_y_mx\n",
    "\n",
    "posteriors_sum = mx_np.sum(posteriors_mx)\n",
    "normalized_posteriors = posteriors_mx / posteriors_sum\n",
    "\n",
    "predicted_idx_mx = mx_np.argmax(posteriors_mx)\n",
    "classes = ['Spam', 'Not Spam']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MXNet Implementation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Unnormalized posteriors: {posteriors_mx.asnumpy()}\")\n",
    "print(f\"Normalized posteriors: {normalized_posteriors.asnumpy()}\")\n",
    "print(f\"Predicted: {classes[int(predicted_idx_mx)]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from mxnet import np as mx_np, npx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "# Common data\n",
    "P_y_data = [0.2, 0.8]\n",
    "P_x_given_y_data = [0.7, 0.05]\n",
    "classes = ['Spam', 'Not Spam']\n",
    "\n",
    "# NumPy\n",
    "P_y_np = np.array(P_y_data)\n",
    "P_x_given_y_np = np.array(P_x_given_y_data)\n",
    "posteriors_np = P_y_np * P_x_given_y_np\n",
    "\n",
    "# PyTorch (already computed in previous cell, recompute for visualization)\n",
    "P_y_torch = torch.tensor(P_y_data)\n",
    "P_x_given_y_torch = torch.tensor(P_x_given_y_data)\n",
    "posteriors_torch = P_y_torch * P_x_given_y_torch\n",
    "\n",
    "# TensorFlow (already computed in previous cell, recompute for visualization)\n",
    "P_y_tf = tf.constant(P_y_data)\n",
    "P_x_given_y_tf = tf.constant(P_x_given_y_data)\n",
    "posteriors_tf = P_y_tf * P_x_given_y_tf\n",
    "\n",
    "# MXNet (already computed in previous cell, recompute for visualization)\n",
    "P_y_mx = mx_np.array(P_y_data)\n",
    "P_x_given_y_mx = mx_np.array(P_x_given_y_data)\n",
    "posteriors_mx = P_y_mx * P_x_given_y_mx\n",
    "\n",
    "# Create 2x2 visualization comparing all frameworks\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "# NumPy\n",
    "axes[0,0].bar(classes, posteriors_np, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,0].set_title('NumPy Implementation', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Unnormalized Posterior', fontsize=10)\n",
    "axes[0,0].set_ylim([0, max(posteriors_np) * 1.3])\n",
    "for i, v in enumerate(posteriors_np):\n",
    "    axes[0,0].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# PyTorch\n",
    "axes[0,1].bar(classes, posteriors_torch.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,1].set_title('PyTorch (torch.tensor)', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Unnormalized Posterior', fontsize=10)\n",
    "axes[0,1].set_ylim([0, max(posteriors_torch.numpy()) * 1.3])\n",
    "for i, v in enumerate(posteriors_torch.numpy()):\n",
    "    axes[0,1].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# TensorFlow\n",
    "axes[1,0].bar(classes, posteriors_tf.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,0].set_title('TensorFlow (tf.constant)', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Unnormalized Posterior', fontsize=10)\n",
    "axes[1,0].set_ylim([0, max(posteriors_tf.numpy()) * 1.3])\n",
    "for i, v in enumerate(posteriors_tf.numpy()):\n",
    "    axes[1,0].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MXNet\n",
    "axes[1,1].bar(classes, posteriors_mx.asnumpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,1].set_title('MXNet (mx.np.array)', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Unnormalized Posterior', fontsize=10)\n",
    "axes[1,1].set_ylim([0, max(posteriors_mx.asnumpy()) * 1.3])\n",
    "for i, v in enumerate(posteriors_mx.asnumpy()):\n",
    "    axes[1,1].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Email Spam Classification: Word \"lottery\" Present (Framework Comparison)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Framework Comparison Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"All frameworks produce identical results:\")\n",
    "print(f\"  Spam: {posteriors_np[0]:.4f}\")\n",
    "print(f\"  Not Spam: {posteriors_np[1]:.4f}\")\n",
    "print(f\"  Prediction: {classes[np.argmax(posteriors_np)]}\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "P_y = {'Spam': 0.2, 'Not Spam': 0.8}\n",
    "P_x_given_y = {'Spam': 0.7, 'Not Spam': 0.05}\n",
    "\n",
    "posteriors_np = {y: P_y[y] * P_x_given_y[y] for y in P_y}\n",
    "predicted_class = max(posteriors_np, key=posteriors_np.get)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Naive Bayes - Email Spam Example\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Feature: Word 'lottery' in email\")\n",
    "print(f\"\\nPriors:\")\n",
    "print(f\"  P(Spam) = {P_y['Spam']}\")\n",
    "print(f\"  P(Not Spam) = {P_y['Not Spam']}\")\n",
    "print(f\"\\nLikelihoods:\")\n",
    "print(f\"  P('lottery'|Spam) = {P_x_given_y['Spam']}\")\n",
    "print(f\"  P('lottery'|Not Spam) = {P_x_given_y['Not Spam']}\")\n",
    "print(f\"\\nPosterior (unnormalized):\")\n",
    "for cls, prob in posteriors_np.items():\n",
    "    print(f\"  {cls}: {prob:.4f}\")\n",
    "print(f\"\\nPredicted: {predicted_class}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Bayes' Theorem for Classification\n",
    "\n",
    "#### Formula 22.9.2: Bayes' Theorem\n",
    "\n",
    "**Original Formula:**\n",
    "\n",
    "$$\n",
    "P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y)P(y)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "### Detailed Derivation:\n",
    "\n",
    "Starting from the basic definition of conditional probability [2], we have explicitly:\n",
    "\n",
    "- Conditional probability of event $y$ given event $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}, y)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "- Similarly, the conditional probability of event $\\mathbf{x}$ given $y$ is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|y) = \\frac{P(\\mathbf{x}, y)}{P(y)}\n",
    "$$\n",
    "\n",
    "- From this second equation, we clearly have:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}, y) = P(\\mathbf{x}|y)P(y)\n",
    "$$\n",
    "\n",
    "- Substitute this expression back into the first equation explicitly, obtaining Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y)P(y)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "This formula has now been explicitly derived from the basic definitions of conditional probabilities.\n",
    "\n",
    "### Concrete Example (detailed calculation):\n",
    "\n",
    "Suppose we have a medical test for a disease. The probabilities are given as follows:\n",
    "\n",
    "- Probability a patient has the disease ($D$): $P(D) = 0.01$\n",
    "- Probability the test is positive if patient has the disease: $P(\\text{Positive}|D) = 0.99$\n",
    "- Probability the test is positive if patient does **not** have the disease: $P(\\text{Positive}|\\neg D) = 0.05$\n",
    "\n",
    "Let's compute clearly the probability a patient actually has the disease, given the test is positive:\n",
    "\n",
    "**Step 1: Compute $P(\\text{Positive})$ explicitly using law of total probability:**\n",
    "\n",
    "$$\n",
    "P(\\text{Positive}) = P(\\text{Positive}|D)P(D) + P(\\text{Positive}|\\neg D)P(\\neg D)\n",
    "$$\n",
    "\n",
    "Substitute numbers clearly:\n",
    "\n",
    "$$\n",
    "= (0.99)(0.01) + (0.05)(0.99) = 0.0099 + 0.0495 = 0.0594\n",
    "$$\n",
    "\n",
    "**Step 2: Apply Bayes' theorem explicitly:**\n",
    "\n",
    "$$\n",
    "P(D|\\text{Positive}) = \\frac{P(\\text{Positive}|D)P(D)}{P(\\text{Positive})} = \\frac{(0.99)(0.01)}{0.0594} = \\frac{0.0099}{0.0594} \\approx 0.1667\n",
    "$$\n",
    "\n",
    "Thus, explicitly, the probability the patient has the disease given a positive test is approximately 16.67%.\n",
    "\n",
    "### Python Implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recalculate for visualization\n",
    "P_D = 0.01\n",
    "P_Pos_given_D = 0.99\n",
    "P_Pos_given_not_D = 0.05\n",
    "\n",
    "P_Pos = P_Pos_given_D * P_D + P_Pos_given_not_D * (1 - P_D)\n",
    "P_D_given_Pos = (P_Pos_given_D * P_D) / P_Pos\n",
    "P_not_D_given_Pos = 1 - P_D_given_Pos\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart showing all probabilities\n",
    "categories = ['P(D)', 'P(Pos|D)', 'P(Pos|Â¬D)', 'P(Pos)', 'P(D|Pos)']\n",
    "values = [P_D, P_Pos_given_D, P_Pos_given_not_D, P_Pos, P_D_given_Pos]\n",
    "colors_bar = ['#E74C3C', '#3498DB', '#95A5A6', '#F39C12', '#2ECC71']\n",
    "\n",
    "axes[0].bar(categories, values, color=colors_bar, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "axes[0].set_title('Medical Test Probabilities', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "for i, v in enumerate(values):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Pie chart showing posterior distribution\n",
    "labels = [f'Disease\\n{P_D_given_Pos:.2%}', f'No Disease\\n{P_not_D_given_Pos:.2%}']\n",
    "sizes = [P_D_given_Pos, P_not_D_given_Pos]\n",
    "colors_pie = ['#E74C3C', '#2ECC71']\n",
    "explode = (0.1, 0)\n",
    "\n",
    "axes[1].pie(sizes, explode=explode, labels=labels, colors=colors_pie,\n",
    "           shadow=True, startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Posterior: P(Disease | Positive Test)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Bayes\\' Theorem: Medical Diagnosis Example', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Bayes' Theorem - Medical Test Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Prior: P(Disease) = {P_D}\")\n",
    "print(f\"Likelihood: P(Positive | Disease) = {P_Pos_given_D}\")\n",
    "print(f\"Likelihood: P(Positive | No Disease) = {P_Pos_given_not_D}\")\n",
    "print(f\"\\nMarginal: P(Positive) = {P_Pos:.4f}\")\n",
    "print(f\"Posterior: P(Disease | Positive) = {P_D_given_Pos:.4f} ({P_D_given_Pos:.2%})\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_D = 0.01\n",
    "P_Pos_given_D = 0.99\n",
    "P_Pos_given_not_D = 0.05\n",
    "\n",
    "P_Pos = P_Pos_given_D * P_D + P_Pos_given_not_D * (1 - P_D)\n",
    "P_D_given_Pos = (P_Pos_given_D * P_D) / P_Pos\n",
    "\n",
    "print(\"P(Positive) =\", P_Pos)\n",
    "print(\"P(Disease | Positive) =\", P_D_given_Pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python implementation clearly confirms our manual calculation.**\n",
    "\n",
    "### Interpretation of this Formula:\n",
    "\n",
    "- Bayes' theorem is a powerful tool for \"updating\" the probability estimate of an event based on new observed data.\n",
    "- It is foundational in many machine learning algorithms and probabilistic modeling techniques, notably the Naive Bayes classifier, Bayesian networks, and Bayesian inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Conditional Probability Expansion (Chain Rule)\n",
    "\n",
    "#### Formula 22.9.3: Chain Rule\n",
    "\n",
    "**Original Formula:**\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|y) = P(x_1|y) \\cdot P(x_2|x_1, y) \\cdot P(x_3|x_2, x_1, y) \\ldots P(x_d|x_{d-1}, \\ldots, x_1, y)\n",
    "$$\n",
    "\n",
    "### Detailed Derivation:\n",
    "\n",
    "We will clearly derive this formula using the **chain rule of conditional probabilities**, which states that:\n",
    "\n",
    "$$\n",
    "P(A, B) = P(A) \\cdot P(B|A)\n",
    "$$\n",
    "\n",
    "Generalizing this for multiple variables, we have explicitly:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, x_3, \\ldots, x_d|y) = P(x_1|y) \\cdot P(x_2|x_1, y) \\cdot P(x_3|x_2, x_1, y) \\ldots P(x_d|x_{d-1}, \\ldots, x_1, y)\n",
    "$$\n",
    "\n",
    "This shows clearly how the probability of a complex event (a sequence of variables) can be broken down sequentially into simpler conditional probabilities.\n",
    "\n",
    "### Concrete Example (detailed calculation):\n",
    "\n",
    "Suppose we have the class \"Rainy day\" (denoted by $y$). We want to find the probability of three consecutive events happening given that it's a rainy day:\n",
    "\n",
    "- $x_1$: Traffic jam occurs\n",
    "- $x_2$: Late arrival to office occurs\n",
    "- $x_3$: Missed morning meeting occurs\n",
    "\n",
    "Using chain rule:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, x_3|y) = P(x_1|y) \\cdot P(x_2|x_1, y) \\cdot P(x_3|x_2, x_1, y)\n",
    "$$\n",
    "\n",
    "Given hypothetical probabilities clearly:\n",
    "\n",
    "- $P(x_1|y) = 0.7$ (Traffic jam given rainy day)\n",
    "- $P(x_2|x_1, y) = 0.8$ (Late arrival given rainy day and traffic jam)\n",
    "- $P(x_3|x_2, x_1, y) = 0.5$ (Missed meeting given rainy day, traffic jam, and late arrival)\n",
    "\n",
    "detailed computation explicitly:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, x_3|y) = 0.7 \\times 0.8 \\times 0.5 = 0.28\n",
    "$$\n",
    "\n",
    "Thus, the probability of all three events happening given it's a rainy day is explicitly 0.28.\n",
    "\n",
    "### Python Implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_x1_given_y = 0.7\n",
    "P_x2_given_x1_y = 0.8\n",
    "P_x3_given_x2_x1_y = 0.5\n",
    "\n",
    "P_joint_given_y = P_x1_given_y * P_x2_given_x1_y * P_x3_given_x2_x1_y\n",
    "\n",
    "print(\"P(x1, x2, x3 | y) =\", P_joint_given_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of this Formula:\n",
    "\n",
    "- The chain rule provides a systematic approach to handle complex joint conditional probabilities by breaking them into smaller, manageable steps.\n",
    "- It is a foundational tool in probability theory, extensively used in Machine Learning models such as Hidden Markov Models (HMM) and Bayesian Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Naive Independence Assumption and Binary Features\n",
    "\n",
    "#### Formula 22.9.4 (Naive Independence Assumption):\n",
    "\n",
    "To simplify the complexity, Naive Bayes assumes **conditional independence** of all features $x_i$ given the class $y$:\n",
    "\n",
    "$$\n",
    "P(x_i|x_{i-1}, \\ldots, x_1, y) = P(x_i|y)\n",
    "$$\n",
    "\n",
    "This reduces the likelihood dramatically to:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|y) = \\prod_{i=1}^{d} P(x_i|y)\n",
    "$$\n",
    "\n",
    "Now the classification rule becomes very clear:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y P(y) \\prod_{i=1}^{d} P(x_i|y)\n",
    "$$\n",
    "\n",
    "#### Formula 22.9.5 (Binary Feature Representation):\n",
    "\n",
    "When features are binary ($x_i \\in \\{0, 1\\}$), we have the simplified form:\n",
    "\n",
    "$$\n",
    "P(x_i = t_i|y) = \\begin{cases}\n",
    "P_{xy}[i, y], & t_i = 1 \\\\\n",
    "1 - P_{xy}[i, y], & t_i = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $P_y[y]$ is the prior probability of class $y$.\n",
    "- $P_{xy}[i, y] = P(x_i = 1|y)$ is the probability that feature $x_i = 1$ given class $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed detailed Derivation (Formula 22.9.5):\n",
    "\n",
    "- For a binary random variable $x_i$, we know clearly:\n",
    "\n",
    "$$\n",
    "P(x_i = 1|y) + P(x_i = 0|y) = 1\n",
    "$$\n",
    "\n",
    "- Thus, the probability of observing $x_i = 0$ is explicitly:\n",
    "\n",
    "$$\n",
    "P(x_i = 0|y) = 1 - P(x_i = 1|y)\n",
    "$$\n",
    "\n",
    "- We define explicitly $P_{xy}[i, y] \\equiv P(x_i = 1|y)$. Therefore, the conditional probability can be compactly represented as:\n",
    "\n",
    "$$\n",
    "P(x_i = t_i|y) = \\begin{cases}\n",
    "P_{xy}[i, y], & t_i = 1 \\\\\n",
    "1 - P_{xy}[i, y], & t_i = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This formula is now clearly proven detailed from basic probability definitions.\n",
    "\n",
    "### Concrete Example (detailed calculation):\n",
    "\n",
    "**Example:** Classifying an email as \"Spam\" or \"Not Spam\" based on one binary feature (the word \"win\"):\n",
    "\n",
    "- $x = 1$: word \"win\" appears in email\n",
    "- $x = 0$: word \"win\" does not appear\n",
    "\n",
    "Given conditional probabilities clearly:\n",
    "\n",
    "| Class | $P(x = 1\\|y)$ | $P(x = 0 \\| y)$ |\n",
    "| --- | --- | --- |\n",
    "| Spam (S) | 0.9 | $1 - 0.9 = 0.1$ |\n",
    "| Not Spam (NS)| 0.2 | $1 - 0.2 = 0.8$ |\n",
    "\n",
    "Given an email **without** the word \"win\" ($t = 0$), we compute explicitly:\n",
    "\n",
    "- $P(x = 0|\\text{Spam}) = 1 - 0.9 = 0.1$\n",
    "- $P(x = 0|\\text{Not Spam}) = 1 - 0.2 = 0.8$\n",
    "\n",
    "We clearly see:\n",
    "\n",
    "- Email without \"win\" is more likely \"Not Spam\".\n",
    "\n",
    "### Python Implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_xy = {\n",
    "    'Spam': 0.9,\n",
    "    'Not Spam': 0.2\n",
    "}\n",
    "\n",
    "t_i = 0  # Word \"win\" does NOT appear\n",
    "\n",
    "probabilities = {}\n",
    "for cls in P_xy:\n",
    "    if t_i == 1:\n",
    "        prob = P_xy[cls]\n",
    "    else:\n",
    "        prob = 1 - P_xy[cls]\n",
    "    probabilities[cls] = prob\n",
    "\n",
    "print(\"P(x=0|Spam):\", probabilities['Spam'])\n",
    "print(\"P(x=0|Not Spam):\", probabilities['Not Spam'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from mxnet import np as mx_np, npx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Binary Naive Bayes - Fruit Classification\")\n",
    "print(\"=\"*70)\n",
    "print(\"Features: x1=Yellow, x2=Elongated, x3=Seeds\")\n",
    "print(\"Classes: Banana (0), Grape (1)\")\n",
    "print(\"\\nPriors: P(Banana) = 0.5, P(Grape) = 0.5\")\n",
    "print(\"\\nP(xi=1|class):\")\n",
    "print(\"              Yellow  Elongated  Seeds\")\n",
    "print(\"  Banana:      0.9      0.95      0.05\")\n",
    "print(\"  Grape:       0.05     0.05      0.9\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_features = [1, 1, 0]\n",
    "print(f\"\\nTest: Yellow={test_features[0]}, Elongated={test_features[1]}, Seeds={test_features[2]}\")\n",
    "\n",
    "P_y = np.array([0.5, 0.5])\n",
    "P_xy = np.array([\n",
    "    [0.9, 0.95, 0.05],   # Banana\n",
    "    [0.05, 0.05, 0.9]    # Grape\n",
    "])\n",
    "\n",
    "# NumPy\n",
    "def compute_naive_bayes_np(features, P_y, P_xy):\n",
    "    posteriors = P_y.copy()\n",
    "    for cls in range(len(P_y)):\n",
    "        for i, feature in enumerate(features):\n",
    "            if feature == 1:\n",
    "                posteriors[cls] *= P_xy[cls, i]\n",
    "            else:\n",
    "                posteriors[cls] *= (1 - P_xy[cls, i])\n",
    "    return posteriors\n",
    "\n",
    "posteriors_np = compute_naive_bayes_np(test_features, P_y, P_xy)\n",
    "\n",
    "# PyTorch\n",
    "P_y_torch = torch.tensor(P_y, dtype=torch.float32)\n",
    "P_xy_torch = torch.tensor(P_xy, dtype=torch.float32)\n",
    "features_torch = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "def compute_naive_bayes_torch(features, P_y, P_xy):\n",
    "    log_posteriors = torch.log(P_y).clone()\n",
    "    for cls in range(len(P_y)):\n",
    "        for i, feature in enumerate(features):\n",
    "            if feature == 1:\n",
    "                log_posteriors[cls] += torch.log(P_xy[cls, i])\n",
    "            else:\n",
    "                log_posteriors[cls] += torch.log(1 - P_xy[cls, i])\n",
    "    return torch.exp(log_posteriors)\n",
    "\n",
    "posteriors_torch = compute_naive_bayes_torch(features_torch, P_y_torch, P_xy_torch)\n",
    "\n",
    "# TensorFlow\n",
    "P_y_tf = tf.constant(P_y, dtype=tf.float32)\n",
    "P_xy_tf = tf.constant(P_xy, dtype=tf.float32)\n",
    "features_tf = tf.constant(test_features, dtype=tf.float32)\n",
    "\n",
    "def compute_naive_bayes_tf(features, P_y, P_xy):\n",
    "    log_posteriors = tf.math.log(P_y)\n",
    "    posteriors_list = []\n",
    "    for cls in range(len(P_y)):\n",
    "        log_prob = log_posteriors[cls]\n",
    "        for i in range(len(features)):\n",
    "            if features[i] == 1:\n",
    "                log_prob += tf.math.log(P_xy[cls, i])\n",
    "            else:\n",
    "                log_prob += tf.math.log(1 - P_xy[cls, i])\n",
    "        posteriors_list.append(tf.exp(log_prob))\n",
    "    return tf.stack(posteriors_list)\n",
    "\n",
    "posteriors_tf = compute_naive_bayes_tf(features_tf, P_y_tf, P_xy_tf)\n",
    "\n",
    "# MXNet\n",
    "P_y_mx = mx_np.array(P_y)\n",
    "P_xy_mx = mx_np.array(P_xy)\n",
    "\n",
    "posteriors_mx = mx_np.array([\n",
    "    P_y_mx[0] * P_xy_mx[0,0]**test_features[0] * (1-P_xy_mx[0,0])**(1-test_features[0]) * \n",
    "    P_xy_mx[0,1]**test_features[1] * (1-P_xy_mx[0,1])**(1-test_features[1]) * \n",
    "    P_xy_mx[0,2]**test_features[2] * (1-P_xy_mx[0,2])**(1-test_features[2]),\n",
    "    P_y_mx[1] * P_xy_mx[1,0]**test_features[0] * (1-P_xy_mx[1,0])**(1-test_features[0]) * \n",
    "    P_xy_mx[1,1]**test_features[1] * (1-P_xy_mx[1,1])**(1-test_features[1]) * \n",
    "    P_xy_mx[1,2]**test_features[2] * (1-P_xy_mx[1,2])**(1-test_features[2])\n",
    "])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "classes = ['Banana', 'Grape']\n",
    "colors = ['#FFD700', '#9370DB']\n",
    "\n",
    "# NumPy\n",
    "axes[0,0].bar(classes, posteriors_np, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,0].set_title('NumPy Implementation', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Posterior Probability', fontsize=10)\n",
    "axes[0,0].set_ylim([0, max(posteriors_np) * 1.2])\n",
    "for i, v in enumerate(posteriors_np):\n",
    "    axes[0,0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# PyTorch\n",
    "axes[0,1].bar(classes, posteriors_torch.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,1].set_title('PyTorch (torch.log/exp)', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Posterior Probability', fontsize=10)\n",
    "axes[0,1].set_ylim([0, max(posteriors_torch.numpy()) * 1.2])\n",
    "for i, v in enumerate(posteriors_torch.numpy()):\n",
    "    axes[0,1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# TensorFlow\n",
    "axes[1,0].bar(classes, posteriors_tf.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,0].set_title('TensorFlow (tf.math.log/exp)', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Posterior Probability', fontsize=10)\n",
    "axes[1,0].set_ylim([0, max(posteriors_tf.numpy()) * 1.2])\n",
    "for i, v in enumerate(posteriors_tf.numpy()):\n",
    "    axes[1,0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MXNet\n",
    "axes[1,1].bar(classes, posteriors_mx.asnumpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,1].set_title('MXNet (mx.np operations)', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Posterior Probability', fontsize=10)\n",
    "axes[1,1].set_ylim([0, max(posteriors_mx.asnumpy()) * 1.2])\n",
    "for i, v in enumerate(posteriors_mx.asnumpy()):\n",
    "    axes[1,1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Naive Bayes Classification: Test=[{test_features[0]}, {test_features[1]}, {test_features[2]}]', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Classification Results\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Unnormalized Posteriors:\")\n",
    "print(f\"  NumPy:      Banana={posteriors_np[0]:.6f}, Grape={posteriors_np[1]:.6f}\")\n",
    "print(f\"  PyTorch:    Banana={posteriors_torch[0].item():.6f}, Grape={posteriors_torch[1].item():.6f}\")\n",
    "print(f\"  TensorFlow: Banana={posteriors_tf[0].numpy():.6f}, Grape={posteriors_tf[1].numpy():.6f}\")\n",
    "print(f\"  MXNet:      Banana={float(posteriors_mx[0]):.6f}, Grape={float(posteriors_mx[1]):.6f}\")\n",
    "\n",
    "predicted_np = classes[np.argmax(posteriors_np)]\n",
    "\n",
    "print(f\"\\nPredicted: {predicted_np}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clearly verifies our manual calculation.**\n",
    "\n",
    "### Interpretation of this Formula:\n",
    "\n",
    "- This simplified representation is crucial when dealing with binary features, allowing efficient computation and storage of probabilities in practice.\n",
    "- It forms the basis for efficient implementations of the Naive Bayes classifier, especially for text classification and spam detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. General Binary Naive Bayes Classifier\n",
    "\n",
    "#### Formula 22.9.6: General Binary Naive Bayes\n",
    "\n",
    "**Original Formula:**\n",
    "\n",
    "The general binary Naive Bayes classifier clearly expressed as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y P_y[y] \\prod_{i=1}^{d} P_{xy}[i, y]^{t_i}(1 - P_{xy}[i, y])^{(1-t_i)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $P_y[y]$: Prior probability of class $y$.\n",
    "- $P_{xy}[i, y] = P(x_i = 1|y)$: Conditional probability of binary feature $x_i$.\n",
    "- $t_i \\in \\{0, 1\\}$: Observed value of feature $x_i$.\n",
    "\n",
    "### Detailed Derivation:\n",
    "\n",
    "**Step 1:**\n",
    "\n",
    "Starting from the general Naive Bayes formula:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y P(y|\\mathbf{x}) \\propto P(y) \\prod_{i=1}^{d} P(x_i|y)\n",
    "$$\n",
    "\n",
    "**Step 2:**\n",
    "\n",
    "Substitute the binary representation explicitly (from formula 22.9.5):\n",
    "\n",
    "$$\n",
    "P(x_i = t_i|y) = P_{xy}[i, y]^{t_i}(1 - P_{xy}[i, y])^{(1-t_i)}\n",
    "$$\n",
    "\n",
    "**Step 3:**\n",
    "\n",
    "Replace each $P(x_i|y)$ explicitly:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|y) = \\prod_{i=1}^{d} P(x_i = t_i|y) = \\prod_{i=1}^{d} \\left[P_{xy}[i, y]^{t_i}(1 - P_{xy}[i, y])^{(1-t_i)}\\right]\n",
    "$$\n",
    "\n",
    "**Step 4:**\n",
    "\n",
    "Hence, clearly, the classifier decision rule becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y P_y[y] \\prod_{i=1}^{d} \\left[P_{xy}[i, y]^{t_i}(1 - P_{xy}[i, y])^{(1-t_i)}\\right]\n",
    "$$\n",
    "\n",
    "Thus, we've clearly derived formula 22.9.6 detailed from basic probability principles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete Example (detailed calculation):\n",
    "\n",
    "Consider classifying a fruit as either \"Banana\" or \"Grape\" using three binary features:\n",
    "\n",
    "- $x_1$: Yellow color? (1 = Yes, 0 = No)\n",
    "- $x_2$: Elongated shape? (1 = Yes, 0 = No)\n",
    "- $x_3$: Contains seeds? (1 = Yes, 0 = No)\n",
    "\n",
    "Given clearly the following probabilities:\n",
    "\n",
    "| Class | $P_y[y]$ | $P_{xy}[1, y]$ | $P_{xy}[2, y]$ | $P_{xy}[3, y]$ |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Banana | 0.5 | 0.9 | 0.95 | 0.05 |\n",
    "| Grape | 0.5 | 0.05 | 0.05 | 0.9 |\n",
    "\n",
    "Classify clearly a fruit with features:\n",
    "\n",
    "- Yellow ($t_1 = 1$), Elongated ($t_2 = 1$), No seeds ($t_3 = 0$).\n",
    "\n",
    "**Compute explicitly each class probability:**\n",
    "\n",
    "- **Banana:**\n",
    "\n",
    "$$\n",
    "P = 0.5 \\times (0.9)^1 \\times (0.95)^1 \\times (1 - 0.05)^1 = 0.5 \\times 0.9 \\times 0.95 \\times 0.95 = 0.4055\n",
    "$$\n",
    "\n",
    "- **Grape:**\n",
    "\n",
    "$$\n",
    "P = 0.5 \\times (0.05)^1 \\times (0.05)^1 \\times (1 - 0.9)^1 = 0.5 \\times 0.05 \\times 0.05 \\times 0.1 = 0.000125\n",
    "$$\n",
    "\n",
    "Clearly comparing:\n",
    "\n",
    "$$\n",
    "0.4055 > 0.000125 \\quad \\Rightarrow \\quad \\hat{y} = \\text{Banana}\n",
    "$$\n",
    "\n",
    "Thus, explicitly classified as a \"Banana\".\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_y = {'Banana': 0.5, 'Grape': 0.5}\n",
    "P_xy = {\n",
    "    'Banana': [0.9, 0.95, 0.05],\n",
    "    'Grape': [0.05, 0.05, 0.9]\n",
    "}\n",
    "\n",
    "t = [1, 1, 0]\n",
    "\n",
    "posteriors = {}\n",
    "for cls in P_y:\n",
    "    prob = P_y[cls]\n",
    "    for i, feature in enumerate(t):\n",
    "        prob *= P_xy[cls][i]**feature * (1 - P_xy[cls][i])**(1 - feature)\n",
    "    posteriors[cls] = prob\n",
    "\n",
    "predicted_class = max(posteriors, key=posteriors.get)\n",
    "\n",
    "print(\"Posterior Probabilities:\", posteriors)\n",
    "print(\"Predicted:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python code clearly confirms our manual calculation.**\n",
    "\n",
    "### Interpretation of this Formula:\n",
    "\n",
    "- Formula 22.9.6 provides a simple yet powerful classifier for binary-feature datasets.\n",
    "- Widely applicable in fields such as text classification, medical diagnosis, and spam detection due to its efficiency and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Naive Bayes Training (Logarithmic Form)\n",
    "\n",
    "When computing probabilities, especially with large feature sets (e.g., images with many pixels), directly multiplying probabilities can lead to numerical underflow (very small numbers). To avoid this, we use a logarithmic transformation, converting multiplication into addition, greatly enhancing numerical stability.\n",
    "\n",
    "#### Formula 22.9.7 (Logarithmic Form of Naive Bayes):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y \\left[\\log P_y[y] + \\sum_{i=1}^{d} \\left[t_i \\log P_{xy}[i, y] + (1 - t_i)\\log(1 - P_{xy}[i, y])\\right]\\right]\n",
    "$$\n",
    "\n",
    "- $P_y[y]$: Prior probability of class $y$.\n",
    "- $P_{xy}[i, y]$: Probability feature $x_i = 1$, given class $y$.\n",
    "- $t_i$: Observed value of feature $x_i$ (binary, either 0 or 1).\n",
    "\n",
    "### Derivation & Explanation:\n",
    "\n",
    "- Recall the original Naive Bayes formula (binary form):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y P_y[y] \\prod_{i=1}^{d} P_{xy}[i, y]^{t_i}(1 - P_{xy}[i, y])^{(1-t_i)}\n",
    "$$\n",
    "\n",
    "- Apply logarithmic transformation clearly to avoid underflow:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_y \\log \\left(P_y[y] \\prod_{i=1}^{d} P_{xy}[i, y]^{t_i}(1 - P_{xy}[i, y])^{(1-t_i)}\\right)\n",
    "$$\n",
    "\n",
    "- Using properties of logarithms, convert the product into sum explicitly:\n",
    "\n",
    "$$\n",
    "= \\operatorname*{argmax}_y \\left[\\log P_y[y] + \\sum_{i=1}^{d} \\log \\left(P_{xy}[i, y]^{t_i}(1 - P_{xy}[i, y])^{(1-t_i)}\\right)\\right]\n",
    "$$\n",
    "\n",
    "- Simplify the exponents:\n",
    "\n",
    "$$\n",
    "= \\operatorname*{argmax}_y \\left[\\log P_y[y] + \\sum_{i=1}^{d} \\left[t_i \\log P_{xy}[i, y] + (1 - t_i)\\log(1 - P_{xy}[i, y])\\right]\\right]\n",
    "$$\n",
    "\n",
    "Thus, we clearly obtained the numerically stable logarithmic form for Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete Example (detailed calculation):\n",
    "\n",
    "Let's reuse the previous fruit classification example with logarithmic calculations:\n",
    "\n",
    "| Class | $P(y)$ | $P(x_1 = 1 \\| y)$ | $P(x_2 = 1 \\| y)$ |\n",
    "| --- | --- | --- | --- |\n",
    "| Apple | 0.6 | 0.8 | 0.9 |\n",
    "| Orange | 0.4 | 0.1 | 0.95 |\n",
    "\n",
    "Given fruit features: $x_1 = 1$ (red), $x_2 = 1$ (round):\n",
    "\n",
    "- **Apple** (log-form):\n",
    "\n",
    "$$\n",
    "\\log(0.6) + [1 \\cdot \\log(0.8) + 1 \\cdot \\log(0.9)] = -0.5108 + (-0.2231) + (-0.1054) = -0.8393\n",
    "$$\n",
    "\n",
    "- **Orange** (log-form):\n",
    "\n",
    "$$\n",
    "\\log(0.4) + [1 \\cdot \\log(0.1) + 1 \\cdot \\log(0.95)] = -0.9163 + (-2.3026) + (-0.0513) = -3.2702\n",
    "$$\n",
    "\n",
    "Compare log-probabilities:\n",
    "\n",
    "$$\n",
    "-0.8393 > -3.2702 \\quad \\Rightarrow \\quad \\hat{y} = \\text{Apple}\n",
    "$$\n",
    "\n",
    "Again, clearly classified as Apple.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "P_y = {'Apple': 0.6, 'Orange': 0.4}\n",
    "P_xy = {\n",
    "    'Apple': [0.8, 0.9],\n",
    "    'Orange': [0.1, 0.95]\n",
    "}\n",
    "\n",
    "x_test = [1, 1]\n",
    "\n",
    "log_posteriors = {}\n",
    "for cls in P_y:\n",
    "    log_prob = np.log(P_y[cls])\n",
    "    for i, x in enumerate(x_test):\n",
    "        prob = P_xy[cls][i] if x == 1 else (1 - P_xy[cls][i])\n",
    "        log_prob += np.log(prob)\n",
    "    log_posteriors[cls] = log_prob\n",
    "\n",
    "prediction = max(log_posteriors, key=log_posteriors.get)\n",
    "print(\"Log-Posteriors:\", log_posteriors)\n",
    "print(\"Predicted:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from mxnet import np as mx_np, npx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "# Exercise 1 data\n",
    "P_y_data = [0.4, 0.6]\n",
    "P_xy_data = [[0.8, 0.7], [0.3, 0.4]]  # P(xi=1|y)\n",
    "test_features = [1, 0]  # x1=1, x2=0\n",
    "class_labels = ['Class 0', 'Class 1']\n",
    "\n",
    "# Compute posteriors for all frameworks\n",
    "def compute_posteriors(P_y, P_xy, features):\n",
    "    posteriors = []\n",
    "    for cls in range(len(P_y)):\n",
    "        prob = P_y[cls]\n",
    "        for i, feature in enumerate(features):\n",
    "            if feature == 1:\n",
    "                prob *= P_xy[cls][i]\n",
    "            else:\n",
    "                prob *= (1 - P_xy[cls][i])\n",
    "        posteriors.append(prob)\n",
    "    return posteriors\n",
    "\n",
    "# NumPy\n",
    "posteriors_np = np.array(compute_posteriors(P_y_data, P_xy_data, test_features))\n",
    "\n",
    "# PyTorch\n",
    "P_y_torch = torch.tensor(P_y_data, dtype=torch.float32)\n",
    "P_xy_torch = torch.tensor(P_xy_data, dtype=torch.float32)\n",
    "posteriors_torch = torch.tensor(compute_posteriors(P_y_data, P_xy_data, test_features), dtype=torch.float32)\n",
    "\n",
    "# TensorFlow\n",
    "posteriors_tf = tf.constant(compute_posteriors(P_y_data, P_xy_data, test_features), dtype=tf.float32)\n",
    "\n",
    "# MXNet\n",
    "posteriors_mx = mx_np.array(compute_posteriors(P_y_data, P_xy_data, test_features))\n",
    "\n",
    "# Normalize posteriors\n",
    "total_np = posteriors_np.sum()\n",
    "normalized_np = posteriors_np / total_np\n",
    "\n",
    "# Create 2x2 visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "colors = ['#FF9999', '#99CCFF']\n",
    "\n",
    "# NumPy - Unnormalized\n",
    "axes[0,0].bar(class_labels, posteriors_np, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,0].set_title('NumPy - Unnormalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Probability', fontsize=10)\n",
    "axes[0,0].set_ylim([0, max(posteriors_np) * 1.3])\n",
    "for i, v in enumerate(posteriors_np):\n",
    "    axes[0,0].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# PyTorch - Unnormalized\n",
    "axes[0,1].bar(class_labels, posteriors_torch.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,1].set_title('PyTorch - Unnormalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Probability', fontsize=10)\n",
    "axes[0,1].set_ylim([0, max(posteriors_torch.numpy()) * 1.3])\n",
    "for i, v in enumerate(posteriors_torch.numpy()):\n",
    "    axes[0,1].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# TensorFlow - Unnormalized\n",
    "axes[1,0].bar(class_labels, posteriors_tf.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,0].set_title('TensorFlow - Unnormalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Probability', fontsize=10)\n",
    "axes[1,0].set_ylim([0, max(posteriors_tf.numpy()) * 1.3])\n",
    "for i, v in enumerate(posteriors_tf.numpy()):\n",
    "    axes[1,0].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MXNet - Unnormalized\n",
    "axes[1,1].bar(class_labels, posteriors_mx.asnumpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,1].set_title('MXNet - Unnormalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Probability', fontsize=10)\n",
    "axes[1,1].set_ylim([0, max(posteriors_mx.asnumpy()) * 1.3])\n",
    "for i, v in enumerate(posteriors_mx.asnumpy()):\n",
    "    axes[1,1].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Exercise 1: Binary Classification (x1={test_features[0]}, x2={test_features[1]})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Exercise 1 - Framework Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Features: x1={test_features[0]}, x2={test_features[1]}\")\n",
    "print(f\"\\nUnnormalized Posteriors:\")\n",
    "print(f\"  NumPy:      Class 0={posteriors_np[0]:.6f}, Class 1={posteriors_np[1]:.6f}\")\n",
    "print(f\"  PyTorch:    Class 0={posteriors_torch[0].item():.6f}, Class 1={posteriors_torch[1].item():.6f}\")\n",
    "print(f\"  TensorFlow: Class 0={posteriors_tf[0].numpy():.6f}, Class 1={posteriors_tf[1].numpy():.6f}\")\n",
    "print(f\"  MXNet:      Class 0={float(posteriors_mx[0]):.6f}, Class 1={float(posteriors_mx[1]):.6f}\")\n",
    "print(f\"\\nNormalized Posteriors:\")\n",
    "print(f\"  Class 0: {normalized_np[0]:.6f} ({normalized_np[0]:.2%})\")\n",
    "print(f\"  Class 1: {normalized_np[1]:.6f} ({normalized_np[1]:.2%})\")\n",
    "print(f\"\\nPredicted: Class {np.argmax(posteriors_np)}\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise Solutions\n",
    "\n",
    "This section provides detailed solutions to all exercises, including:\n",
    "- **Detailed reasoning:** Clear mathematical derivations\n",
    "- **Computations:** All calculation steps shown explicitly\n",
    "- **Python implementation:** Numerical verification of analytical results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Suppose we have a dataset with two binary features $x_1, x_2$, and a binary class $y \\in \\{0, 1\\}$. The probabilities are given as follows:\n",
    "\n",
    "| $y$ | $P(y)$ | $P(x_1 = 1 \\| y)$ | $P(x_2 = 1 \\| y)$ |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | 0.4 | 0.8 | 0.7 |\n",
    "| 1 | 0.6 | 0.3 | 0.4 |\n",
    "\n",
    "Given a new sample with features: $x_1 = 1, x_2 = 0$, classify this sample using Naive Bayes clearly and explicitly.\n",
    "\n",
    "### detailed Solution (Detailed Computations):\n",
    "\n",
    "**Step 1: Compute probability for class $y = 0$ explicitly:**\n",
    "\n",
    "- Prior: $P(y = 0) = 0.4$\n",
    "- Likelihood: $P(x_1 = 1|y = 0) = 0.8$, $P(x_2 = 0|y = 0) = 1 - 0.7 = 0.3$\n",
    "\n",
    "Compute explicitly:\n",
    "\n",
    "$$\n",
    "P(y = 0|\\mathbf{x}) \\propto P(y = 0)P(x_1 = 1|y = 0)P(x_2 = 0|y = 0) = 0.4 \\times 0.8 \\times 0.3 = 0.096\n",
    "$$\n",
    "\n",
    "**Step 2: Compute probability for class $y = 1$ explicitly:**\n",
    "\n",
    "- Prior: $P(y = 1) = 0.6$\n",
    "- Likelihood: $P(x_1 = 1|y = 1) = 0.3$, $P(x_2 = 0|y = 1) = 1 - 0.4 = 0.6$\n",
    "\n",
    "Compute explicitly:\n",
    "\n",
    "$$\n",
    "P(y = 1|\\mathbf{x}) \\propto P(y = 1)P(x_1 = 1|y = 1)P(x_2 = 0|y = 1) = 0.6 \\times 0.3 \\times 0.6 = 0.108\n",
    "$$\n",
    "\n",
    "**Step 3: Compare explicitly:**\n",
    "\n",
    "$$\n",
    "0.108 > 0.096 \\quad \\Rightarrow \\quad \\hat{y} = 1\n",
    "$$\n",
    "\n",
    "Thus, explicitly classified as class $y = 1$.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from mxnet import np as mx_np, npx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "# Exercise 2 data\n",
    "P_y_data = [0.7, 0.3]\n",
    "P_xy_data = [[0.2, 0.1], [0.6, 0.8]]  # P(xi=1|y)\n",
    "test_features = [1, 0]  # x1=1 (saw ad), x2=0 (no friend recommendation)\n",
    "class_labels = ['No Buy (0)', 'Buy (1)']\n",
    "\n",
    "# Compute posteriors for all frameworks\n",
    "def compute_posteriors_ex2(P_y, P_xy, features):\n",
    "    posteriors = []\n",
    "    for cls in range(len(P_y)):\n",
    "        prob = P_y[cls]\n",
    "        for i, feature in enumerate(features):\n",
    "            if feature == 1:\n",
    "                prob *= P_xy[cls][i]\n",
    "            else:\n",
    "                prob *= (1 - P_xy[cls][i])\n",
    "        posteriors.append(prob)\n",
    "    return posteriors\n",
    "\n",
    "# NumPy\n",
    "posteriors_np = np.array(compute_posteriors_ex2(P_y_data, P_xy_data, test_features))\n",
    "total_np = posteriors_np.sum()\n",
    "normalized_np = posteriors_np / total_np\n",
    "\n",
    "# PyTorch\n",
    "posteriors_torch = torch.tensor(compute_posteriors_ex2(P_y_data, P_xy_data, test_features), dtype=torch.float32)\n",
    "total_torch = posteriors_torch.sum()\n",
    "normalized_torch = posteriors_torch / total_torch\n",
    "\n",
    "# TensorFlow\n",
    "posteriors_tf = tf.constant(compute_posteriors_ex2(P_y_data, P_xy_data, test_features), dtype=tf.float32)\n",
    "total_tf = tf.reduce_sum(posteriors_tf)\n",
    "normalized_tf = posteriors_tf / total_tf\n",
    "\n",
    "# MXNet\n",
    "posteriors_mx = mx_np.array(compute_posteriors_ex2(P_y_data, P_xy_data, test_features))\n",
    "total_mx = mx_np.sum(posteriors_mx)\n",
    "normalized_mx = posteriors_mx / total_mx\n",
    "\n",
    "# Create 2x2 visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "colors = ['#E67E22', '#27AE60']\n",
    "\n",
    "# NumPy\n",
    "axes[0,0].bar(class_labels, normalized_np, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,0].set_title('NumPy - Normalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Probability', fontsize=10)\n",
    "axes[0,0].set_ylim([0, 1.0])\n",
    "for i, v in enumerate(normalized_np):\n",
    "    axes[0,0].text(i, v + 0.02, f'{v:.4f}\\n({v:.1%})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# PyTorch\n",
    "axes[0,1].bar(class_labels, normalized_torch.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,1].set_title('PyTorch - Normalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Probability', fontsize=10)\n",
    "axes[0,1].set_ylim([0, 1.0])\n",
    "for i, v in enumerate(normalized_torch.numpy()):\n",
    "    axes[0,1].text(i, v + 0.02, f'{v:.4f}\\n({v:.1%})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# TensorFlow\n",
    "axes[1,0].bar(class_labels, normalized_tf.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,0].set_title('TensorFlow - Normalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Probability', fontsize=10)\n",
    "axes[1,0].set_ylim([0, 1.0])\n",
    "for i, v in enumerate(normalized_tf.numpy()):\n",
    "    axes[1,0].text(i, v + 0.02, f'{v:.4f}\\n({v:.1%})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MXNet\n",
    "axes[1,1].bar(class_labels, normalized_mx.asnumpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,1].set_title('MXNet - Normalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Probability', fontsize=10)\n",
    "axes[1,1].set_ylim([0, 1.0])\n",
    "for i, v in enumerate(normalized_mx.asnumpy()):\n",
    "    axes[1,1].text(i, v + 0.02, f'{v:.4f}\\n({v:.1%})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Exercise 2: Product Purchase Prediction (Saw Ad=Yes, Friend Rec=No)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Exercise 2 - Product Purchase Prediction\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Features: x1=1 (Saw ad), x2=0 (No friend recommendation)\")\n",
    "print(f\"\\nNormalized Posteriors (all frameworks agree):\")\n",
    "print(f\"  No Buy (0): {normalized_np[0]:.6f} ({normalized_np[0]:.2%})\")\n",
    "print(f\"  Buy (1):    {normalized_np[1]:.6f} ({normalized_np[1]:.2%})\")\n",
    "print(f\"\\nDecision: {'Not Buy' if np.argmax(posteriors_np)==0 else 'Buy'}\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_y = {0: 0.4, 1: 0.6}\n",
    "P_xy = {\n",
    "    0: [0.8, 0.7],\n",
    "    1: [0.3, 0.4]\n",
    "}\n",
    "\n",
    "t = [1, 0]\n",
    "\n",
    "posteriors = {}\n",
    "for cls in P_y:\n",
    "    prob = P_y[cls]\n",
    "    for i, feature in enumerate(t):\n",
    "        prob *= P_xy[cls][i] if feature == 1 else (1 - P_xy[cls][i])\n",
    "    posteriors[cls] = prob\n",
    "\n",
    "prediction = max(posteriors, key=posteriors.get)\n",
    "\n",
    "print(\"Posterior Probabilities:\", posteriors)\n",
    "print(\"Predicted:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python explicitly confirms our manual calculation.**\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- This exercise clearly illustrates how Naive Bayes works with binary features and classes.\n",
    "- The result explicitly shows that even though the difference in probabilities is small, Naive Bayes can effectively distinguish the more probable class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from mxnet import np as mx_np, npx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "# Exercise 3 data\n",
    "P_y_data = [0.9, 0.1]\n",
    "P_xy_data = [[0.1, 0.2], [0.8, 0.7]]  # P(xi=1|y) for [Fever, Cough]\n",
    "test_features = [1, 1]  # x1=1 (fever), x2=1 (cough)\n",
    "class_labels = ['Healthy (0)', 'Flu (1)']\n",
    "\n",
    "# Compute posteriors for all frameworks\n",
    "def compute_posteriors_ex3(P_y, P_xy, features):\n",
    "    posteriors = []\n",
    "    for cls in range(len(P_y)):\n",
    "        prob = P_y[cls]\n",
    "        for i, feature in enumerate(features):\n",
    "            if feature == 1:\n",
    "                prob *= P_xy[cls][i]\n",
    "            else:\n",
    "                prob *= (1 - P_xy[cls][i])\n",
    "        posteriors.append(prob)\n",
    "    return posteriors\n",
    "\n",
    "# NumPy\n",
    "posteriors_np = np.array(compute_posteriors_ex3(P_y_data, P_xy_data, test_features))\n",
    "total_np = posteriors_np.sum()\n",
    "normalized_np = posteriors_np / total_np\n",
    "\n",
    "# PyTorch\n",
    "posteriors_torch = torch.tensor(compute_posteriors_ex3(P_y_data, P_xy_data, test_features), dtype=torch.float32)\n",
    "total_torch = posteriors_torch.sum()\n",
    "normalized_torch = posteriors_torch / total_torch\n",
    "\n",
    "# TensorFlow\n",
    "posteriors_tf = tf.constant(compute_posteriors_ex3(P_y_data, P_xy_data, test_features), dtype=tf.float32)\n",
    "total_tf = tf.reduce_sum(posteriors_tf)\n",
    "normalized_tf = posteriors_tf / total_tf\n",
    "\n",
    "# MXNet\n",
    "posteriors_mx = mx_np.array(compute_posteriors_ex3(P_y_data, P_xy_data, test_features))\n",
    "total_mx = mx_np.sum(posteriors_mx)\n",
    "normalized_mx = posteriors_mx / total_mx\n",
    "\n",
    "# Create 2x2 visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "colors = ['#3498DB', '#E74C3C']\n",
    "\n",
    "# NumPy\n",
    "axes[0,0].bar(class_labels, normalized_np, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,0].set_title('NumPy - Normalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Probability', fontsize=10)\n",
    "axes[0,0].set_ylim([0, 1.0])\n",
    "for i, v in enumerate(normalized_np):\n",
    "    axes[0,0].text(i, v + 0.02, f'{v:.4f}\\n({v:.1%})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# PyTorch\n",
    "axes[0,1].bar(class_labels, normalized_torch.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0,1].set_title('PyTorch - Normalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Probability', fontsize=10)\n",
    "axes[0,1].set_ylim([0, 1.0])\n",
    "for i, v in enumerate(normalized_torch.numpy()):\n",
    "    axes[0,1].text(i, v + 0.02, f'{v:.4f}\\n({v:.1%})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# TensorFlow\n",
    "axes[1,0].bar(class_labels, normalized_tf.numpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,0].set_title('TensorFlow - Normalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Probability', fontsize=10)\n",
    "axes[1,0].set_ylim([0, 1.0])\n",
    "for i, v in enumerate(normalized_tf.numpy()):\n",
    "    axes[1,0].text(i, v + 0.02, f'{v:.4f}\\n({v:.1%})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MXNet\n",
    "axes[1,1].bar(class_labels, normalized_mx.asnumpy(), color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1,1].set_title('MXNet - Normalized Posteriors', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Probability', fontsize=10)\n",
    "axes[1,1].set_ylim([0, 1.0])\n",
    "for i, v in enumerate(normalized_mx.asnumpy()):\n",
    "    axes[1,1].text(i, v + 0.02, f'{v:.4f}\\n({v:.1%})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Exercise 3: Medical Diagnosis (Fever=Yes, Cough=Yes)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Exercise 3 - Medical Diagnosis (Flu Detection)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Symptoms: Fever=Yes, Cough=Yes\")\n",
    "print(f\"\\nPriors:\")\n",
    "print(f\"  P(Healthy) = {P_y_data[0]} (90%)\")\n",
    "print(f\"  P(Flu) = {P_y_data[1]} (10%)\")\n",
    "print(f\"\\nPosteriors (all frameworks agree):\")\n",
    "print(f\"  Healthy: {normalized_np[0]:.6f} ({normalized_np[0]:.2%})\")\n",
    "print(f\"  Flu:     {normalized_np[1]:.6f} ({normalized_np[1]:.2%})\")\n",
    "print(f\"\\nDiagnosis: {'Healthy' if np.argmax(posteriors_np)==0 else 'Flu'}\")\n",
    "print(f\"\\nInterpretation: Despite low prior (10%), both symptoms strongly\")\n",
    "print(f\"indicate Flu, increasing posterior to {normalized_np[1]:.1%}!\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Consider a binary classification problem to determine whether a person will buy a product ($y = 1$) or not ($y = 0$) based on two binary features:\n",
    "\n",
    "- $x_1$: Saw an advertisement (1: yes, 0: no)\n",
    "- $x_2$: Friend recommended (1: yes, 0: no)\n",
    "\n",
    "Probabilities based on historical data are as follows:\n",
    "\n",
    "| Class $y$ | Prior $P(y)$ | $P(x_1 = 1 \\| y)$ | $P(x_2 = 1 \\| y)$ |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 (No) | 0.7 | 0.2 | 0.1 |\n",
    "| 1 (Yes) | 0.3 | 0.6 | 0.8 |\n",
    "\n",
    "Given a new customer who saw the advertisement but didn't have a friend recommendation ($x_1 = 1, x_2 = 0$), explicitly compute the posterior probabilities and classify using the Naive Bayes classifier.\n",
    "\n",
    "### detailed Solution (Detailed Computations):\n",
    "\n",
    "**Step 1: Compute posterior for class $y = 0$ explicitly:**\n",
    "\n",
    "- Prior probability: $P(y = 0) = 0.7$\n",
    "- Conditional likelihood:\n",
    "  - $P(x_1 = 1|y = 0) = 0.2$\n",
    "  - $P(x_2 = 0|y = 0) = 1 - 0.1 = 0.9$\n",
    "\n",
    "Compute explicitly:\n",
    "\n",
    "$$\n",
    "P(y = 0|\\mathbf{x}) \\propto P(y = 0)P(x_1 = 1|y = 0)P(x_2 = 0|y = 0) = 0.7 \\times 0.2 \\times 0.9 = 0.126\n",
    "$$\n",
    "\n",
    "**Step 2: Compute posterior for class $y = 1$ explicitly:**\n",
    "\n",
    "- Prior probability: $P(y = 1) = 0.3$\n",
    "- Conditional likelihood:\n",
    "  - $P(x_1 = 1|y = 1) = 0.6$\n",
    "  - $P(x_2 = 0|y = 1) = 1 - 0.8 = 0.2$\n",
    "\n",
    "Compute explicitly:\n",
    "\n",
    "$$\n",
    "P(y = 1|\\mathbf{x}) \\propto P(y = 1)P(x_1 = 1|y = 1)P(x_2 = 0|y = 1) = 0.3 \\times 0.6 \\times 0.2 = 0.036\n",
    "$$\n",
    "\n",
    "**Step 3: Normalize to get exact posterior probabilities explicitly:**\n",
    "\n",
    "$$\n",
    "P(y = 0|\\mathbf{x}) = \\frac{0.126}{0.126 + 0.036} = \\frac{0.126}{0.162} \\approx 0.7778\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y = 1|\\mathbf{x}) = \\frac{0.036}{0.162} \\approx 0.2222\n",
    "$$\n",
    "\n",
    "**Step 4: Decision explicitly:**\n",
    "\n",
    "$$\n",
    "0.7778 > 0.2222 \\quad \\Rightarrow \\quad \\hat{y} = 0 \\quad \\text{(Not buy)}\n",
    "$$\n",
    "\n",
    "### Python Implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_y = {0: 0.7, 1: 0.3}\n",
    "P_xy = {\n",
    "    0: [0.2, 0.1],\n",
    "    1: [0.6, 0.8]\n",
    "}\n",
    "\n",
    "t = [1, 0]\n",
    "\n",
    "posteriors = {}\n",
    "for cls in P_y:\n",
    "    prob = P_y[cls]\n",
    "    for i, feature in enumerate(t):\n",
    "        prob *= P_xy[cls][i] if feature == 1 else (1 - P_xy[cls][i])\n",
    "    posteriors[cls] = prob\n",
    "\n",
    "total = sum(posteriors.values())\n",
    "for cls in posteriors:\n",
    "    posteriors[cls] /= total\n",
    "\n",
    "predicted_class = max(posteriors, key=posteriors.get)\n",
    "\n",
    "print(\"Normalized Posteriors:\", posteriors)\n",
    "print(\"Predicted:\", predicted_class, \"(Not buy)\" if predicted_class==0 else \"(Buy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python implementation explicitly confirms our manual calculation.**\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- Clearly, even though the customer saw the advertisement, the absence of a friend recommendation significantly lowered the probability of purchasing.\n",
    "- This demonstrates how Naive Bayes combines different evidence (features) to produce a clear decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Consider a medical diagnosis scenario with two symptoms as binary features:\n",
    "\n",
    "- $x_1$: Fever (1 = yes, 0 = no)\n",
    "- $x_2$: Cough (1 = yes, 0 = no)\n",
    "\n",
    "Patients are classified as having either \"Flu\" ($y = 1$) or \"Healthy\" ($y = 0$). Historical data probabilities are as follows:\n",
    "\n",
    "| Class $y$ | Prior $P(y)$ | $P(x_1 = 1 \\| y)$ (Fever) | $P(x_2 = 1 \\| y)$ (Cough) |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 (Healthy) | 0.9 | 0.1 | 0.2 |\n",
    "| 1 (Flu) | 0.1 | 0.8 | 0.7 |\n",
    "\n",
    "Given a new patient with both symptoms: fever and cough ($x_1 = 1, x_2 = 1$), explicitly compute posterior probabilities using Naive Bayes and classify the patient.\n",
    "\n",
    "### detailed Solution (Detailed Computations):\n",
    "\n",
    "**Step 1: Compute posterior for class $y = 0$ (Healthy) explicitly:**\n",
    "\n",
    "- Prior: $P(y = 0) = 0.9$\n",
    "- Conditional likelihoods explicitly:\n",
    "  - $P(x_1 = 1|y = 0) = 0.1$ (Fever)\n",
    "  - $P(x_2 = 1|y = 0) = 0.2$ (Cough)\n",
    "\n",
    "Compute explicitly:\n",
    "\n",
    "$$\n",
    "P(y = 0|\\mathbf{x}) \\propto P(y = 0)P(x_1 = 1|y = 0)P(x_2 = 1|y = 0) = 0.9 \\times 0.1 \\times 0.2 = 0.018\n",
    "$$\n",
    "\n",
    "**Step 2: Compute posterior for class $y = 1$ (Flu) explicitly:**\n",
    "\n",
    "- Prior: $P(y = 1) = 0.1$\n",
    "- Conditional likelihoods explicitly:\n",
    "  - $P(x_1 = 1|y = 1) = 0.8$ (Fever)\n",
    "  - $P(x_2 = 1|y = 1) = 0.7$ (Cough)\n",
    "\n",
    "Compute explicitly:\n",
    "\n",
    "$$\n",
    "P(y = 1|\\mathbf{x}) \\propto P(y = 1)P(x_1 = 1|y = 1)P(x_2 = 1|y = 1) = 0.1 \\times 0.8 \\times 0.7 = 0.056\n",
    "$$\n",
    "\n",
    "**Step 3: Normalize explicitly to get exact posterior probabilities:**\n",
    "\n",
    "- Total probability explicitly:\n",
    "\n",
    "$$\n",
    "0.018 + 0.056 = 0.074\n",
    "$$\n",
    "\n",
    "- Posterior probabilities explicitly:\n",
    "\n",
    "$$\n",
    "P(y = 0|\\mathbf{x}) = \\frac{0.018}{0.074} \\approx 0.2432\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y = 1|\\mathbf{x}) = \\frac{0.056}{0.074} \\approx 0.7568\n",
    "$$\n",
    "\n",
    "**Step 4: Decision explicitly:**\n",
    "\n",
    "$$\n",
    "0.7568 > 0.2432 \\quad \\Rightarrow \\quad \\hat{y} = 1 \\quad \\text{(Flu)}\n",
    "$$\n",
    "\n",
    "Thus, the patient is explicitly classified as having Flu.\n",
    "\n",
    "### Python Implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_y = {0: 0.9, 1: 0.1}\n",
    "P_xy = {\n",
    "    0: [0.1, 0.2],\n",
    "    1: [0.8, 0.7]\n",
    "}\n",
    "\n",
    "t = [1, 1]\n",
    "\n",
    "posteriors = {}\n",
    "for cls in P_y:\n",
    "    prob = P_y[cls]\n",
    "    for i, feature in enumerate(t):\n",
    "        prob *= P_xy[cls][i] if feature == 1 else (1 - P_xy[cls][i])\n",
    "    posteriors[cls] = prob\n",
    "\n",
    "total = sum(posteriors.values())\n",
    "for cls in posteriors:\n",
    "    posteriors[cls] /= total\n",
    "\n",
    "predicted_class = max(posteriors, key=posteriors.get)\n",
    "\n",
    "print(\"Normalized Posteriors:\", posteriors)\n",
    "print(\"Predicted:\", predicted_class, \"(Flu)\" if predicted_class==1 else \"(Healthy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python explicitly confirms our manual calculations.**\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- Although the flu is initially rare (only 10%), the presence of both symptoms (fever and cough) significantly increases the posterior probability, clearly demonstrating the power of Bayesian updating.\n",
    "- This illustrates explicitly how Naive Bayes effectively combines prior information with new evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. Cambridge University Press. Retrieved from https://d2l.ai\n",
    "   - Chapter 22.9: Naive Bayes\n",
    "   - Section 4.1: Softmax Regression\n",
    "\n",
    "[2] Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.\n",
    "   - Chapter 2: Probability Distributions (pp. 67-88)\n",
    "   - Chapter 4: Linear Models for Classification (pp. 179-224)\n",
    "\n",
    "[3] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.\n",
    "   - Chapter 3: Generative Models for Discrete Data (pp. 82-95)\n",
    "   - Chapter 8: Graphical Models (pp. 239-258)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
