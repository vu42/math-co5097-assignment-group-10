{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Probability Theory Review\n",
    "## Based on D2L Chapter 2.6: Probability\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Core Definitions and Axioms of Probability\n",
    "\n",
    "### 1.1 Sample Space and Events\n",
    "\n",
    "**Definition 1.1 (Sample Space):**\n",
    "The **sample space**, denoted $\\mathcal{S}$ (or sometimes $\\Omega$), is the set of all possible outcomes of a random experiment.\n",
    "\n",
    "**Definition 1.2 (Event):**\n",
    "An **event** $\\mathcal{A}$ is a subset of the sample space: $\\mathcal{A} \\subseteq \\mathcal{S}$.\n",
    "\n",
    "**Definition 1.3 (Event Occurrence):**\n",
    "We say that event $\\mathcal{A}$ has occurred if and only if the realized outcome $z$ satisfies $z \\in \\mathcal{A}$.\n",
    "\n",
    "**Examples:**\n",
    "- Coin flip: $\\mathcal{S} = \\{\\text{Heads}, \\text{Tails}\\}$\n",
    "- Die roll: $\\mathcal{S} = \\{1, 2, 3, 4, 5, 6\\}$\n",
    "- Temperature measurement: $\\mathcal{S} = \\mathbb{R}$\n",
    "\n",
    "**Reviewer's Commentary:**\n",
    "- These definitions are standard and correct.\n",
    "- The notation $\\mathcal{S}$ for sample space is conventional (alternative notations include $\\Omega$ or $S$).\n",
    "- Events form a $\\sigma$-algebra (discussed in Section 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Kolmogorov Axioms of Probability\n",
    "\n",
    "**Definition 1.4 (Probability Function):**\n",
    "A **probability function** (or probability measure) is a function\n",
    "$$P: \\mathcal{F} \\to [0,1]$$\n",
    "where $\\mathcal{F}$ is a $\\sigma$-algebra of events on $\\mathcal{S}$, that satisfies the following three axioms:\n",
    "\n",
    "**Axiom 1 (Non-negativity):** For any event $\\mathcal{A} \\in \\mathcal{F}$,\n",
    "$$P(\\mathcal{A}) \\geq 0$$\n",
    "\n",
    "**Axiom 2 (Normalization):** The probability of the entire sample space is 1:\n",
    "$$P(\\mathcal{S}) = 1$$\n",
    "\n",
    "**Axiom 3 (Countable Additivity):** For any countable collection of **mutually exclusive** (disjoint) events $\\{\\mathcal{A}_i\\}_{i=1}^{\\infty}$ where $\\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset$ for all $i \\neq j$,\n",
    "$$P\\left(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i\\right) = \\sum_{i=1}^{\\infty} P(\\mathcal{A}_i)$$\n",
    "\n",
    "**Reviewer's Commentary:**\n",
    "- These are the **Kolmogorov axioms**, the foundation of modern probability theory (1933).\n",
    "- **Critical requirement:** Axiom 3 requires that events be **mutually exclusive** (disjoint). This condition must be stated explicitly.\n",
    "- For finite cases, we often use the simpler finite additivity: $P(A \\cup B) = P(A) + P(B)$ when $A \\cap B = \\emptyset$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Immediate Consequences of the Axioms\n",
    "\n",
    "**Theorem 1.1 (Probability of Empty Set):**\n",
    "$$P(\\emptyset) = 0$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Consider the sample space $\\mathcal{S}$ and the empty set $\\emptyset$.\n",
    "\n",
    "**Step 2:** Note that $\\mathcal{S} \\cap \\emptyset = \\emptyset$ and $\\mathcal{S} \\cup \\emptyset = \\mathcal{S}$.\n",
    "\n",
    "*This follows from the definition of the empty set.*\n",
    "\n",
    "**Step 3:** Therefore $\\mathcal{S}$ and $\\emptyset$ are disjoint events.\n",
    "\n",
    "*This follows directly from Step 2.*\n",
    "\n",
    "**Step 4:** By Axiom 3 (countable additivity with $n=2$):\n",
    "$$P(\\mathcal{S} \\cup \\emptyset) = P(\\mathcal{S}) + P(\\emptyset)$$\n",
    "\n",
    "*The additivity axiom applies since the events are disjoint.*\n",
    "\n",
    "**Step 5:** Simplify the left side using $\\mathcal{S} \\cup \\emptyset = \\mathcal{S}$:\n",
    "$$P(\\mathcal{S}) = P(\\mathcal{S}) + P(\\emptyset)$$\n",
    "\n",
    "**Step 6:** Subtract $P(\\mathcal{S})$ from both sides:\n",
    "$$0 = P(\\emptyset)$$\n",
    "\n",
    "Therefore $P(\\emptyset) = 0$. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 1.2 (Complement Rule):**\n",
    "For any event $\\mathcal{A}$ and its complement $\\mathcal{A}^c = \\mathcal{S} \\setminus \\mathcal{A}$,\n",
    "$$P(\\mathcal{A}) + P(\\mathcal{A}^c) = 1$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** By definition of complement, $\\mathcal{A} \\cap \\mathcal{A}^c = \\emptyset$.\n",
    "\n",
    "*An event and its complement are disjoint by definition.*\n",
    "\n",
    "**Step 2:** Also by definition of complement, $\\mathcal{A} \\cup \\mathcal{A}^c = \\mathcal{S}$.\n",
    "\n",
    "*An event and its complement together partition the sample space.*\n",
    "\n",
    "**Step 3:** Since $\\mathcal{A}$ and $\\mathcal{A}^c$ are disjoint, by Axiom 3:\n",
    "$$P(\\mathcal{A} \\cup \\mathcal{A}^c) = P(\\mathcal{A}) + P(\\mathcal{A}^c)$$\n",
    "\n",
    "**Step 4:** Substitute from Step 2:\n",
    "$$P(\\mathcal{S}) = P(\\mathcal{A}) + P(\\mathcal{A}^c)$$\n",
    "\n",
    "**Step 5:** By Axiom 2, $P(\\mathcal{S}) = 1$, therefore:\n",
    "$$1 = P(\\mathcal{A}) + P(\\mathcal{A}^c)$$\n",
    "\n",
    "Therefore $P(\\mathcal{A}) + P(\\mathcal{A}^c) = 1$. $\\blacksquare$\n",
    "\n",
    "**Corollary 1.2.1:**\n",
    "$$P(\\mathcal{A}^c) = 1 - P(\\mathcal{A})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 1.3 (Monotonicity):**\n",
    "If $\\mathcal{A} \\subseteq \\mathcal{B}$, then $P(\\mathcal{A}) \\leq P(\\mathcal{B})$.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Write $\\mathcal{B} = \\mathcal{A} \\cup (\\mathcal{B} \\setminus \\mathcal{A})$.\n",
    "\n",
    "*Any set can be decomposed into a subset and its relative complement.*\n",
    "\n",
    "**Step 2:** Note that $\\mathcal{A}$ and $\\mathcal{B} \\setminus \\mathcal{A}$ are disjoint.\n",
    "\n",
    "**Step 3:** By Axiom 3:\n",
    "$$P(\\mathcal{B}) = P(\\mathcal{A}) + P(\\mathcal{B} \\setminus \\mathcal{A})$$\n",
    "\n",
    "**Step 4:** By Axiom 1, $P(\\mathcal{B} \\setminus \\mathcal{A}) \\geq 0$.\n",
    "\n",
    "**Step 5:** Therefore:\n",
    "$$P(\\mathcal{B}) = P(\\mathcal{A}) + P(\\mathcal{B} \\setminus \\mathcal{A}) \\geq P(\\mathcal{A})$$\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Random Variables\n",
    "\n",
    "### 2.1 What is a Random Variable?\n",
    "\n",
    "A random variable provides a mathematical framework for mapping outcomes from a sample space to numerical values, enabling quantitative analysis.\n",
    "\n",
    "**Definition 2.1 (Random Variable - Formal Version):**\n",
    "A **random variable** is a measurable function $X: \\mathcal{S} \\to \\mathbb{R}$ that maps outcomes from the sample space to real numbers.\n",
    "\n",
    "**Intuition:** A random variable assigns a number to each possible outcome of a random experiment.\n",
    "\n",
    "**Example 1: Coin Toss** ðŸª™\n",
    "\n",
    "Consider a coin flip experiment where the sample space is $\\mathcal{S} = \\{\\text{Heads}, \\text{Tails}\\}$.\n",
    "\n",
    "Define a random variable $X$ that maps:\n",
    "- Heads â†’ 1\n",
    "- Tails â†’ 0\n",
    "\n",
    "Now we can perform mathematical operations like computing averages.\n",
    "\n",
    "**Example 2: Rolling a Die** ðŸŽ²\n",
    "\n",
    "When rolling a die, define $X$ = \"the number showing on the top face.\"\n",
    "\n",
    "This gives us values in $\\{1, 2, 3, 4, 5, 6\\}$.\n",
    "\n",
    "**Two Types of Random Variables:**\n",
    "\n",
    "| Type | Description | Examples |\n",
    "|------|-------------|----------|\n",
    "| **Discrete** | Countable values | Coin flips, dice rolls, number of customers |\n",
    "| **Continuous** | Any value in a range | Height, weight, temperature, time |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Probability Mass Function (PMF) and Probability Density Function (PDF)\n",
    "\n",
    "**Definition 2.2 (Probability Mass Function):**\n",
    "For a discrete random variable $X$, the **probability mass function (PMF)** is:\n",
    "$$p_X(x) = P(X = x)$$\n",
    "\n",
    "**Properties of PMF:**\n",
    "1. $p_X(x) \\geq 0$ for all $x$\n",
    "2. $\\sum_{x} p_X(x) = 1$\n",
    "\n",
    "**Definition 2.3 (Probability Density Function):**\n",
    "For a continuous random variable $X$, the **probability density function (PDF)** is a function $f_X(x)$ such that:\n",
    "$$P(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx$$\n",
    "\n",
    "**Properties of PDF:**\n",
    "1. $f_X(x) \\geq 0$ for all $x$\n",
    "2. $\\int_{-\\infty}^{\\infty} f_X(x) \\, dx = 1$\n",
    "\n",
    "**Important Note:** For continuous random variables, $P(X = x) = 0$ for any specific value $x$. We can only compute probabilities for intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cumulative Distribution Function (CDF)\n",
    "\n",
    "**Definition 2.4 (Cumulative Distribution Function):**\n",
    "For any random variable $X$, the **cumulative distribution function (CDF)** is:\n",
    "$$F_X(x) = P(X \\leq x)$$\n",
    "\n",
    "**Properties of CDF:**\n",
    "1. $\\lim_{x \\to -\\infty} F_X(x) = 0$\n",
    "2. $\\lim_{x \\to +\\infty} F_X(x) = 1$\n",
    "3. $F_X$ is non-decreasing: if $a < b$, then $F_X(a) \\leq F_X(b)$\n",
    "4. $F_X$ is right-continuous\n",
    "\n",
    "**Relationship between PDF and CDF (continuous case):**\n",
    "$$f_X(x) = \\frac{d}{dx}F_X(x)$$\n",
    "$$F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt$$\n",
    "\n",
    "**Computing Probabilities:**\n",
    "$$P(a < X \\leq b) = F_X(b) - F_X(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Example: The Classic Coin Toss\n",
    "\n",
    "Let's work through the most famous probability example - flipping a fair coin.\n",
    "\n",
    "**The Setup:**\n",
    "- Fair coin with 50-50 chance of heads or tails\n",
    "- Sample space: $\\mathcal{S} = \\{\\text{Heads}, \\text{Tails}\\}$\n",
    "\n",
    "**Step 1: Define a Random Variable**\n",
    "\n",
    "Let $X = 1$ if Heads, and $X = 0$ if Tails.\n",
    "\n",
    "So:\n",
    "- $P(X = 1) = 0.5$ (probability of heads)\n",
    "- $P(X = 0) = 0.5$ (probability of tails)\n",
    "\n",
    "**Step 2: Calculate the Expected Value**\n",
    "\n",
    "What value do we expect on average if we flip many times?\n",
    "\n",
    "$$E[X] = \\sum_{x} x \\cdot P(X = x) = (1)(0.5) + (0)(0.5) = 0.5$$\n",
    "\n",
    "**Interpretation:** If we flip the coin many times and average all the results, we'll get close to 0.5.\n",
    "\n",
    "**Step 3: Calculate the Variance**\n",
    "\n",
    "How spread out are the results?\n",
    "\n",
    "Using the formula: $\\text{Var}[X] = E[X^2] - (E[X])^2$\n",
    "\n",
    "First, find $E[X^2]$:\n",
    "$$E[X^2] = (1^2)(0.5) + (0^2)(0.5) = 0.5$$\n",
    "\n",
    "Then:\n",
    "$$\\text{Var}[X] = 0.5 - (0.5)^2 = 0.5 - 0.25 = 0.25$$\n",
    "\n",
    "Standard deviation:\n",
    "$$\\sigma = \\sqrt{0.25} = 0.5$$\n",
    "\n",
    "**Interpretation:** The results are quite spread out since you either get 0 or 1, nothing in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Example: Flipping Two Coins\n",
    "\n",
    "**Sample space:** $\\mathcal{S} = \\{HH, HT, TH, TT\\}$\n",
    "\n",
    "Let $Y$ = total number of heads. Possible values: 0, 1, or 2.\n",
    "\n",
    "**Probabilities:**\n",
    "- $P(Y = 0) = 1/4$ (both tails: TT)\n",
    "- $P(Y = 1) = 2/4 = 1/2$ (one head: HT or TH)\n",
    "- $P(Y = 2) = 1/4$ (both heads: HH)\n",
    "\n",
    "**Expected number of heads:**\n",
    "$$E[Y] = (0)(1/4) + (1)(1/2) + (2)(1/4) = 0 + 0.5 + 0.5 = 1$$\n",
    "\n",
    "With 2 coins, on average you get 1 head.\n",
    "\n",
    "**Variance:**\n",
    "$$E[Y^2] = (0^2)(1/4) + (1^2)(1/2) + (2^2)(1/4) = 0 + 0.5 + 1 = 1.5$$\n",
    "\n",
    "$$\\text{Var}[Y] = 1.5 - (1)^2 = 1.5 - 1 = 0.5$$\n",
    "\n",
    "**Pattern:** For 1 coin, $E[X] = 0.5$ and for 2 coins, $E[Y] = 1 = 2 \\times 0.5$. Expectations add up, which demonstrates the **linearity of expectation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 More Real-Life Examples\n",
    "\n",
    "#### Example 1: Weather Forecasting\n",
    "\n",
    "When the weather person says \"30% chance of rain tomorrow,\" what does that mean?\n",
    "\n",
    "**Sample Space:** $\\mathcal{S} = \\{\\text{Rain}, \\text{No Rain}\\}$\n",
    "\n",
    "**Probabilities:**\n",
    "- $P(\\text{Rain}) = 0.3$\n",
    "- $P(\\text{No Rain}) = 0.7$\n",
    "\n",
    "Let $X = 1$ if it rains, $X = 0$ if it doesn't.\n",
    "\n",
    "**Expected value:**\n",
    "$$E[X] = (1)(0.3) + (0)(0.7) = 0.3$$\n",
    "\n",
    "**Interpretation:** If there were 100 days exactly like tomorrow, it would rain on about 30 of them. There's a 70% chance you won't need an umbrella, and 30% chance you will.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 2: Exam Scores\n",
    "\n",
    "Suppose your exam performance follows this distribution:\n",
    "- Ace it (90-100): 40% probability\n",
    "- Do okay (70-89): 50% probability\n",
    "- Barely pass (60-69): 10% probability\n",
    "\n",
    "Using the midpoint of each range:\n",
    "\n",
    "$$E[\\text{Score}] = (95)(0.4) + (80)(0.5) + (65)(0.1) = 38 + 40 + 6.5 = 84.5$$\n",
    "\n",
    "**Expected score: 84.5** - That's a solid B!\n",
    "\n",
    "**Variance calculation:**\n",
    "$$E[\\text{Score}^2] = (95^2)(0.4) + (80^2)(0.5) + (65^2)(0.1) = 3610 + 3200 + 422.5 = 7232.5$$\n",
    "\n",
    "$$\\text{Var}[\\text{Score}] = 7232.5 - (84.5)^2 = 7232.5 - 7140.25 = 92.25$$\n",
    "\n",
    "$$\\sigma = \\sqrt{92.25} \\approx 9.6$$\n",
    "\n",
    "**Interpretation:** Your scores vary by about 10 points either way. Even though you expect an 84.5, you might get anywhere from 75 to 95!\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 3: Gaming Loot Boxes\n",
    "\n",
    "A loot box in a game has:\n",
    "- Common item (worth \\$1): 70% chance\n",
    "- Rare item (worth \\$5): 25% chance\n",
    "- Legendary item (worth \\$50): 5% chance\n",
    "\n",
    "**Expected value of a loot box:**\n",
    "$$E[X] = (1)(0.70) + (5)(0.25) + (50)(0.05) = 0.70 + 1.25 + 2.50 = 4.45$$\n",
    "\n",
    "Each box is worth \\$4.45 on average. **If the box costs \\$5 to buy, you're losing money!** About \\$0.55 per box on average.\n",
    "\n",
    "**Checking the variance:**\n",
    "$$E[X^2] = (1^2)(0.70) + (5^2)(0.25) + (50^2)(0.05) = 0.70 + 6.25 + 125 = 131.95$$\n",
    "\n",
    "$$\\text{Var}[X] = 131.95 - (4.45)^2 = 131.95 - 19.80 = 112.15$$\n",
    "\n",
    "$$\\sigma = \\sqrt{112.15} \\approx 10.59$$\n",
    "\n",
    "**Huge variance!** Most of the time you'll get a \\$1 item, but occasionally you might hit that \\$50 legendary.\n",
    "\n",
    "**Lesson:** Expected value tells you the average, but variance tells you the RISK. High variance = high unpredictability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Joint and Conditional Probability\n",
    "\n",
    "### 3.1 Joint Probability\n",
    "\n",
    "**Definition 3.1 (Joint Probability):**\n",
    "For two random variables $A$ and $B$, the **joint probability** is:\n",
    "$$P(A = a, B = b)$$\n",
    "which represents the probability that both events $\\{A = a\\}$ and $\\{B = b\\}$ occur simultaneously.\n",
    "\n",
    "**Alternative notation:** $P(A = a \\cap B = b)$ or $P(A = a \\text{ and } B = b)$\n",
    "\n",
    "**Theorem 3.1 (Joint Probability Bounds):**\n",
    "For any values $a, b$:\n",
    "$$P(A = a, B = b) \\leq P(A = a)$$\n",
    "$$P(A = a, B = b) \\leq P(B = b)$$\n",
    "\n",
    "**Proof of first inequality:**\n",
    "\n",
    "**Step 1:** Partition the event $\\{A = a\\}$ based on all possible values of $B$:\n",
    "$$\\{A = a\\} = \\bigcup_{v \\in \\text{Val}(B)} \\{A = a, B = v\\}$$\n",
    "\n",
    "*This follows from the law of total probability.*\n",
    "\n",
    "**Step 2:** The events $\\{A = a, B = v\\}$ for different values of $v$ are mutually exclusive.\n",
    "\n",
    "*$B$ cannot take two different values simultaneously.*\n",
    "\n",
    "**Step 3:** Apply Axiom 3 (countable additivity):\n",
    "$$P(A = a) = \\sum_{v \\in \\text{Val}(B)} P(A = a, B = v)$$\n",
    "\n",
    "**Step 4:** Since all probabilities are non-negative (Axiom 1), each term in the sum is $\\geq 0$:\n",
    "$$P(A = a) = P(A = a, B = b) + \\sum_{v \\neq b} P(A = a, B = v) \\geq P(A = a, B = b)$$\n",
    "\n",
    "Therefore $P(A = a, B = b) \\leq P(A = a)$. The proof for the second inequality is symmetric. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Conditional Probability\n",
    "\n",
    "**Definition 3.2 (Conditional Probability):**\n",
    "The **conditional probability** of event $B = b$ given that event $A = a$ has occurred is defined as:\n",
    "$$P(B = b \\mid A = a) = \\frac{P(A = a, B = b)}{P(A = a)}$$\n",
    "provided that $P(A = a) > 0$.\n",
    "\n",
    "**Interpretation:**\n",
    "- We restrict our sample space to outcomes where $A = a$ occurred\n",
    "- We renormalize probabilities so they sum to 1 over this restricted space\n",
    "- The conditional probability measures the likelihood of $B = b$ within this restricted space\n",
    "\n",
    "**Important Condition:** This definition is only valid when $P(A = a) > 0$. If $P(A = a) = 0$, conditional probability is undefined (division by zero).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Roll a fair die. Let:\n",
    "- $A$ = \"roll is even\" = $\\{2, 4, 6\\}$\n",
    "- $B$ = \"roll is greater than 3\" = $\\{4, 5, 6\\}$\n",
    "\n",
    "Then:\n",
    "- $P(A) = 3/6 = 1/2$\n",
    "- $P(B) = 3/6 = 1/2$\n",
    "- $P(A \\cap B) = P(\\{4, 6\\}) = 2/6 = 1/3$\n",
    "\n",
    "$$P(B \\mid A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{1/3}{1/2} = \\frac{2}{3}$$\n",
    "\n",
    "Given that the roll is even, there's a 2/3 chance it's greater than 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Verification that Conditional Probability is a Valid Probability\n",
    "\n",
    "**Theorem 3.2:** For fixed $A = a$ with $P(A = a) > 0$, the function $Q(B = b) = P(B = b \\mid A = a)$ satisfies all probability axioms.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Axiom 1 (Non-negativity):**\n",
    "\n",
    "**Step 1:** By definition:\n",
    "$$Q(B = b) = P(B = b \\mid A = a) = \\frac{P(A = a, B = b)}{P(A = a)}$$\n",
    "\n",
    "**Step 2:** The numerator satisfies $P(A = a, B = b) \\geq 0$ (by Axiom 1).\n",
    "\n",
    "**Step 3:** The denominator satisfies $P(A = a) > 0$ (given assumption).\n",
    "\n",
    "**Step 4:** Therefore:\n",
    "$$Q(B = b) = \\frac{P(A = a, B = b)}{P(A = a)} \\geq 0$$\n",
    "\n",
    "**Axiom 2 (Normalization):**\n",
    "\n",
    "**Step 1:** Sum over all possible values of $B$:\n",
    "$$\\sum_{b} Q(B = b) = \\sum_{b} \\frac{P(A = a, B = b)}{P(A = a)}$$\n",
    "\n",
    "**Step 2:** Factor out constant denominator:\n",
    "$$= \\frac{1}{P(A = a)} \\sum_{b} P(A = a, B = b)$$\n",
    "\n",
    "**Step 3:** By marginalization:\n",
    "$$= \\frac{1}{P(A = a)} \\cdot P(A = a) = 1$$\n",
    "\n",
    "**Axiom 3 (Additivity):** Similar verification for disjoint events.\n",
    "\n",
    "Therefore, conditional probability defines a valid probability measure. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Marginalization (Law of Total Probability)\n",
    "\n",
    "**Theorem 3.3 (Marginalization):**\n",
    "For random variables $A$ and $B$:\n",
    "$$P(A = a) = \\sum_{v \\in \\text{Val}(B)} P(A = a, B = v)$$\n",
    "\n",
    "**Proof:** This was proven in Theorem 3.1.\n",
    "\n",
    "**Theorem 3.4 (Law of Total Probability - Alternative Form):**\n",
    "$$P(A = a) = \\sum_{v \\in \\text{Val}(B)} P(A = a \\mid B = v) P(B = v)$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Start with the marginalization formula:\n",
    "$$P(A = a) = \\sum_{v} P(A = a, B = v)$$\n",
    "\n",
    "**Step 2:** Apply the definition of conditional probability to each term:\n",
    "$$P(A = a, B = v) = P(A = a \\mid B = v) P(B = v)$$\n",
    "\n",
    "*This assumes $P(B = v) > 0$.*\n",
    "\n",
    "**Step 3:** Substitute into Step 1:\n",
    "$$P(A = a) = \\sum_{v} P(A = a \\mid B = v) P(B = v)$$\n",
    "\n",
    "**Note:** For values $v$ where $P(B = v) = 0$, the term contributes 0 to the sum. $\\blacksquare$\n",
    "\n",
    "**Intuition:** To find the probability of $A$, sum over all possible \"paths\" through values of $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Bayes' Theorem\n",
    "\n",
    "### 4.1 Derivation of Bayes' Theorem\n",
    "\n",
    "**Theorem 4.1 (Bayes' Theorem - Basic Form):**\n",
    "For events $A$ and $B$ with $P(A) > 0$ and $P(B) > 0$:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** By definition of conditional probability:\n",
    "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "*This is valid when $P(B) > 0$.*\n",
    "\n",
    "**Step 2:** Note that joint probability is symmetric:\n",
    "$$P(A, B) = P(B, A)$$\n",
    "\n",
    "*$P(A \\cap B) = P(B \\cap A)$.*\n",
    "\n",
    "**Step 3:** Apply definition of conditional probability to express joint probability:\n",
    "$$P(B, A) = P(B \\mid A) P(A)$$\n",
    "\n",
    "*This is valid when $P(A) > 0$.*\n",
    "\n",
    "**Step 4:** Combine Steps 2 and 3:\n",
    "$$P(A, B) = P(B \\mid A) P(A)$$\n",
    "\n",
    "**Step 5:** Substitute Step 4 into Step 1:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\n",
    "\n",
    "Therefore Bayes' theorem is established. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Terminology and Interpretation\n",
    "\n",
    "In Bayes' theorem:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "| Term | Name | Meaning |\n",
    "|------|------|--------|\n",
    "| $P(A)$ | **Prior** | What we believed BEFORE seeing evidence |\n",
    "| $P(B \\mid A)$ | **Likelihood** | How likely is the evidence if our belief is true? |\n",
    "| $P(B)$ | **Marginal / Evidence** | How likely is the evidence overall? |\n",
    "| $P(A \\mid B)$ | **Posterior** | What we believe AFTER seeing evidence |\n",
    "\n",
    "**The Big Idea:** Bayes' theorem tells us how to update our beliefs when we get new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Bayes' Theorem with Marginalization\n",
    "\n",
    "**Theorem 4.2 (Bayes' Theorem - Normalized Form):**\n",
    "When $P(B)$ is unknown, we can compute it via marginalization:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{\\sum_{a'} P(B \\mid A = a') P(A = a')}$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Start with Bayes' theorem:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\n",
    "\n",
    "**Step 2:** Apply marginalization to compute $P(B)$:\n",
    "$$P(B) = \\sum_{a'} P(B, A = a') = \\sum_{a'} P(B \\mid A = a') P(A = a')$$\n",
    "\n",
    "**Step 3:** Substitute into Step 1:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{\\sum_{a'} P(B \\mid A = a') P(A = a')}$$\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Proportional Form of Bayes' Theorem\n",
    "\n",
    "**Theorem 4.3 (Bayes' Theorem - Proportional Form):**\n",
    "$$P(A \\mid B) \\propto P(B \\mid A) P(A)$$\n",
    "\n",
    "**Explanation:** The proportionality symbol $\\propto$ means \"proportional to\" or \"equal up to a normalization constant.\"\n",
    "\n",
    "**Detailed Meaning:**\n",
    "\n",
    "**Step 1:** From Bayes' theorem:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\n",
    "\n",
    "**Step 2:** When computing $P(A \\mid B)$ for different values of $A$ with $B$ fixed, the denominator $P(B)$ is constant.\n",
    "\n",
    "**Step 3:** Therefore:\n",
    "$$P(A \\mid B) = \\frac{1}{P(B)} \\cdot P(B \\mid A) P(A) \\propto P(B \\mid A) P(A)$$\n",
    "\n",
    "**Step 4:** To recover the full probability, we normalize:\n",
    "$$P(A = a \\mid B) = \\frac{P(B \\mid A = a) P(A = a)}{\\sum_{a'} P(B \\mid A = a') P(A = a')}$$\n",
    "\n",
    "**Usage:** This form is useful when we only need to compare relative probabilities or when we'll normalize at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Real-World Example: The Unreliable Friend\n",
    "\n",
    "**Situation:** Your friend says \"I'm coming to your party!\" But you know from experience:\n",
    "- When they actually come, they ALWAYS say they're coming: $P(\\text{Says Yes} \\mid \\text{Comes}) = 1.0$\n",
    "- But they only actually show up 30% of the time: $P(\\text{Comes}) = 0.3$\n",
    "- They say \"yes\" 80% of the time: $P(\\text{Says Yes}) = 0.8$\n",
    "\n",
    "**Question:** Given that they said yes, what's the probability they'll actually come?\n",
    "\n",
    "**Using Bayes' theorem:**\n",
    "$$P(\\text{Comes} \\mid \\text{Says Yes}) = \\frac{P(\\text{Says Yes} \\mid \\text{Comes}) \\cdot P(\\text{Comes})}{P(\\text{Says Yes})}$$\n",
    "\n",
    "$$= \\frac{(1.0)(0.3)}{0.8} = \\frac{0.3}{0.8} = 0.375 = 37.5\\%$$\n",
    "\n",
    "**Result:** Even though they said yes, there's only a 37.5% chance they'll actually come!\n",
    "\n",
    "**Why?** Because your friend says \"yes\" a LOT (80% of the time), but only shows up 30% of the time. So \"saying yes\" doesn't mean much!\n",
    "\n",
    "---\n",
    "\n",
    "### 4.6 Real-World Example: Email Spam Filter\n",
    "\n",
    "Your email has these characteristics:\n",
    "- **Prior:** 20% of all emails are spam: $P(\\text{Spam}) = 0.2$\n",
    "- **Likelihood:** If it's spam, there's a 90% chance it contains \"FREE MONEY!!!\": $P(\\text{FREE MONEY} \\mid \\text{Spam}) = 0.9$\n",
    "- **Marginal:** Overall, 25% of emails contain \"FREE MONEY\": $P(\\text{FREE MONEY}) = 0.25$\n",
    "\n",
    "You get an email with \"FREE MONEY!!!\" - is it spam?\n",
    "\n",
    "$$P(\\text{Spam} \\mid \\text{FREE MONEY}) = \\frac{(0.9)(0.2)}{0.25} = \\frac{0.18}{0.25} = 0.72 = 72\\%$$\n",
    "\n",
    "**Result:** There's a 72% chance it's spam. Your filter flags it!\n",
    "\n",
    "**Why this works:**\n",
    "1. We STARTED with \"20% of emails are spam\" (prior)\n",
    "2. We SAW evidence: \"FREE MONEY!!!\"\n",
    "3. We UPDATED our belief to \"72% likely spam\" (posterior)\n",
    "\n",
    "**This is exactly how spam filters, medical diagnosis, and AI work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Independence\n",
    "\n",
    "### 5.1 Independence of Events\n",
    "\n",
    "**Definition 5.1 (Independence):**\n",
    "Two random variables $A$ and $B$ are **independent** (denoted $A \\perp B$) if and only if:\n",
    "$$P(A, B) = P(A) P(B)$$\n",
    "for all values of $A$ and $B$.\n",
    "\n",
    "**Theorem 5.1 (Equivalent Characterization):**\n",
    "If $P(B) > 0$, then $A \\perp B$ if and only if:\n",
    "$$P(A \\mid B) = P(A)$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Direction 1: Independence implies conditional equals marginal**\n",
    "\n",
    "**Step 1:** Assume $A \\perp B$, so $P(A, B) = P(A) P(B)$.\n",
    "\n",
    "**Step 2:** By definition of conditional probability:\n",
    "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "**Step 3:** Substitute independence:\n",
    "$$P(A \\mid B) = \\frac{P(A) P(B)}{P(B)}$$\n",
    "\n",
    "**Step 4:** Cancel $P(B)$ (valid since $P(B) > 0$):\n",
    "$$P(A \\mid B) = P(A)$$\n",
    "\n",
    "**Direction 2: Conditional equals marginal implies independence**\n",
    "\n",
    "**Step 1:** Assume $P(A \\mid B) = P(A)$.\n",
    "\n",
    "**Step 2:** By definition of conditional probability:\n",
    "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "**Step 3:** Substitute assumption:\n",
    "$$P(A) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "**Step 4:** Multiply both sides by $P(B)$:\n",
    "$$P(A) P(B) = P(A, B)$$\n",
    "\n",
    "Therefore $A \\perp B$. $\\blacksquare$\n",
    "\n",
    "**Interpretation:** Independence means that knowing $B$ provides no information about $A$ - the conditional probability equals the unconditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Conditional Independence\n",
    "\n",
    "**Definition 5.2 (Conditional Independence):**\n",
    "Random variables $A$ and $B$ are **conditionally independent given $C$** (denoted $A \\perp B \\mid C$) if and only if:\n",
    "$$P(A, B \\mid C) = P(A \\mid C) P(B \\mid C)$$\n",
    "for all values of $A$, $B$, and $C$ (where $P(C) > 0$).\n",
    "\n",
    "**Theorem 5.2 (Equivalent Characterization):**\n",
    "If $P(B, C) > 0$, then $A \\perp B \\mid C$ if and only if:\n",
    "$$P(A \\mid B, C) = P(A \\mid C)$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Direction 1:**\n",
    "\n",
    "**Step 1:** Assume $A \\perp B \\mid C$, so $P(A, B \\mid C) = P(A \\mid C) P(B \\mid C)$.\n",
    "\n",
    "**Step 2:** By definition of conditional probability:\n",
    "$$P(A \\mid B, C) = \\frac{P(A, B \\mid C)}{P(B \\mid C)}$$\n",
    "\n",
    "**Step 3:** Substitute conditional independence:\n",
    "$$P(A \\mid B, C) = \\frac{P(A \\mid C) P(B \\mid C)}{P(B \\mid C)} = P(A \\mid C)$$\n",
    "\n",
    "**Direction 2:** Similar (reverse the steps). $\\blacksquare$\n",
    "\n",
    "**Important Notes:**\n",
    "- Variables can be **marginally independent** ($A \\perp B$) but **conditionally dependent** ($A \\not\\perp B \\mid C$)\n",
    "- Variables can be **marginally dependent** ($A \\not\\perp B$) but **conditionally independent** ($A \\perp B \\mid C$)\n",
    "- Example of first case: Common effect (explaining away)\n",
    "- Example of second case: Common cause (confounding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Expectation and Variance\n",
    "\n",
    "### 6.1 Expectation (Expected Value)\n",
    "\n",
    "**Definition 6.1 (Expectation - Discrete Case):**\n",
    "For a discrete random variable $X$ with probability mass function $P(X = x)$, the **expectation** (or expected value, or mean) is:\n",
    "$$E[X] = \\sum_{x} x \\cdot P(X = x)$$\n",
    "where the sum is over all possible values of $X$.\n",
    "\n",
    "**Conditions for existence:** The expectation exists if and only if $\\sum_{x} |x| \\cdot P(X = x) < \\infty$.\n",
    "\n",
    "**Alternative Notation:** $E[X] = E_{X \\sim P}[X] = \\mu_X = \\mu$\n",
    "\n",
    "**Definition 6.2 (Expectation of Function):**\n",
    "For a function $f: \\mathbb{R} \\to \\mathbb{R}$ and discrete random variable $X$:\n",
    "$$E_{X \\sim P}[f(X)] = \\sum_{x} f(x) \\cdot P(X = x)$$\n",
    "\n",
    "**Definition 6.3 (Expectation - Continuous Case):**\n",
    "For a continuous random variable $X$ with probability density function $f(x)$:\n",
    "$$E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx$$\n",
    "\n",
    "For a function $g$:\n",
    "$$E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) \\cdot f(x) \\, dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Properties of Expectation\n",
    "\n",
    "**Theorem 6.1 (Linearity of Expectation):**\n",
    "For random variables $X$ and $Y$ and constants $a, b \\in \\mathbb{R}$:\n",
    "$$E[aX + bY] = aE[X] + bE[Y]$$\n",
    "\n",
    "**Proof (Discrete Case):**\n",
    "\n",
    "**Step 1:** Write the expectation:\n",
    "$$E[aX + bY] = \\sum_{x, y} (ax + by) \\cdot P(X = x, Y = y)$$\n",
    "\n",
    "**Step 2:** Distribute:\n",
    "$$= \\sum_{x, y} ax \\cdot P(X = x, Y = y) + \\sum_{x, y} by \\cdot P(X = x, Y = y)$$\n",
    "\n",
    "**Step 3:** Factor out constants:\n",
    "$$= a \\sum_{x, y} x \\cdot P(X = x, Y = y) + b \\sum_{x, y} y \\cdot P(X = x, Y = y)$$\n",
    "\n",
    "**Step 4:** For the first term, marginalize over $y$:\n",
    "$$\\sum_{x, y} x \\cdot P(X = x, Y = y) = \\sum_{x} x \\sum_{y} P(X = x, Y = y) = \\sum_{x} x \\cdot P(X = x) = E[X]$$\n",
    "\n",
    "**Step 5:** Similarly for the second term:\n",
    "$$\\sum_{x, y} y \\cdot P(X = x, Y = y) = E[Y]$$\n",
    "\n",
    "**Step 6:** Combine:\n",
    "$$E[aX + bY] = aE[X] + bE[Y]$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Important Note:** Linearity holds **regardless of whether $X$ and $Y$ are independent**. This is a powerful property.\n",
    "\n",
    "**Corollary 6.1.1:** For constants $a$ and $b$:\n",
    "- $E[aX] = aE[X]$\n",
    "- $E[X + b] = E[X] + b$\n",
    "- $E[a] = a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Variance\n",
    "\n",
    "**Definition 6.4 (Variance):**\n",
    "The **variance** of a random variable $X$ measures the spread of its distribution around the mean:\n",
    "$$\\text{Var}[X] = E\\left[(X - E[X])^2\\right]$$\n",
    "\n",
    "**Alternative Notation:** $\\text{Var}[X] = \\sigma_X^2 = \\sigma^2$\n",
    "\n",
    "**Theorem 6.2 (Computational Formula for Variance):**\n",
    "$$\\text{Var}[X] = E[X^2] - (E[X])^2$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Start with definition:\n",
    "$$\\text{Var}[X] = E\\left[(X - E[X])^2\\right]$$\n",
    "\n",
    "**Step 2:** Let $\\mu = E[X]$ for notational simplicity:\n",
    "$$\\text{Var}[X] = E\\left[(X - \\mu)^2\\right]$$\n",
    "\n",
    "**Step 3:** Expand the square:\n",
    "$$= E\\left[X^2 - 2\\mu X + \\mu^2\\right]$$\n",
    "\n",
    "**Step 4:** Apply linearity of expectation:\n",
    "$$= E[X^2] - E[2\\mu X] + E[\\mu^2]$$\n",
    "\n",
    "**Step 5:** Factor out constants:\n",
    "$$= E[X^2] - 2\\mu E[X] + \\mu^2$$\n",
    "\n",
    "**Step 6:** Substitute $\\mu = E[X]$:\n",
    "$$= E[X^2] - 2E[X] \\cdot E[X] + (E[X])^2$$\n",
    "\n",
    "**Step 7:** Simplify:\n",
    "$$= E[X^2] - 2(E[X])^2 + (E[X])^2$$\n",
    "\n",
    "**Step 8:** Combine like terms:\n",
    "$$= E[X^2] - (E[X])^2$$\n",
    "\n",
    "Therefore $\\text{Var}[X] = E[X^2] - (E[X])^2$. $\\blacksquare$\n",
    "\n",
    "**Interpretation:** Variance is the difference between the \"mean of squares\" and \"square of mean.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Properties of Variance\n",
    "\n",
    "**Theorem 6.3 (Variance of Scaled Random Variable):**\n",
    "For any constant $c$:\n",
    "$$\\text{Var}[cX] = c^2 \\text{Var}[X]$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Use the computational formula:\n",
    "$$\\text{Var}[cX] = E[(cX)^2] - (E[cX])^2$$\n",
    "\n",
    "**Step 2:** Simplify:\n",
    "$$= E[c^2 X^2] - (cE[X])^2$$\n",
    "$$= c^2 E[X^2] - c^2 (E[X])^2$$\n",
    "$$= c^2 (E[X^2] - (E[X])^2)$$\n",
    "$$= c^2 \\text{Var}[X]$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Theorem 6.4 (Variance is Shift-Invariant):**\n",
    "For any constant $c$:\n",
    "$$\\text{Var}[X + c] = \\text{Var}[X]$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Note that $E[X + c] = E[X] + c$.\n",
    "\n",
    "**Step 2:** Apply definition:\n",
    "$$\\text{Var}[X + c] = E[(X + c - E[X + c])^2] = E[(X + c - E[X] - c)^2] = E[(X - E[X])^2] = \\text{Var}[X]$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Theorem 6.5 (Variance of Sum - Independent Case):**\n",
    "If $X$ and $Y$ are **independent**, then:\n",
    "$$\\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y]$$\n",
    "\n",
    "**Note:** This does NOT hold in general for dependent random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Standard Deviation\n",
    "\n",
    "**Definition 6.5 (Standard Deviation):**\n",
    "The **standard deviation** is the square root of variance:\n",
    "$$\\sigma_X = \\sqrt{\\text{Var}[X]}$$\n",
    "\n",
    "**Purpose:** Standard deviation is expressed in the same units as the original random variable, making it more interpretable than variance.\n",
    "\n",
    "**Example:** If $X$ represents height in centimeters:\n",
    "- $\\text{Var}[X]$ has units of cmÂ² (squared centimeters)\n",
    "- $\\sigma_X$ has units of cm (centimeters)\n",
    "\n",
    "**Properties:**\n",
    "- $\\sigma_X \\geq 0$\n",
    "- $\\sigma_{cX} = |c| \\sigma_X$\n",
    "- $\\sigma_{X+c} = \\sigma_X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Vector-Valued Random Variables\n",
    "\n",
    "### 7.1 Expectation of Random Vectors\n",
    "\n",
    "**Definition 7.1 (Random Vector):**\n",
    "A **random vector** $\\mathbf{x} \\in \\mathbb{R}^n$ is a vector whose components are random variables:\n",
    "$$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$$\n",
    "\n",
    "**Definition 7.2 (Expectation of Random Vector):**\n",
    "The expectation of a random vector is computed component-wise:\n",
    "$$\\boldsymbol{\\mu} = E[\\mathbf{x}] = \\begin{bmatrix} E[x_1] \\\\ E[x_2] \\\\ \\vdots \\\\ E[x_n] \\end{bmatrix}$$\n",
    "\n",
    "More explicitly:\n",
    "$$\\mu_i = E[x_i]$$\n",
    "for $i = 1, 2, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Covariance and Covariance Matrix\n",
    "\n",
    "**Definition 7.3 (Covariance):**\n",
    "The **covariance** between two random variables $X$ and $Y$ is:\n",
    "$$\\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]$$\n",
    "\n",
    "**Properties:**\n",
    "- $\\text{Cov}(X, X) = \\text{Var}(X)$\n",
    "- $\\text{Cov}(X, Y) = \\text{Cov}(Y, X)$ (symmetric)\n",
    "- If $X \\perp Y$, then $\\text{Cov}(X, Y) = 0$\n",
    "\n",
    "**Definition 7.4 (Covariance Matrix):**\n",
    "For a random vector $\\mathbf{x} \\in \\mathbb{R}^n$ with mean $\\boldsymbol{\\mu} = E[\\mathbf{x}]$, the **covariance matrix** is:\n",
    "$$\\boldsymbol{\\Sigma} = \\text{Cov}[\\mathbf{x}] = E\\left[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^T\\right]$$\n",
    "\n",
    "**Expanded Form:**\n",
    "$$\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n",
    "\\text{Var}[x_1] & \\text{Cov}[x_1, x_2] & \\cdots & \\text{Cov}[x_1, x_n] \\\\\n",
    "\\text{Cov}[x_2, x_1] & \\text{Var}[x_2] & \\cdots & \\text{Cov}[x_2, x_n] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}[x_n, x_1] & \\text{Cov}[x_n, x_2] & \\cdots & \\text{Var}[x_n]\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Properties:**\n",
    "- Diagonal elements are variances: $\\Sigma_{ii} = \\text{Var}[x_i]$\n",
    "- Off-diagonal elements are covariances: $\\Sigma_{ij} = \\text{Cov}[x_i, x_j]$\n",
    "- The matrix is symmetric: $\\Sigma_{ij} = \\Sigma_{ji}$\n",
    "- The matrix is positive semi-definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Variance of Linear Combinations\n",
    "\n",
    "**Theorem 7.1 (Variance of Linear Combination):**\n",
    "For any constant vector $\\mathbf{v} \\in \\mathbb{R}^n$ and random vector $\\mathbf{x}$:\n",
    "$$\\text{Var}[\\mathbf{v}^T \\mathbf{x}] = \\mathbf{v}^T \\boldsymbol{\\Sigma} \\mathbf{v}$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Let $Y = \\mathbf{v}^T \\mathbf{x}$ be a scalar random variable.\n",
    "\n",
    "**Step 2:** Compute the mean of $Y$:\n",
    "$$E[Y] = E[\\mathbf{v}^T \\mathbf{x}] = \\mathbf{v}^T E[\\mathbf{x}] = \\mathbf{v}^T \\boldsymbol{\\mu}$$\n",
    "\n",
    "**Step 3:** Therefore:\n",
    "$$Y - E[Y] = \\mathbf{v}^T \\mathbf{x} - \\mathbf{v}^T \\boldsymbol{\\mu} = \\mathbf{v}^T (\\mathbf{x} - \\boldsymbol{\\mu})$$\n",
    "\n",
    "**Step 4:** Compute variance:\n",
    "$$\\text{Var}[Y] = E\\left[(Y - E[Y])^2\\right] = E\\left[(\\mathbf{v}^T (\\mathbf{x} - \\boldsymbol{\\mu}))^2\\right]$$\n",
    "\n",
    "**Step 5:** Note that $(\\mathbf{v}^T (\\mathbf{x} - \\boldsymbol{\\mu}))^2 = \\mathbf{v}^T (\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{v}$:\n",
    "$$= E\\left[\\mathbf{v}^T (\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{v}\\right]$$\n",
    "\n",
    "**Step 6:** Factor out constants:\n",
    "$$= \\mathbf{v}^T E\\left[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^T\\right] \\mathbf{v}$$\n",
    "\n",
    "**Step 7:** Recognize the covariance matrix:\n",
    "$$= \\mathbf{v}^T \\boldsymbol{\\Sigma} \\mathbf{v}$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Important Consequence:** Since variance is always non-negative, we have $\\mathbf{v}^T \\boldsymbol{\\Sigma} \\mathbf{v} \\geq 0$ for all $\\mathbf{v}$, which proves that $\\boldsymbol{\\Sigma}$ is positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Covariance and Independence\n",
    "\n",
    "**Theorem 7.2 (Independence Implies Zero Covariance):**\n",
    "If $X \\perp Y$ (i.e., $X$ and $Y$ are independent), then $\\text{Cov}(X, Y) = 0$.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** By definition of covariance:\n",
    "$$\\text{Cov}(X, Y) = E[XY] - E[X]E[Y]$$\n",
    "\n",
    "**Step 2:** If $X \\perp Y$, then:\n",
    "$$E[XY] = E[X]E[Y]$$\n",
    "\n",
    "**Step 3:** Substitute into Step 1:\n",
    "$$\\text{Cov}(X, Y) = E[X]E[Y] - E[X]E[Y] = 0$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Important Note:** The converse is NOT true in general. Zero covariance does not imply independence.\n",
    "\n",
    "**Counterexample:** Let $X \\sim \\text{Uniform}(-1, 1)$ and $Y = X^2$. Then:\n",
    "- $E[X] = 0$\n",
    "- $E[XY] = E[X \\cdot X^2] = E[X^3] = 0$ (odd function)\n",
    "- $\\text{Cov}(X, Y) = E[XY] - E[X]E[Y] = 0 - 0 = 0$\n",
    "\n",
    "But $X$ and $Y$ are clearly dependent (knowing $X$ determines $Y$ exactly)!\n",
    "\n",
    "**Exception:** For jointly Gaussian random variables, zero covariance DOES imply independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Practical Examples\n",
    "\n",
    "### 8.1 HIV Test Example - Complete Calculation\n",
    "\n",
    "**Problem Setup:**\n",
    "\n",
    "**Given Information:**\n",
    "- Let $H = 1$ denote \"has HIV\", $H = 0$ denote \"does not have HIV\"\n",
    "- Let $D_1 = 1$ denote \"first test is positive\", $D_1 = 0$ denote \"first test is negative\"\n",
    "- Prior probability of having HIV: $P(H = 1) = 0.0015$\n",
    "- Consequently: $P(H = 0) = 1 - 0.0015 = 0.9985$\n",
    "- Test sensitivity (true positive rate): $P(D_1 = 1 \\mid H = 1) = 1.0$\n",
    "- False positive rate: $P(D_1 = 1 \\mid H = 0) = 0.01$\n",
    "\n",
    "**Question:** What is the probability of having HIV given a positive test result, i.e., $P(H = 1 \\mid D_1 = 1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1 Solution - Single Test\n",
    "\n",
    "**Step 1:** Apply Bayes' theorem:\n",
    "$$P(H = 1 \\mid D_1 = 1) = \\frac{P(D_1 = 1 \\mid H = 1) P(H = 1)}{P(D_1 = 1)}$$\n",
    "\n",
    "**Step 2:** Compute $P(D_1 = 1)$ using marginalization:\n",
    "$$P(D_1 = 1) = P(D_1 = 1 \\mid H = 0) P(H = 0) + P(D_1 = 1 \\mid H = 1) P(H = 1)$$\n",
    "\n",
    "**Step 3:** Substitute values:\n",
    "$$P(D_1 = 1) = (0.01)(0.9985) + (1.0)(0.0015)$$\n",
    "\n",
    "**Step 4:** Compute:\n",
    "$$P(D_1 = 1) = 0.009985 + 0.0015 = 0.011485$$\n",
    "\n",
    "**Step 5:** Compute the numerator:\n",
    "$$P(D_1 = 1 \\mid H = 1) P(H = 1) = (1.0)(0.0015) = 0.0015$$\n",
    "\n",
    "**Step 6:** Apply Bayes' theorem:\n",
    "$$P(H = 1 \\mid D_1 = 1) = \\frac{0.0015}{0.011485} = 0.1306$$\n",
    "\n",
    "**Conclusion:** The probability of actually having HIV given a positive test is approximately **13.06%** or about **1 in 8**.\n",
    "\n",
    "**Interpretation:** Despite a positive test, the probability of actually having the disease is relatively low because:\n",
    "1. The disease is rare (low prior: 0.15%)\n",
    "2. The false positive rate (1%) is much higher than the disease prevalence\n",
    "3. Most positive tests are false positives, not true positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Solution - Two Tests\n",
    "\n",
    "**Extended Problem:**\n",
    "A second test is administered with properties:\n",
    "- Sensitivity: $P(D_2 = 1 \\mid H = 1) = 0.98$\n",
    "- False positive rate: $P(D_2 = 1 \\mid H = 0) = 0.03$\n",
    "\n",
    "**Assumption:** The tests are **conditionally independent given $H$**, meaning:\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H) = P(D_1 = 1 \\mid H) P(D_2 = 1 \\mid H)$$\n",
    "\n",
    "**Question:** What is $P(H = 1 \\mid D_1 = 1, D_2 = 1)$?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "**Step 1:** Apply Bayes' theorem:\n",
    "$$P(H = 1 \\mid D_1 = 1, D_2 = 1) = \\frac{P(D_1 = 1, D_2 = 1 \\mid H = 1) P(H = 1)}{P(D_1 = 1, D_2 = 1)}$$\n",
    "\n",
    "**Step 2:** Use conditional independence for $H = 1$:\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 1) = (1.0)(0.98) = 0.98$$\n",
    "\n",
    "**Step 3:** Use conditional independence for $H = 0$:\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 0) = (0.01)(0.03) = 0.0003$$\n",
    "\n",
    "**Step 4:** Compute marginal probability:\n",
    "$$P(D_1 = 1, D_2 = 1) = (0.0003)(0.9985) + (0.98)(0.0015)$$\n",
    "$$= 0.00029955 + 0.00147 = 0.00176955$$\n",
    "\n",
    "**Step 5:** Compute numerator:\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 1) P(H = 1) = (0.98)(0.0015) = 0.00147$$\n",
    "\n",
    "**Step 6:** Apply Bayes' theorem:\n",
    "$$P(H = 1 \\mid D_1 = 1, D_2 = 1) = \\frac{0.00147}{0.00176955} = 0.8307$$\n",
    "\n",
    "**Conclusion:** With two positive tests, the probability of having HIV increases to approximately **83.07%**.\n",
    "\n",
    "**Interpretation:** The second positive test provides significant additional evidence, increasing confidence from 13.06% to 83.07%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Investment Variance Example\n",
    "\n",
    "**Problem Setup:**\n",
    "\n",
    "An investment has three possible outcomes:\n",
    "- Return nothing (0Ã—): probability 0.5\n",
    "- Double investment (2Ã—): probability 0.4\n",
    "- Tenfold return (10Ã—): probability 0.1\n",
    "\n",
    "Let $X$ be the return multiplier.\n",
    "\n",
    "**Question:** Compute $E[X]$ and $\\text{Var}[X]$.\n",
    "\n",
    "#### 8.2.1 Expected Return\n",
    "\n",
    "$$E[X] = \\sum_{x} x \\cdot P(X = x)$$\n",
    "$$= (0)(0.5) + (2)(0.4) + (10)(0.1)$$\n",
    "$$= 0 + 0.8 + 1.0 = 1.8$$\n",
    "\n",
    "**Conclusion:** The expected return is **1.8Ã—** the investment (80% gain on average).\n",
    "\n",
    "#### 8.2.2 Variance of Return\n",
    "\n",
    "**Step 1:** Compute $E[X^2]$:\n",
    "$$E[X^2] = (0^2)(0.5) + (2^2)(0.4) + (10^2)(0.1)$$\n",
    "$$= 0 + 1.6 + 10 = 11.6$$\n",
    "\n",
    "**Step 2:** Compute variance:\n",
    "$$\\text{Var}[X] = E[X^2] - (E[X])^2 = 11.6 - (1.8)^2 = 11.6 - 3.24 = 8.36$$\n",
    "\n",
    "**Step 3:** Standard deviation:\n",
    "$$\\sigma = \\sqrt{8.36} \\approx 2.89$$\n",
    "\n",
    "**Conclusion:** The variance is **8.36**, indicating high risk.\n",
    "\n",
    "**Interpretation:**\n",
    "- The large variance relative to the mean indicates significant uncertainty\n",
    "- Most likely outcome (50% chance) is losing everything\n",
    "- Occasional large gains (10Ã— return with 10% probability) pull the expected value up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Important Inequalities and Limit Theorems\n",
    "\n",
    "### 9.1 Chebyshev's Inequality\n",
    "\n",
    "**Theorem 9.1 (Chebyshev's Inequality):**\n",
    "For any random variable $X$ with mean $\\mu$ and variance $\\sigma^2$, and for any $k > 0$:\n",
    "$$P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}$$\n",
    "\n",
    "**Equivalently:** For any $\\epsilon > 0$:\n",
    "$$P(|X - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Define an indicator random variable:\n",
    "$$I = \\begin{cases} 1 & \\text{if } |X - \\mu| \\geq \\epsilon \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "**Step 2:** Note that $E[I] = P(|X - \\mu| \\geq \\epsilon)$.\n",
    "\n",
    "**Step 3:** Observe that for all outcomes:\n",
    "$$I \\leq \\frac{(X - \\mu)^2}{\\epsilon^2}$$\n",
    "\n",
    "*Justification:*\n",
    "- If $|X - \\mu| \\geq \\epsilon$: Then $(X - \\mu)^2 \\geq \\epsilon^2$, so $\\frac{(X - \\mu)^2}{\\epsilon^2} \\geq 1 = I$\n",
    "- If $|X - \\mu| < \\epsilon$: Then $I = 0$ and $\\frac{(X - \\mu)^2}{\\epsilon^2} \\geq 0 = I$\n",
    "\n",
    "**Step 4:** Take expectations:\n",
    "$$E[I] \\leq E\\left[\\frac{(X - \\mu)^2}{\\epsilon^2}\\right] = \\frac{\\sigma^2}{\\epsilon^2}$$\n",
    "\n",
    "**Step 5:** Therefore:\n",
    "$$P(|X - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Interpretation:**\n",
    "- For $k = 2$: At least 75% of values lie within 2 standard deviations of the mean\n",
    "- For $k = 3$: At least 88.9% of values lie within 3 standard deviations of the mean\n",
    "\n",
    "**Note:** This bound applies to ANY distribution with finite variance, but is often loose for specific distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Law of Large Numbers\n",
    "\n",
    "**Theorem 9.2 (Weak Law of Large Numbers):**\n",
    "Let $X_1, X_2, \\ldots, X_n$ be independent and identically distributed (i.i.d.) random variables with mean $\\mu$ and finite variance $\\sigma^2$. Then the sample average\n",
    "$$\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$$\n",
    "converges in probability to $\\mu$ as $n \\to \\infty$:\n",
    "$$\\lim_{n \\to \\infty} P(|\\bar{X}_n - \\mu| > \\epsilon) = 0 \\quad \\text{for all } \\epsilon > 0$$\n",
    "\n",
    "**Proof Sketch:**\n",
    "\n",
    "**Step 1:** Compute the mean of $\\bar{X}_n$:\n",
    "$$E[\\bar{X}_n] = \\frac{1}{n} \\sum_{i=1}^n E[X_i] = \\frac{1}{n} \\cdot n\\mu = \\mu$$\n",
    "\n",
    "**Step 2:** Compute the variance of $\\bar{X}_n$:\n",
    "$$\\text{Var}[\\bar{X}_n] = \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}[X_i] = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "**Step 3:** Apply Chebyshev's inequality:\n",
    "$$P(|\\bar{X}_n - \\mu| \\geq \\epsilon) \\leq \\frac{\\text{Var}[\\bar{X}_n]}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2}$$\n",
    "\n",
    "**Step 4:** Take the limit:\n",
    "$$\\lim_{n \\to \\infty} P(|\\bar{X}_n - \\mu| \\geq \\epsilon) \\leq \\lim_{n \\to \\infty} \\frac{\\sigma^2}{n\\epsilon^2} = 0$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Interpretation:**\n",
    "- Empirical averages converge to theoretical means\n",
    "- Foundation for statistical inference\n",
    "- Justifies using sample means to estimate population means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Central Limit Theorem\n",
    "\n",
    "**Theorem 9.3 (Central Limit Theorem):**\n",
    "Let $X_1, X_2, \\ldots, X_n$ be i.i.d. random variables with mean $\\mu$ and variance $\\sigma^2 < \\infty$. Then the standardized sample average\n",
    "$$Z_n = \\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma}$$\n",
    "converges in distribution to a standard normal distribution $N(0,1)$ as $n \\to \\infty$.\n",
    "\n",
    "**Equivalently:**\n",
    "$$\\bar{X}_n \\approx N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ for large } n$$\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Convergence Rate:** The standard error is $\\sigma/\\sqrt{n}$, meaning:\n",
    "   - To halve the error, need 4Ã— more samples\n",
    "   - To reduce error by factor of 10, need 100Ã— more samples\n",
    "\n",
    "2. **Universality:** The CLT applies regardless of the original distribution (as long as variance exists)\n",
    "\n",
    "3. **Rule of Thumb:** CLT approximation is usually good for $n \\geq 30$\n",
    "\n",
    "**Applications:**\n",
    "- Confidence intervals\n",
    "- Hypothesis testing\n",
    "- Monte Carlo estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Types of Uncertainty\n",
    "\n",
    "### 10.1 Aleatoric Uncertainty\n",
    "\n",
    "**Definition 10.1 (Aleatoric Uncertainty):**\n",
    "**Aleatoric uncertainty** (also called **irreducible uncertainty** or **stochastic uncertainty**) is the inherent randomness in a system that cannot be reduced by collecting more data.\n",
    "\n",
    "**Etymology:** From Latin \"aleator\" (dice player)\n",
    "\n",
    "**Examples:**\n",
    "- Quantum randomness\n",
    "- Thermal noise in electronic circuits\n",
    "- Outcome of a fair coin flip\n",
    "\n",
    "**Characteristics:**\n",
    "- Intrinsic to the system\n",
    "- Cannot be eliminated\n",
    "- Best we can do is characterize the distribution\n",
    "\n",
    "### 10.2 Epistemic Uncertainty\n",
    "\n",
    "**Definition 10.2 (Epistemic Uncertainty):**\n",
    "**Epistemic uncertainty** (also called **model uncertainty** or **reducible uncertainty**) is uncertainty about model parameters or structure that can be reduced by collecting more data.\n",
    "\n",
    "**Etymology:** From Greek \"episteme\" (knowledge)\n",
    "\n",
    "**Examples:**\n",
    "- Uncertainty in parameter estimates due to limited data\n",
    "- Model selection uncertainty\n",
    "- Uncertainty about the true underlying distribution\n",
    "\n",
    "**Characteristics:**\n",
    "- Due to lack of knowledge\n",
    "- Can be reduced with more data\n",
    "- Captured by distributions over parameters (Bayesian approach)\n",
    "\n",
    "**Important Distinction:**\n",
    "- **Aleatoric:** \"We'll never know which outcome will occur, even with infinite data\"\n",
    "- **Epistemic:** \"We don't know yet, but more data will help\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Sampling\n",
    "\n",
    "### 11.1 Sampling from Distributions\n",
    "\n",
    "**Motivation:** In practice, we often need to generate random samples from a probability distribution for:\n",
    "- Monte Carlo estimation\n",
    "- Simulation studies\n",
    "- Bayesian inference\n",
    "- Machine learning (dropout, data augmentation)\n",
    "\n",
    "**Definition 11.1 (Random Sample):**\n",
    "A **random sample** of size $n$ from a distribution $P$ is a collection of $n$ i.i.d. random variables $X_1, X_2, \\ldots, X_n$ where each $X_i \\sim P$.\n",
    "\n",
    "### 11.2 Inverse Transform Sampling\n",
    "\n",
    "**Theorem 11.1 (Inverse Transform):**\n",
    "If $U \\sim \\text{Uniform}(0,1)$ and $F$ is a CDF with inverse $F^{-1}$, then:\n",
    "$$X = F^{-1}(U) \\sim F$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** We need to show $P(X \\leq x) = F(x)$.\n",
    "\n",
    "**Step 2:** Compute:\n",
    "$$P(X \\leq x) = P(F^{-1}(U) \\leq x) = P(U \\leq F(x))$$\n",
    "\n",
    "**Step 3:** Since $U \\sim \\text{Uniform}(0,1)$:\n",
    "$$P(U \\leq F(x)) = F(x)$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Generate $U \\sim \\text{Uniform}(0,1)$\n",
    "2. Return $X = F^{-1}(U)$\n",
    "\n",
    "**Example:** Exponential distribution with rate $\\lambda$\n",
    "- CDF: $F(x) = 1 - e^{-\\lambda x}$\n",
    "- Inverse: $F^{-1}(u) = -\\frac{\\ln(1-u)}{\\lambda}$\n",
    "- Sample: $X = -\\frac{\\ln(1-U)}{\\lambda}$ where $U \\sim \\text{Uniform}(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 12: Sigma-Algebra (Advanced Topic)\n",
    "\n",
    "### 12.1 Why Do We Need Sigma-Algebras?\n",
    "\n",
    "**Motivation:** For continuous sample spaces like $\\mathbb{R}$, not all subsets can be assigned consistent probabilities. The $\\sigma$-algebra defines which events are \"measurable.\"\n",
    "\n",
    "**Definition 12.1 ($\\sigma$-Algebra):**\n",
    "A **$\\sigma$-algebra** $\\mathcal{F}$ on a set $\\mathcal{S}$ is a collection of subsets satisfying:\n",
    "\n",
    "1. **Contains sample space:** $\\mathcal{S} \\in \\mathcal{F}$\n",
    "\n",
    "2. **Closed under complementation:** If $E \\in \\mathcal{F}$, then $E^c \\in \\mathcal{F}$\n",
    "\n",
    "3. **Closed under countable unions:** If $E_1, E_2, E_3, \\ldots \\in \\mathcal{F}$, then $\\bigcup_{i=1}^{\\infty} E_i \\in \\mathcal{F}$\n",
    "\n",
    "**Consequences:**\n",
    "- $\\emptyset \\in \\mathcal{F}$ (since $\\emptyset = \\mathcal{S}^c$)\n",
    "- Closed under countable intersections (by De Morgan's laws)\n",
    "- Closed under set differences\n",
    "\n",
    "**Common Examples:**\n",
    "1. **Trivial $\\sigma$-algebra:** $\\mathcal{F} = \\{\\emptyset, \\mathcal{S}\\}$\n",
    "2. **Power set:** $\\mathcal{F} = 2^{\\mathcal{S}}$ (all subsets) - works for countable $\\mathcal{S}$\n",
    "3. **Borel $\\sigma$-algebra on $\\mathbb{R}$:** Generated by all open intervals\n",
    "\n",
    "**For practical purposes:** In discrete/finite settings, we can use the power set and ignore these technicalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 13: Gaussian (Normal) Distribution\n",
    "\n",
    "### 13.1 Definition\n",
    "\n",
    "**Definition 13.1 (Gaussian Distribution):**\n",
    "A random variable $X$ follows a **Gaussian (normal) distribution** with parameters $\\mu$ (mean) and $\\sigma^2$ (variance), denoted $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, if it has PDF:\n",
    "$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "**Standard Normal:** When $\\mu = 0$ and $\\sigma = 1$:\n",
    "$$\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$$\n",
    "\n",
    "### 13.2 Key Properties\n",
    "\n",
    "1. **Symmetry:** $f(\\mu + x) = f(\\mu - x)$\n",
    "2. **Mean:** $E[X] = \\mu$\n",
    "3. **Variance:** $\\text{Var}(X) = \\sigma^2$\n",
    "4. **Standardization:** If $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then $Z = \\frac{X - \\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "### 13.3 The 68-95-99.7 Rule\n",
    "\n",
    "For a normal distribution:\n",
    "- Approximately 68% of values lie within $\\mu \\pm \\sigma$\n",
    "- Approximately 95% of values lie within $\\mu \\pm 2\\sigma$\n",
    "- Approximately 99.7% of values lie within $\\mu \\pm 3\\sigma$\n",
    "\n",
    "### 13.4 Proof that the PDF Integrates to 1\n",
    "\n",
    "**Claim:** $\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz = 1$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Let $I = \\int_{-\\infty}^{\\infty} e^{-z^2/2} dz$. We need to show $I = \\sqrt{2\\pi}$.\n",
    "\n",
    "**Step 2:** Compute $I^2$:\n",
    "$$I^2 = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} e^{-(z^2 + w^2)/2} dz \\, dw$$\n",
    "\n",
    "**Step 3:** Convert to polar coordinates $(r, \\theta)$:\n",
    "$$I^2 = \\int_0^{2\\pi} \\int_0^{\\infty} e^{-r^2/2} r \\, dr \\, d\\theta$$\n",
    "\n",
    "**Step 4:** Evaluate the radial integral with substitution $u = r^2/2$:\n",
    "$$\\int_0^{\\infty} e^{-r^2/2} r \\, dr = \\int_0^{\\infty} e^{-u} du = 1$$\n",
    "\n",
    "**Step 5:** Complete the calculation:\n",
    "$$I^2 = \\int_0^{2\\pi} 1 \\, d\\theta = 2\\pi$$\n",
    "\n",
    "**Step 6:** Therefore $I = \\sqrt{2\\pi}$.\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 14: Bernoulli Distribution\n",
    "\n",
    "### 14.1 Definition\n",
    "\n",
    "**Definition 14.1 (Bernoulli Distribution):**\n",
    "A random variable $X$ follows a **Bernoulli distribution** with parameter $p$, denoted $X \\sim \\text{Bernoulli}(p)$, if:\n",
    "$$P(X = 1) = p, \\quad P(X = 0) = 1 - p$$\n",
    "\n",
    "where $0 \\leq p \\leq 1$.\n",
    "\n",
    "### 14.2 Mean and Variance\n",
    "\n",
    "**Theorem 14.1:**\n",
    "For $X \\sim \\text{Bernoulli}(p)$:\n",
    "- $E[X] = p$\n",
    "- $\\text{Var}(X) = p(1-p)$\n",
    "\n",
    "**Proof of Mean:**\n",
    "$$E[X] = 0 \\cdot (1-p) + 1 \\cdot p = p$$\n",
    "\n",
    "**Proof of Variance:**\n",
    "\n",
    "**Step 1:** Note that $X^2 = X$ since $X \\in \\{0, 1\\}$.\n",
    "\n",
    "**Step 2:** Therefore $E[X^2] = E[X] = p$.\n",
    "\n",
    "**Step 3:** Apply variance formula:\n",
    "$$\\text{Var}(X) = E[X^2] - (E[X])^2 = p - p^2 = p(1-p)$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "**Properties:**\n",
    "- Maximum variance at $p = 0.5$: $\\text{Var}(X) = 0.25$\n",
    "- Variance is symmetric: same for $p$ and $1-p$\n",
    "- Variance is 0 when $p = 0$ or $p = 1$ (deterministic outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 15: Verification and Summary\n",
    "\n",
    "### 15.1 Cross-Verification Against Authoritative Sources\n",
    "\n",
    "All formulas and proofs in this notebook have been verified against:\n",
    "\n",
    "1. **D2L Textbook** (https://d2l.ai/chapter_preliminaries/probability.html)\n",
    "2. **Wikipedia** articles on probability theory\n",
    "3. **Standard textbooks:**\n",
    "   - Sheldon Ross, \"A First Course in Probability\"\n",
    "   - Dimitri Bertsekas and John Tsitsiklis, \"Introduction to Probability\"\n",
    "\n",
    "### 15.2 Key Formulas Summary\n",
    "\n",
    "| Concept | Formula | Conditions |\n",
    "|---------|---------|------------|\n",
    "| Conditional Probability | $P(A \\mid B) = \\frac{P(A,B)}{P(B)}$ | $P(B) > 0$ |\n",
    "| Bayes' Theorem | $P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$ | $P(A), P(B) > 0$ |\n",
    "| Marginalization | $P(A) = \\sum_b P(A,B=b)$ | - |\n",
    "| Independence | $P(A,B) = P(A)P(B)$ | Definition |\n",
    "| Expectation (discrete) | $E[X] = \\sum_x x \\cdot P(X=x)$ | Sum converges |\n",
    "| Variance | $\\text{Var}(X) = E[X^2] - (E[X])^2$ | Variance exists |\n",
    "| Linearity of Expectation | $E[aX+bY] = aE[X] + bE[Y]$ | Always holds |\n",
    "| Chebyshev's Inequality | $P(|X-\\mu| \\geq k\\sigma) \\leq 1/k^2$ | $\\sigma^2 < \\infty$ |\n",
    "| Bernoulli Mean | $E[X] = p$ | $X \\sim \\text{Bernoulli}(p)$ |\n",
    "| Bernoulli Variance | $\\text{Var}(X) = p(1-p)$ | $X \\sim \\text{Bernoulli}(p)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3 Notation Summary\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|--------|\n",
    "| $\\mathcal{S}$ or $\\Omega$ | Sample space |\n",
    "| $P(A)$ | Probability of event $A$ |\n",
    "| $P(A,B)$ | Joint probability |\n",
    "| $P(A \\mid B)$ | Conditional probability |\n",
    "| $E[X]$ or $\\mu$ | Expected value |\n",
    "| $\\text{Var}(X)$ or $\\sigma^2$ | Variance |\n",
    "| $\\sigma$ | Standard deviation |\n",
    "| $A \\perp B$ | $A$ independent of $B$ |\n",
    "| $A \\perp B \\mid C$ | $A$ conditionally independent of $B$ given $C$ |\n",
    "| $\\boldsymbol{\\Sigma}$ | Covariance matrix |\n",
    "| $\\mathcal{N}(\\mu, \\sigma^2)$ | Normal distribution |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2021). Dive into Deep Learning. https://d2l.ai/\n",
    "\n",
    "2. Ross, S. M. (2014). A First Course in Probability (9th ed.). Pearson.\n",
    "\n",
    "3. Bertsekas, D. P., & Tsitsiklis, J. N. (2008). Introduction to Probability (2nd ed.). Athena Scientific.\n",
    "\n",
    "4. Wikipedia contributors. (2024). Probability axioms. Wikipedia. https://en.wikipedia.org/wiki/Probability_axioms\n",
    "\n",
    "5. Wikipedia contributors. (2024). Bayes' theorem. Wikipedia. https://en.wikipedia.org/wiki/Bayes%27_theorem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
