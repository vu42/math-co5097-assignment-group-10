# Concrete Example: Spam Filter Implementation

# Simulate training data
np.random.seed(42)

# Word probabilities (simplified model)
# P(word | spam) and P(word | legitimate)
words = ['free', 'money', 'winner', 'meeting', 'report', 'project']

# Probability of each word appearing in spam vs legitimate emails
word_probs = {
    'free':    {'spam': 0.60, 'legit': 0.05},
    'money':   {'spam': 0.50, 'legit': 0.08},
    'winner':  {'spam': 0.45, 'legit': 0.02},
    'meeting': {'spam': 0.10, 'legit': 0.40},
    'report':  {'spam': 0.08, 'legit': 0.35},
    'project': {'spam': 0.05, 'legit': 0.45}
}

# Prior probabilities
p_spam = 0.30  # 30% of emails are spam
p_legit = 0.70

# Test emails
test_emails = [
    {'content': ['free', 'money', 'winner'], 'true_label': 'spam'},
    {'content': ['meeting', 'project', 'report'], 'true_label': 'legit'},
    {'content': ['free', 'project'], 'true_label': 'uncertain'},
    {'content': ['money'], 'true_label': 'spam'}
]

print("="*70)
print("SPAM FILTER DEMONSTRATION USING NAIVE BAYES")
print("="*70)

print("\nTraining Data:")
print(f"  Prior P(Spam) = {p_spam:.2f}")
print(f"  Prior P(Legitimate) = {p_legit:.2f}")

print("\nWord Probabilities:")
print(f"{'Word':<12} {'P(word|Spam)':<15} {'P(word|Legit)'}")
print("-"*70)
for word in words:
    print(f"{word:<12} {word_probs[word]['spam']:<15.2f} {word_probs[word]['legit']:.2f}")

print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

results = []

for idx, email in enumerate(test_emails, 1):
    print(f"\nEmail {idx}: {email['content']}")
    print("-"*70)
    
    # Compute P(words | spam) * P(spam)
    prob_spam = p_spam
    for word in email['content']:
        prob_spam *= word_probs[word]['spam']
    
    # Compute P(words | legit) * P(legit)
    prob_legit = p_legit
    for word in email['content']:
        prob_legit *= word_probs[word]['legit']
    
    # Normalize to get posterior
    total = prob_spam + prob_legit
    posterior_spam = prob_spam / total
    posterior_legit = prob_legit / total
    
    # Decision
    prediction = 'spam' if posterior_spam > 0.5 else 'legit'
    
    print(f"  P(words | spam) × P(spam) = {prob_spam:.6f}")
    print(f"  P(words | legit) × P(legit) = {prob_legit:.6f}")
    print(f"  P(spam | words) = {posterior_spam:.4f} = {posterior_spam*100:.2f}%")
    print(f"  P(legit | words) = {posterior_legit:.4f} = {posterior_legit*100:.2f}%")
    print(f"  → Prediction: {prediction.upper()}")
    
    results.append({
        'words': email['content'],
        'p_spam': posterior_spam,
        'prediction': prediction,
        'true_label': email['true_label']
    })

# Visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Posterior probabilities for each email
email_labels = [f"Email {i+1}\n{' '.join(r['words'][:2])}" for i, r in enumerate(results)]
spam_probs = [r['p_spam'] for r in results]
legit_probs = [1 - r['p_spam'] for r in results]

x = np.arange(len(email_labels))
width = 0.35

bars1 = ax1.bar(x - width/2, spam_probs, width, label='P(Spam|words)', color='red', alpha=0.7)
bars2 = ax1.bar(x + width/2, legit_probs, width, label='P(Legit|words)', color='green', alpha=0.7)

ax1.set_ylabel('Posterior Probability')
ax1.set_title('Naive Bayes Classification Results')
ax1.set_xticks(x)
ax1.set_xticklabels(email_labels, fontsize=8)
ax1.legend()
ax1.axhline(y=0.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax1.grid(True, alpha=0.3, axis='y')

# Add percentage labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height*100:.1f}%', ha='center', va='bottom', fontsize=8)

# Plot 2: Word likelihood ratios
likelihood_ratios = {}
for word in words:
    ratio = word_probs[word]['spam'] / word_probs[word]['legit']
    likelihood_ratios[word] = ratio

words_sorted = sorted(words, key=lambda w: likelihood_ratios[w], reverse=True)
ratios_sorted = [likelihood_ratios[w] for w in words_sorted]
colors = ['red' if r > 1 else 'green' for r in ratios_sorted]

ax2.barh(words_sorted, ratios_sorted, color=colors, alpha=0.7)
ax2.set_xlabel('Likelihood Ratio: P(word|Spam) / P(word|Legit)')
ax2.set_title('Spam Indicators: Likelihood Ratios')
ax2.axvline(x=1, color='black', linestyle='--', linewidth=2, label='Neutral (ratio=1)')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='x')

# Add ratio labels
for i, (word, ratio) in enumerate(zip(words_sorted, ratios_sorted)):
    ax2.text(ratio + 0.2, i, f'{ratio:.1f}x', va='center', fontsize=9)

plt.tight_layout()
plt.show()

print("\n" + "="*70)
print("KEY INSIGHTS:")
print("="*70)
print("1. Words like 'free', 'money', 'winner' are strong spam indicators")
print("2. Words like 'meeting', 'project', 'report' suggest legitimate emails")
print("3. Bayes' theorem allows us to combine multiple pieces of evidence")
print("4. The prior probability P(spam) affects final classification")
print("5. This probabilistic approach quantifies uncertainty in predictions")
print("="*70)
