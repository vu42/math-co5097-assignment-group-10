{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6. Probability and Statistics\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Probability theory provides the mathematical foundation for reasoning under uncertainty. In machine learning, we rarely have complete information—instead, we make predictions based on incomplete or noisy data. Probability gives us tools to quantify uncertainty and learn from data.\n",
    "\n",
    "Statistical methods extend probability theory to analyze real-world data, estimate parameters, and make predictions. Together, they form the backbone of modern data science.\n",
    "\n",
    "**Key Questions:**\n",
    "- How do we formally model uncertainty?\n",
    "- How can we update beliefs when observing new evidence?\n",
    "- How do we make optimal decisions under uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example: Spam Classification\n",
    "\n",
    "Instead of deterministic rules, we compute:\n",
    "\n",
    "$$P(\\text{spam} | \\text{words in email})$$\n",
    "\n",
    "Using **Bayes' Theorem**:\n",
    "\n",
    "$$P(\\text{spam}|\\text{words}) = \\frac{P(\\text{words}|\\text{spam}) \\cdot P(\\text{spam})}{P(\\text{words})}$$\n",
    "\n",
    "This allows us to quantify uncertainty, update beliefs with new evidence, and learn from data systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Computer Science\n",
    "\n",
    "| Domain | Applications |\n",
    "|--------|-------------|\n",
    "| **Machine Learning** | Classification, regression, Bayesian networks |\n",
    "| **NLP** | Language models, translation, speech recognition |\n",
    "| **Computer Vision** | Object recognition, image generation (GANs, VAEs) |\n",
    "| **Search & Recommendations** | Document ranking, collaborative filtering |\n",
    "| **Cybersecurity** | Anomaly detection, intrusion detection |\n",
    "| **Algorithms** | Randomized algorithms, Bloom filters |\n",
    "| **Reinforcement Learning** | MDPs, exploration-exploitation |\n",
    "\n"
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"2.6. Probability and Statistics - D2L\")\n",
    "print(\"=\"*50)\n",
    "print(\"Environment initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concrete Example: Dice Rolling Simulation\n",
    "\n",
    "Demonstrating probability concepts using a fair die:\n",
    "- **Prior probability**: Each face has $P = \\frac{1}{6}$\n",
    "- **Law of Large Numbers**: Frequencies converge to true probabilities\n",
    "- **Expectation**: $E[X] = \\sum_{i=1}^{6} i \\cdot P(X=i) = 3.5$\n",
    "- **Variance**: $Var(X) = E[X^2] - (E[X])^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fair dice - equal probability for each face\n",
    "fair_probs = np.ones(6) / 6\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DICE ROLLING DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nProbability Distribution:\")\n",
    "print(f\"{'Face':<10} {'P(Face)':<15}\")\n",
    "print(\"-\"*60)\n",
    "for i in range(6):\n",
    "    print(f\"{i+1:<10} {fair_probs[i]:.4f}\")\n",
    "\n",
    "# Simulate rolling dice\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMULATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Single roll\n",
    "single_roll = np.random.multinomial(1, fair_probs)\n",
    "print(f\"\\nSingle roll result: {single_roll.tolist()}\")\n",
    "\n",
    "# 10 rolls\n",
    "ten_rolls = np.random.multinomial(10, fair_probs)\n",
    "print(f\"10 rolls count: {ten_rolls.tolist()}\")\n",
    "\n",
    "# 1000 rolls\n",
    "thousand_rolls = np.random.multinomial(1000, fair_probs)\n",
    "print(f\"1000 rolls frequency: {(thousand_rolls/1000).tolist()}\")\n",
    "\n",
    "# Law of Large Numbers visualization\n",
    "counts = np.array([np.random.multinomial(1, fair_probs) for _ in range(500)])\n",
    "cum_counts = counts.cumsum(axis=0)\n",
    "estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Convergence to true probability\n",
    "for i in range(6):\n",
    "    ax1.plot(estimates[:, i], label=f'P(die={i+1})')\n",
    "ax1.axhline(y=1/6, color='black', linestyle='dashed', linewidth=2, label='True prob (1/6)')\n",
    "ax1.set_xlabel('Number of experiments')\n",
    "ax1.set_ylabel('Estimated probability')\n",
    "ax1.set_title('Law of Large Numbers')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Final distribution vs theoretical\n",
    "final_counts = thousand_rolls\n",
    "faces = [1, 2, 3, 4, 5, 6]\n",
    "theoretical = [1000/6] * 6\n",
    "\n",
    "x = range(6)\n",
    "width = 0.35\n",
    "ax2.bar([i - width/2 for i in x], final_counts, width, label='Observed', color='blue', alpha=0.7)\n",
    "ax2.bar([i + width/2 for i in x], theoretical, width, label='Theoretical', color='red', alpha=0.7)\n",
    "ax2.set_xlabel('Die Face')\n",
    "ax2.set_ylabel('Count (out of 1000)')\n",
    "ax2.set_title('Observed vs Theoretical Distribution')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(faces)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Expectation and Variance\n",
    "values = np.arange(1, 7, dtype=np.float32)\n",
    "expectation = (values * fair_probs).sum()\n",
    "expectation_x2 = ((values ** 2) * fair_probs).sum()\n",
    "variance = expectation_x2 - expectation ** 2\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"E[X] = {expectation:.4f}\")\n",
    "print(f\"Var(X) = {variance:.4f}\")\n",
    "print(f\"Std(X) = {np.sqrt(variance):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Core Definitions and Axioms of Probability\n",
    "\n",
    "### 1.1 Sample Space and Events\n",
    "\n",
    "**Definition 1.1 (Sample Space):**\n",
    "The **sample space**, denoted $\\mathcal{S}$ (or sometimes $\\Omega$), is the set of all possible outcomes of a random experiment.\n",
    "\n",
    "**Definition 1.2 (Event):**\n",
    "An **event** $\\mathcal{A}$ is a subset of the sample space: $\\mathcal{A} \\subseteq \\mathcal{S}$.\n",
    "\n",
    "**Definition 1.3 (Event Occurrence):**\n",
    "We say that event $\\mathcal{A}$ has occurred if and only if the realized outcome $z$ satisfies $z \\in \\mathcal{A}$.\n",
    "\n",
    "**Examples:**\n",
    "- Coin flip: $\\mathcal{S} = \\{\\text{Heads}, \\text{Tails}\\}$\n",
    "- Die roll: $\\mathcal{S} = \\{1, 2, 3, 4, 5, 6\\}$\n",
    "- Temperature measurement: $\\mathcal{S} = \\mathbb{R}$\n",
    "\n",
    "**Reviewer's Commentary:**\n",
    "- These definitions are standard and correct.\n",
    "- The notation $\\mathcal{S}$ for sample space is conventional (alternative notations include $\\Omega$ or $S$).\n",
    "- Events form a $\\sigma$-algebra (discussed in Section 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Kolmogorov Axioms of Probability\n",
    "\n",
    "**Definition 1.4 (Probability Function):**\n",
    "A **probability function** (or probability measure) is a function\n",
    "$$P: \\mathcal{F} \\to [0,1]$$\n",
    "where $\\mathcal{F}$ is a $\\sigma$-algebra of events on $\\mathcal{S}$, that satisfies the following three axioms:\n",
    "\n",
    "**Axiom 1 (Non-negativity):** For any event $\\mathcal{A} \\in \\mathcal{F}$,\n",
    "$$P(\\mathcal{A}) \\geq 0$$\n",
    "\n",
    "**Axiom 2 (Normalization):** The probability of the entire sample space is 1:\n",
    "$$P(\\mathcal{S}) = 1$$\n",
    "\n",
    "**Axiom 3 (Countable Additivity):** For any countable collection of **mutually exclusive** (disjoint) events $\\{\\mathcal{A}_i\\}_{i=1}^{\\infty}$ where $\\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset$ for all $i \\neq j$,\n",
    "$$P\\left(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i\\right) = \\sum_{i=1}^{\\infty} P(\\mathcal{A}_i)$$\n",
    "\n",
    "**Reviewer's Commentary:**\n",
    "- These are the **Kolmogorov axioms**, the foundation of modern probability theory (1933).\n",
    "- **Critical requirement:** Axiom 3 requires that events be **mutually exclusive** (disjoint). This condition must be stated explicitly.\n",
    "- For finite cases, we often use the simpler finite additivity: $P(A \\cup B) = P(A) + P(B)$ when $A \\cap B = \\emptyset$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Immediate Consequences of the Axioms\n",
    "\n",
    "**Theorem 1.1 (Probability of Empty Set):**\n",
    "$$P(\\emptyset) = 0$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Consider the sample space $\\mathcal{S}$ and the empty set $\\emptyset$.\n",
    "\n",
    "**Step 2:** Note that $\\mathcal{S} \\cap \\emptyset = \\emptyset$ and $\\mathcal{S} \\cup \\emptyset = \\mathcal{S}$.\n",
    "\n",
    "*This follows from the definition of the empty set.*\n",
    "\n",
    "**Step 3:** Therefore $\\mathcal{S}$ and $\\emptyset$ are disjoint events.\n",
    "\n",
    "*This follows directly from Step 2.*\n",
    "\n",
    "**Step 4:** By Axiom 3 (countable additivity with $n=2$):\n",
    "$$P(\\mathcal{S} \\cup \\emptyset) = P(\\mathcal{S}) + P(\\emptyset)$$\n",
    "\n",
    "*The additivity axiom applies since the events are disjoint.*\n",
    "\n",
    "**Step 5:** Simplify the left side using $\\mathcal{S} \\cup \\emptyset = \\mathcal{S}$:\n",
    "$$P(\\mathcal{S}) = P(\\mathcal{S}) + P(\\emptyset)$$\n",
    "\n",
    "**Step 6:** Subtract $P(\\mathcal{S})$ from both sides:\n",
    "$$0 = P(\\emptyset)$$\n",
    "\n",
    "Therefore $P(\\emptyset) = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 1.2 (Complement Rule):**\n",
    "For any event $\\mathcal{A}$ and its complement $\\mathcal{A}^c = \\mathcal{S} \\setminus \\mathcal{A}$,\n",
    "$$P(\\mathcal{A}) + P(\\mathcal{A}^c) = 1$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** By definition of complement, $\\mathcal{A} \\cap \\mathcal{A}^c = \\emptyset$.\n",
    "\n",
    "*An event and its complement are disjoint by definition.*\n",
    "\n",
    "**Step 2:** Also by definition of complement, $\\mathcal{A} \\cup \\mathcal{A}^c = \\mathcal{S}$.\n",
    "\n",
    "*An event and its complement together partition the sample space.*\n",
    "\n",
    "**Step 3:** Since $\\mathcal{A}$ and $\\mathcal{A}^c$ are disjoint, by Axiom 3:\n",
    "$$P(\\mathcal{A} \\cup \\mathcal{A}^c) = P(\\mathcal{A}) + P(\\mathcal{A}^c)$$\n",
    "\n",
    "**Step 4:** Substitute from Step 2:\n",
    "$$P(\\mathcal{S}) = P(\\mathcal{A}) + P(\\mathcal{A}^c)$$\n",
    "\n",
    "**Step 5:** By Axiom 2, $P(\\mathcal{S}) = 1$, therefore:\n",
    "$$1 = P(\\mathcal{A}) + P(\\mathcal{A}^c)$$\n",
    "\n",
    "Therefore $P(\\mathcal{A}) + P(\\mathcal{A}^c) = 1$. \n",
    "\n",
    "**Corollary 1.2.1:**\n",
    "$$P(\\mathcal{A}^c) = 1 - P(\\mathcal{A})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 1.3 (Monotonicity):**\n",
    "If $\\mathcal{A} \\subseteq \\mathcal{B}$, then $P(\\mathcal{A}) \\leq P(\\mathcal{B})$.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Write $\\mathcal{B} = \\mathcal{A} \\cup (\\mathcal{B} \\setminus \\mathcal{A})$.\n",
    "\n",
    "*Any set can be decomposed into a subset and its relative complement.*\n",
    "\n",
    "**Step 2:** Note that $\\mathcal{A}$ and $\\mathcal{B} \\setminus \\mathcal{A}$ are disjoint.\n",
    "\n",
    "**Step 3:** By Axiom 3:\n",
    "$$P(\\mathcal{B}) = P(\\mathcal{A}) + P(\\mathcal{B} \\setminus \\mathcal{A})$$\n",
    "\n",
    "**Step 4:** By Axiom 1, $P(\\mathcal{B} \\setminus \\mathcal{A}) \\geq 0$.\n",
    "\n",
    "**Step 5:** Therefore:\n",
    "$$P(\\mathcal{B}) = P(\\mathcal{A}) + P(\\mathcal{B} \\setminus \\mathcal{A}) \\geq P(\\mathcal{A})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is a Random Variable?\n",
    "\n",
    "A random variable provides a mathematical framework for mapping outcomes from a sample space to numerical values, enabling quantitative analysis.\n",
    "\n",
    "**Definition 2.1 (Random Variable - Formal Version):**\n",
    "A **random variable** is a measurable function $X: \\mathcal{S} \\to \\mathbb{R}$ that maps outcomes from the sample space to real numbers.\n",
    "\n",
    "**Intuition:** A random variable assigns a number to each possible outcome of a random experiment.\n",
    "\n",
    "**Example 1: Coin Toss**\n",
    "\n",
    "Consider a coin flip experiment where the sample space is $\\mathcal{S} = \\{\\text{Heads}, \\text{Tails}\\}$.\n",
    "\n",
    "Define a random variable $X$ that maps:\n",
    "- Heads → 1\n",
    "- Tails → 0\n",
    "\n",
    "This encoding enables mathematical operations such as computing averages.\n",
    "\n",
    "**Example 2: Rolling a Die**\n",
    "\n",
    "When rolling a die, define $X$ to be the number showing on the top face.\n",
    "\n",
    "The random variable takes values in $\\{1, 2, 3, 4, 5, 6\\}$.\n",
    "\n",
    "**Two Types of Random Variables:**\n",
    "\n",
    "| Type | Description | Examples |\n",
    "|------|-------------|----------|\n",
    "| **Discrete** | Countable values | Coin flips, dice rolls, number of customers |\n",
    "| **Continuous** | Any value in a range | Height, weight, temperature, time |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Probability Mass Function (PMF) and Probability Density Function (PDF)\n",
    "\n",
    "**Definition 2.2 (Probability Mass Function):**\n",
    "For a discrete random variable $X$, the **probability mass function (PMF)** is:\n",
    "$$p_X(x) = P(X = x)$$\n",
    "\n",
    "**Properties of PMF:**\n",
    "1. $p_X(x) \\geq 0$ for all $x$\n",
    "2. $\\sum_{x} p_X(x) = 1$\n",
    "\n",
    "**Definition 2.3 (Probability Density Function):**\n",
    "For a continuous random variable $X$, the **probability density function (PDF)** is a function $f_X(x)$ such that:\n",
    "$$P(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx$$\n",
    "\n",
    "**Properties of PDF:**\n",
    "1. $f_X(x) \\geq 0$ for all $x$\n",
    "2. $\\int_{-\\infty}^{\\infty} f_X(x) \\, dx = 1$\n",
    "\n",
    "**Important Note:** For continuous random variables, $P(X = x) = 0$ for any specific value $x$. We can only compute probabilities for intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Example: The Fair Coin Toss\n",
    "\n",
    "Consider a fair coin flip experiment.\n",
    "\n",
    "**Setup:**\n",
    "- Fair coin with equal probability of heads or tails\n",
    "- Sample space: $\\mathcal{S} = \\{\\text{Heads}, \\text{Tails}\\}$\n",
    "\n",
    "**Step 1: Define a Random Variable**\n",
    "\n",
    "Define $X = 1$ if Heads, and $X = 0$ if Tails.\n",
    "\n",
    "Thus:\n",
    "- $P(X = 1) = 0.5$ (probability of heads)\n",
    "- $P(X = 0) = 0.5$ (probability of tails)\n",
    "\n",
    "**Step 2: Calculate the Expected Value**\n",
    "\n",
    "The expected value is computed as:\n",
    "\n",
    "$$E[X] = \\sum_{x} x \\cdot P(X = x) = (1)(0.5) + (0)(0.5) = 0.5$$\n",
    "\n",
    "**Interpretation:** As the number of flips approaches infinity, the sample average converges to 0.5.\n",
    "\n",
    "**Step 3: Calculate the Variance**\n",
    "\n",
    "Variance measures the dispersion of the distribution:\n",
    "\n",
    "Using the formula: $\\text{Var}[X] = E[X^2] - (E[X])^2$\n",
    "\n",
    "First, compute $E[X^2]$:\n",
    "$$E[X^2] = (1^2)(0.5) + (0^2)(0.5) = 0.5$$\n",
    "\n",
    "Then:\n",
    "$$\\text{Var}[X] = 0.5 - (0.5)^2 = 0.5 - 0.25 = 0.25$$\n",
    "\n",
    "Standard deviation:\n",
    "$$\\sigma = \\sqrt{0.25} = 0.5$$\n",
    "\n",
    "**Interpretation:** The dispersion is maximal for a Bernoulli random variable, as the outcomes are binary with equal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Example: Two Coin Tosses\n",
    "\n",
    "**Sample space:** $\\mathcal{S} = \\{HH, HT, TH, TT\\}$\n",
    "\n",
    "Define $Y$ to be the total number of heads. The random variable takes values in $\\{0, 1, 2\\}$.\n",
    "\n",
    "**Probabilities:**\n",
    "- $P(Y = 0) = 1/4$ (both tails: TT)\n",
    "- $P(Y = 1) = 2/4 = 1/2$ (one head: HT or TH)\n",
    "- $P(Y = 2) = 1/4$ (both heads: HH)\n",
    "\n",
    "**Expected Value:**\n",
    "$$E[Y] = (0)(1/4) + (1)(1/2) + (2)(1/4) = 0 + 0.5 + 0.5 = 1$$\n",
    "\n",
    "The expected number of heads is 1.\n",
    "\n",
    "**Variance:**\n",
    "$$E[Y^2] = (0^2)(1/4) + (1^2)(1/2) + (2^2)(1/4) = 0 + 0.5 + 1 = 1.5$$\n",
    "\n",
    "$$\\text{Var}[Y] = 1.5 - (1)^2 = 1.5 - 1 = 0.5$$\n",
    "\n",
    "**Observation:** For 1 coin, $E[X] = 0.5$ and for 2 coins, $E[Y] = 1 = 2 \\times 0.5$. This demonstrates the **linearity of expectation** property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Additional Examples\n",
    "\n",
    "#### Example 1: Weather Forecasting\n",
    "\n",
    "When a forecast indicates \"30% chance of rain tomorrow,\" this denotes a probability assignment.\n",
    "\n",
    "**Sample Space:** $\\mathcal{S} = \\{\\text{Rain}, \\text{No Rain}\\}$\n",
    "\n",
    "**Probabilities:**\n",
    "- $P(\\text{Rain}) = 0.3$\n",
    "- $P(\\text{No Rain}) = 0.7$\n",
    "\n",
    "Define $X = 1$ if it rains, $X = 0$ otherwise.\n",
    "\n",
    "**Expected value:**\n",
    "$$E[X] = (1)(0.3) + (0)(0.7) = 0.3$$\n",
    "\n",
    "**Interpretation:** Under frequentist interpretation, if there were 100 days with identical atmospheric conditions, rain would occur on approximately 30 of them.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 2: Exam Scores\n",
    "\n",
    "Suppose exam performance follows this distribution:\n",
    "- Score 90-100: probability 0.4\n",
    "- Score 70-89: probability 0.5\n",
    "- Score 60-69: probability 0.1\n",
    "\n",
    "Using the midpoint of each range:\n",
    "\n",
    "$$E[\\text{Score}] = (95)(0.4) + (80)(0.5) + (65)(0.1) = 38 + 40 + 6.5 = 84.5$$\n",
    "\n",
    "Expected score: 84.5\n",
    "\n",
    "**Variance calculation:**\n",
    "$$E[\\text{Score}^2] = (95^2)(0.4) + (80^2)(0.5) + (65^2)(0.1) = 3610 + 3200 + 422.5 = 7232.5$$\n",
    "\n",
    "$$\\text{Var}[\\text{Score}] = 7232.5 - (84.5)^2 = 7232.5 - 7140.25 = 92.25$$\n",
    "\n",
    "$$\\sigma = \\sqrt{92.25} \\approx 9.6$$\n",
    "\n",
    "**Interpretation:** The standard deviation of approximately 10 points indicates the typical deviation from the expected score.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 3: Probabilistic Reward System\n",
    "\n",
    "A reward system has:\n",
    "- Common item (value 1): 70% probability\n",
    "- Rare item (value 5): 25% probability\n",
    "- Legendary item (value 50): 5% probability\n",
    "\n",
    "**Expected value:**\n",
    "$$E[X] = (1)(0.70) + (5)(0.25) + (50)(0.05) = 0.70 + 1.25 + 2.50 = 4.45$$\n",
    "\n",
    "Each trial has an expected value of 4.45 units.\n",
    "\n",
    "**Variance:**\n",
    "$$E[X^2] = (1^2)(0.70) + (5^2)(0.25) + (50^2)(0.05) = 0.70 + 6.25 + 125 = 131.95$$\n",
    "\n",
    "$$\\text{Var}[X] = 131.95 - (4.45)^2 = 131.95 - 19.80 = 112.15$$\n",
    "\n",
    "$$\\sigma = \\sqrt{112.15} \\approx 10.59$$\n",
    "\n",
    "**Analysis:** The high variance (112.15) relative to the mean (4.45) indicates significant variability in outcomes. Most trials yield low values, while rare high-value outcomes substantially influence the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Joint and Conditional Probability\n",
    "\n",
    "### 3.1 Joint Probability\n",
    "\n",
    "**Definition 3.1 (Joint Probability):**\n",
    "For two random variables $A$ and $B$, the **joint probability** is:\n",
    "$$P(A = a, B = b)$$\n",
    "which represents the probability that both events $\\{A = a\\}$ and $\\{B = b\\}$ occur simultaneously.\n",
    "\n",
    "**Alternative notation:** $P(A = a \\cap B = b)$ or $P(A = a \\text{ and } B = b)$\n",
    "\n",
    "**Theorem 3.1 (Joint Probability Bounds)**\n",
    "\n",
    "$$P(A = a, B = b) \\leq P(A = a) \\text{ and } P(A = a, B = b) \\leq P(B = b) \\tag{2.6.1}$$\n",
    "\n",
    "**Proof of first inequality:**\n",
    "\n",
    "**Step 1:** Partition the event $\\{A = a\\}$ based on all possible values of $B$:\n",
    "$$\\{A = a\\} = \\bigcup_{v \\in \\text{Val}(B)} \\{A = a, B = v\\}$$\n",
    "\n",
    "*This follows from the law of total probability.*\n",
    "\n",
    "**Step 2:** The events $\\{A = a, B = v\\}$ for different values of $v$ are mutually exclusive.\n",
    "\n",
    "*$B$ cannot take two different values simultaneously.*\n",
    "\n",
    "**Step 3:** Apply Axiom 3 (countable additivity):\n",
    "$$P(A = a) = \\sum_{v \\in \\text{Val}(B)} P(A = a, B = v)$$\n",
    "\n",
    "**Step 4:** Since all probabilities are non-negative (Axiom 1), each term in the sum is $\\geq 0$:\n",
    "$$P(A = a) = P(A = a, B = b) + \\sum_{v \\neq b} P(A = a, B = v) \\geq P(A = a, B = b)$$\n",
    "\n",
    "Therefore $P(A = a, B = b) \\leq P(A = a)$. The proof for the second inequality is symmetric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Conditional Probability\n",
    "\n",
    "**Definition 3.2 (Conditional Probability):**\n",
    "The **conditional probability** of event $B = b$ given that event $A = a$ has occurred is defined as:\n",
    "\n",
    "$$P(B = b \\mid A = a) = \\frac{P(A = a, B = b)}{P(A = a)}, \\text{ where } P(A = a) > 0 \\tag{2.6.2}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- We restrict our sample space to outcomes where $A = a$ occurred\n",
    "- We renormalize probabilities so they sum to 1 over this restricted space\n",
    "- The conditional probability measures the likelihood of $B = b$ within this restricted space\n",
    "\n",
    "**Important Condition:** This definition is only valid when $P(A = a) > 0$. If $P(A = a) = 0$, conditional probability is undefined (division by zero).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Roll a fair die. Let:\n",
    "- $A$ = \"roll is even\" = $\\{2, 4, 6\\}$\n",
    "- $B$ = \"roll is greater than 3\" = $\\{4, 5, 6\\}$\n",
    "\n",
    "Then:\n",
    "- $P(A) = 3/6 = 1/2$\n",
    "- $P(B) = 3/6 = 1/2$\n",
    "- $P(A \\cap B) = P(\\{4, 6\\}) = 2/6 = 1/3$\n",
    "\n",
    "$$P(B \\mid A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{1/3}{1/2} = \\frac{2}{3}$$\n",
    "\n",
    "Given that the roll is even, there's a 2/3 chance it's greater than 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Verification that Conditional Probability is a Valid Probability\n",
    "\n",
    "**Theorem 3.2:** For fixed $A = a$ with $P(A = a) > 0$, the function $Q(B = b) = P(B = b \\mid A = a)$ satisfies all probability axioms.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Axiom 1 (Non-negativity):**\n",
    "\n",
    "**Step 1:** By definition:\n",
    "$$Q(B = b) = P(B = b \\mid A = a) = \\frac{P(A = a, B = b)}{P(A = a)}$$\n",
    "\n",
    "**Step 2:** The numerator satisfies $P(A = a, B = b) \\geq 0$ (by Axiom 1).\n",
    "\n",
    "**Step 3:** The denominator satisfies $P(A = a) > 0$ (given assumption).\n",
    "\n",
    "**Step 4:** Therefore:\n",
    "$$Q(B = b) = \\frac{P(A = a, B = b)}{P(A = a)} \\geq 0$$\n",
    "\n",
    "**Axiom 2 (Normalization):**\n",
    "\n",
    "**Step 1:** Sum over all possible values of $B$:\n",
    "$$\\sum_{b} Q(B = b) = \\sum_{b} \\frac{P(A = a, B = b)}{P(A = a)}$$\n",
    "\n",
    "**Step 2:** Factor out constant denominator:\n",
    "$$= \\frac{1}{P(A = a)} \\sum_{b} P(A = a, B = b)$$\n",
    "\n",
    "**Step 3:** By marginalization:\n",
    "$$= \\frac{1}{P(A = a)} \\cdot P(A = a) = 1$$\n",
    "\n",
    "**Axiom 3 (Additivity):** Similar verification for disjoint events.\n",
    "\n",
    "Therefore, conditional probability defines a valid probability measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Marginalization (Law of Total Probability)\n",
    "\n",
    "**Theorem 3.3 (Marginalization):**\n",
    "For random variables $A$ and $B$:\n",
    "$$P(A = a) = \\sum_{v \\in \\text{Val}(B)} P(A = a, B = v)$$\n",
    "\n",
    "**Proof:** This was proven in Theorem 3.1.\n",
    "\n",
    "**Theorem 3.4 (Law of Total Probability):**\n",
    "\n",
    "$$\\sum_a P(B \\mid A = a)P(A = a) = \\sum_a P(B, A = a) = P(B) \\tag{2.6.6}$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Start with the marginalization formula for $P(B)$:\n",
    "$$P(B) = \\sum_a P(A = a, B)$$\n",
    "\n",
    "**Step 2:** Apply the definition of conditional probability to each term:\n",
    "$$P(A = a, B) = P(B \\mid A = a)P(A = a)$$\n",
    "\n",
    "This assumes $P(A = a) > 0$.\n",
    "\n",
    "**Step 3:** Substitute into Step 1:\n",
    "$$P(B) = \\sum_a P(B \\mid A = a)P(A = a)$$\n",
    "\n",
    "**Note:** For values $a$ where $P(A = a) = 0$, the term contributes 0 to the sum.\n",
    "\n",
    "**Intuition:** To find the probability of $B$, sum over all possible \"paths\" through values of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Bayes' Theorem\n",
    "\n",
    "### 4.1 Derivation of Bayes' Theorem\n",
    "\n",
    "**Theorem 4.1 (Bayes' Theorem - Basic Form):**\n",
    "For events $A$ and $B$ with $P(A) > 0$ and $P(B) > 0$:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)} \\tag{2.6.3}$$\n",
    "\n",
    "\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** By definition of conditional probability:\n",
    "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "*This is valid when $P(B) > 0$.*\n",
    "\n",
    "**Step 2:** Note that joint probability is symmetric:\n",
    "$$P(A, B) = P(B, A)$$\n",
    "\n",
    "*$P(A \\cap B) = P(B \\cap A)$.*\n",
    "\n",
    "**Step 3:** Apply definition of conditional probability to express joint probability:\n",
    "$$P(B, A) = P(B \\mid A) P(A)$$\n",
    "\n",
    "*This is valid when $P(A) > 0$.*\n",
    "\n",
    "**Step 4:** Combine Steps 2 and 3:\n",
    "$$P(A, B) = P(B \\mid A) P(A)$$\n",
    "\n",
    "**Step 5:** Substitute Step 4 into Step 1:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\n",
    "\n",
    "Therefore Bayes' theorem is established.\n",
    "\n",
    "**Corollary 4.1.1 (Proportional Form of Bayes' Theorem)**\n",
    "\n",
    "When we don't have direct access to $P(B)$, a simplified version of Bayes' theorem comes in handy:\n",
    "\n",
    "$$P(A \\mid B) \\propto P(B \\mid A)P(A) \\tag{2.6.4}$$\n",
    "\n",
    "**Interpretation:** The posterior is *proportional to* likelihood times prior.\n",
    "\n",
    "**Corollary 4.1.2 (Normalized Form)**\n",
    "\n",
    "Since we know that $P(A \\mid B)$ must be normalized to 1, i.e., $\\sum_a P(A = a \\mid B) = 1$, we can compute:\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{\\sum_a P(B \\mid A = a)P(A = a)} \\tag{2.6.5}$$\n",
    "\n",
    "**Note:** The denominator is the normalization constant, which equals $P(B)$ by the Law of Total Probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Terminology and Interpretation\n",
    "\n",
    "In Bayes' theorem:\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "| Term | Name | Meaning |\n",
    "|------|------|--------|\n",
    "| $P(A)$ | **Prior** | What we believed BEFORE seeing evidence |\n",
    "| $P(B \\mid A)$ | **Likelihood** | How likely is the evidence if our belief is true? |\n",
    "| $P(B)$ | **Marginal / Evidence** | How likely is the evidence overall? |\n",
    "| $P(A \\mid B)$ | **Posterior** | What we believe AFTER seeing evidence |\n",
    "\n",
    "**The Big Idea:** Bayes' theorem tells us how to update our beliefs when we get new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Example: Friend Attendance Prediction\n",
    "\n",
    "**Situation:** Consider a friend who states they will attend a party. Historical data shows:\n",
    "- When they actually attend, they always confirm attendance: $P(\\text{Says Yes} \\mid \\text{Comes}) = 1.0$\n",
    "- Actual attendance rate: $P(\\text{Comes}) = 0.3$\n",
    "- Confirmation rate: $P(\\text{Says Yes}) = 0.8$\n",
    "\n",
    "**Question:** Given confirmation, compute the probability of actual attendance.\n",
    "\n",
    "**Using Bayes' theorem:**\n",
    "$$P(\\text{Comes} \\mid \\text{Says Yes}) = \\frac{P(\\text{Says Yes} \\mid \\text{Comes}) \\cdot P(\\text{Comes})}{P(\\text{Says Yes})}$$\n",
    "\n",
    "$$= \\frac{(1.0)(0.3)}{0.8} = \\frac{0.3}{0.8} = 0.375 = 37.5\\%$$\n",
    "\n",
    "**Result:** Despite confirmation, the probability of attendance is 37.5%.\n",
    "\n",
    "**Analysis:** The low posterior probability results from the high base rate of confirmations (80%) relative to the actual attendance rate (30%). Most confirmations do not result in attendance.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Example: Email Spam Classification\n",
    "\n",
    "Email characteristics:\n",
    "- **Prior:** $P(\\text{Spam}) = 0.2$\n",
    "- **Likelihood:** $P(\\text{Contains \"FREE MONEY\"} \\mid \\text{Spam}) = 0.9$\n",
    "- **Marginal:** $P(\\text{Contains \"FREE MONEY\"}) = 0.25$\n",
    "\n",
    "For an email containing \"FREE MONEY\", compute:\n",
    "\n",
    "$$P(\\text{Spam} \\mid \\text{\"FREE MONEY\"}) = \\frac{(0.9)(0.2)}{0.25} = \\frac{0.18}{0.25} = 0.72 = 72\\%$$\n",
    "\n",
    "**Result:** The email has a 72% probability of being spam.\n",
    "\n",
    "**Process:**\n",
    "1. Prior belief: 20% of emails are spam\n",
    "2. Evidence observed: \"FREE MONEY\" present\n",
    "3. Updated belief (posterior): 72% probability of spam\n",
    "\n",
    "This exemplifies Bayesian updating, the foundation of spam filters and probabilistic classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Independence\n",
    "\n",
    "**Definition 5 (Independence):** Two random variables $A$ and $B$ are **independent** (denoted $A \\perp B$) if and only if:\n",
    "\n",
    "$$P(A, B) = P(A)P(B) \\tag{2.6.9}$$\n",
    "\n",
    "for all values of $A$ and $B$.\n",
    "\n",
    "**Interpretation:** Knowing $B$ provides no information about $A$ - the occurrence of one event does not affect the probability of the other.\n",
    "\n",
    "**Example:** Rolling two fair dice\n",
    "\n",
    "Let $A$ = \"first die shows 6\" and $B$ = \"second die shows 6\"\n",
    "\n",
    "- $P(A) = 1/6$\n",
    "- $P(B) = 1/6$\n",
    "- $P(A, B) = 1/36 = (1/6)(1/6) = P(A)P(B)$\n",
    "\n",
    "Therefore, $A \\perp B$ (the two dice are independent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Expectation and Variance\n",
    "\n",
    "### 5.1 Expectation (Expected Value)\n",
    "\n",
    "**Definition 5.1 (Expectation - Discrete Case):**\n",
    "For a discrete random variable $X$ with possible values $x_1, x_2, \\ldots$:\n",
    "\n",
    "$$E[X] = \\sum_{i} x_i P(X = x_i) \\tag{2.6.12}$$\n",
    "where the sum is over all possible values of $X$.\n",
    "\n",
    "**Conditions for existence:** The expectation exists if and only if $\\sum_{x} |x| \\cdot P(X = x) < \\infty$.\n",
    "\n",
    "**Alternative Notation:** $E[X] = E_{X \\sim P}[X] = \\mu_X = \\mu$\n",
    "**Theorem 5.1.1 (Expectation of a Function)**\n",
    "\n",
    "For any function $f$ of a random variable $X$:\n",
    "\n",
    "$$E[f(X)] = \\sum_{i} f(x_i) P(X = x_i) \\tag{2.6.13}$$\n",
    "\n",
    "**Example:** For $f(x) = x^2$:\n",
    "$$E[X^2] = \\sum_{i} x_i^2 P(X = x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Properties of Expectation\n",
    "\n",
    "**Theorem 5.2 (Linearity of Expectation):**\n",
    "For random variables $X$ and $Y$ and constants $a, b \\in \\mathbb{R}$:\n",
    "$$E[aX + bY] = aE[X] + bE[Y]$$\n",
    "\n",
    "**Proof (Discrete Case):**\n",
    "\n",
    "**Step 1:** Write the expectation:\n",
    "$$E[aX + bY] = \\sum_{x, y} (ax + by) \\cdot P(X = x, Y = y)$$\n",
    "\n",
    "**Step 2:** Distribute:\n",
    "$$= \\sum_{x, y} ax \\cdot P(X = x, Y = y) + \\sum_{x, y} by \\cdot P(X = x, Y = y)$$\n",
    "\n",
    "**Step 3:** Factor out constants:\n",
    "$$= a \\sum_{x, y} x \\cdot P(X = x, Y = y) + b \\sum_{x, y} y \\cdot P(X = x, Y = y)$$\n",
    "\n",
    "**Step 4:** For the first term, marginalize over $y$:\n",
    "$$\\sum_{x, y} x \\cdot P(X = x, Y = y) = \\sum_{x} x \\sum_{y} P(X = x, Y = y) = \\sum_{x} x \\cdot P(X = x) = E[X]$$\n",
    "\n",
    "**Step 5:** Similarly for the second term:\n",
    "$$\\sum_{x, y} y \\cdot P(X = x, Y = y) = E[Y]$$\n",
    "\n",
    "**Step 6:** Combine:\n",
    "$$E[aX + bY] = aE[X] + bE[Y]$$\n",
    "\n",
    "**Important Note:** Linearity holds **regardless of whether $X$ and $Y$ are independent**. This is a powerful property.\n",
    "\n",
    "**Corollary:** For constants $a$ and $b$:\n",
    "- $E[aX] = aE[X]$\n",
    "- $E[X + b] = E[X] + b$\n",
    "- $E[a] = a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Variance\n",
    "\n",
    "**Definition 5.4 (Variance):**\n",
    "The variance measures the spread of a random variable around its mean:\n",
    "\n",
    "$$\\text{Var}[X] = E\\left[(X - \\mu)^2\\right] \\tag{2.6.14}$$\n",
    "\n",
    "where $\\mu = E[X]$ is the expected value of $X$.\n",
    "\n",
    "**Alternative Notation:** $\\text{Var}[X] = \\sigma_X^2 = \\sigma^2$\n",
    "\n",
    "**Theorem 5.2 (Computational Formula for Variance):**\n",
    "\n",
    "$$\\text{Var}[X] = E[X^2] - (E[X])^2 \\tag{2.6.15}$$\n",
    "\n",
    "**Proof:**\n",
    "$$\\begin{aligned}\n",
    "\\text{Var}[X] &= E[(X - \\mu)^2] \\\\\n",
    "&= E[X^2 - 2\\mu X + \\mu^2] \\\\\n",
    "&= E[X^2] - 2\\mu E[X] + \\mu^2 \\\\\n",
    "&= E[X^2] - 2\\mu^2 + \\mu^2 \\\\\n",
    "&= E[X^2] - \\mu^2 \\\\\n",
    "&= E[X^2] - (E[X])^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Step 1:** Start with definition:\n",
    "$$\\text{Var}[X] = E\\left[(X - E[X])^2\\right]$$\n",
    "\n",
    "**Step 2:** Let $\\mu = E[X]$ for notational simplicity:\n",
    "$$\\text{Var}[X] = E\\left[(X - \\mu)^2\\right]$$\n",
    "\n",
    "**Step 3:** Expand the square:\n",
    "$$= E\\left[X^2 - 2\\mu X + \\mu^2\\right]$$\n",
    "\n",
    "**Step 4:** Apply linearity of expectation:\n",
    "$$= E[X^2] - E[2\\mu X] + E[\\mu^2]$$\n",
    "\n",
    "**Step 5:** Factor out constants:\n",
    "$$= E[X^2] - 2\\mu E[X] + \\mu^2$$\n",
    "\n",
    "**Step 6:** Substitute $\\mu = E[X]$:\n",
    "$$= E[X^2] - 2E[X] \\cdot E[X] + (E[X])^2$$\n",
    "\n",
    "**Step 7:** Simplify:\n",
    "$$= E[X^2] - 2(E[X])^2 + (E[X])^2$$\n",
    "\n",
    "**Step 8:** Combine like terms:\n",
    "$$= E[X^2] - (E[X])^2$$\n",
    "\n",
    "Therefore $\\text{Var}[X] = E[X^2] - (E[X])^2$. \n",
    "\n",
    "**Interpretation:** Variance is the difference between the \"mean of squares\" and \"square of mean.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Properties of Variance\n",
    "\n",
    "**Theorem 5.3 (Variance of Scaled Random Variable):**\n",
    "For any constant $c$:\n",
    "$$\\text{Var}[cX] = c^2 \\text{Var}[X]$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Use the computational formula:\n",
    "$$\\text{Var}[cX] = E[(cX)^2] - (E[cX])^2$$\n",
    "\n",
    "**Step 2:** Simplify:\n",
    "$$= E[c^2 X^2] - (cE[X])^2$$\n",
    "$$= c^2 E[X^2] - c^2 (E[X])^2$$\n",
    "$$= c^2 (E[X^2] - (E[X])^2)$$\n",
    "$$= c^2 \\text{Var}[X]$$\n",
    "\n",
    "**Theorem 5.4 (Variance is Shift-Invariant):**\n",
    "For any constant $c$:\n",
    "$$\\text{Var}[X + c] = \\text{Var}[X]$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Note that $E[X + c] = E[X] + c$.\n",
    "\n",
    "**Step 2:** Apply definition:\n",
    "$$\\text{Var}[X + c] = E[(X + c - E[X + c])^2] = E[(X + c - E[X] - c)^2] = E[(X - E[X])^2] = \\text{Var}[X]$$\n",
    "\n",
    "**Theorem 5.5 (Variance of Sum - Independent Case):**\n",
    "If $X$ and $Y$ are **independent**, then:\n",
    "$$\\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y]$$\n",
    "\n",
    "**Note:** This does not hold in general for dependent random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Standard Deviation\n",
    "\n",
    "**Definition 5.5 (Standard Deviation):**\n",
    "The **standard deviation** is the square root of variance:\n",
    "$$\\sigma_X = \\sqrt{\\text{Var}[X]}$$\n",
    "\n",
    "**Purpose:** Standard deviation is expressed in the same units as the original random variable, making it more interpretable than variance.\n",
    "\n",
    "**Example:** If $X$ represents height in centimeters:\n",
    "- $\\text{Var}[X]$ has units of cm² (squared centimeters)\n",
    "- $\\sigma_X$ has units of cm (centimeters)\n",
    "\n",
    "**Properties:**\n",
    "- $\\sigma_X \\geq 0$\n",
    "- $\\sigma_{cX} = |c| \\sigma_X$\n",
    "- $\\sigma_{X+c} = \\sigma_X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Practical Examples\n",
    "\n",
    "HIV Test Example - Complete Calculation\n",
    "\n",
    "**Problem Setup:**\n",
    "\n",
    "**Given Information:**\n",
    "- Let $H = 1$ denote \"has HIV\", $H = 0$ denote \"does not have HIV\"\n",
    "- Let $D_1 = 1$ denote \"first test is positive\", $D_1 = 0$ denote \"first test is negative\"\n",
    "- Prior probability of having HIV: $P(H = 1) = 0.0015$\n",
    "- Consequently: $P(H = 0) = 1 - 0.0015 = 0.9985$\n",
    "- Test sensitivity (true positive rate): $P(D_1 = 1 \\mid H = 1) = 1.0$\n",
    "- False positive rate: $P(D_1 = 1 \\mid H = 0) = 0.01$\n",
    "\n",
    "**Question:** What is the probability of having HIV given a positive test result, i.e., $P(H = 1 \\mid D_1 = 1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Solution - Single Test\n",
    "\n",
    "**Step 1:** Apply Bayes' theorem:\n",
    "$$P(H = 1 \\mid D_1 = 1) = \\frac{P(D_1 = 1 \\mid H = 1) P(H = 1)}{P(D_1 = 1)}$$\n",
    "\n",
    "**Step 2:** Compute $P(D_1 = 1)$ using marginalization:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(D_1 = 1) &= P(D_1 = 1, H = 0) + P(D_1 = 1, H = 1) \\\\\n",
    "&= P(D_1 = 1 \\mid H = 0)P(H = 0) + P(D_1 = 1 \\mid H = 1)P(H = 1) \\\\\n",
    "&= 0.01 \\times 0.9985 + 1 \\times 0.0015 \\\\\n",
    "&= 0.011485\n",
    "\\end{aligned} \\tag{2.6.7}$$\n",
    "\n",
    "**Step 3:** Substitute values:\n",
    "$$P(D_1 = 1) = (0.01)(0.9985) + (1.0)(0.0015)$$\n",
    "\n",
    "**Step 4:** Compute marginal probability:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(D_1 = 1, D_2 = 1) &= P(D_1 = 1, D_2 = 1 \\mid H = 0)P(H = 0) + P(D_1 = 1, D_2 = 1 \\mid H = 1)P(H = 1) \\\\\n",
    "&= 0.0003 \\times 0.9985 + 0.98 \\times 0.0015 \\\\\n",
    "&= 0.00029955 + 0.00147 \\\\\n",
    "&= 0.00176955\n",
    "\\end{aligned} \\tag{2.6.10}$$\n",
    "\n",
    "**Step 5:** Compute the numerator:\n",
    "$$P(D_1 = 1 \\mid H = 1) P(H = 1) = (1.0)(0.0015) = 0.0015$$\n",
    "\n",
    "**Step 6:** Apply Bayes' theorem:\n",
    "$$P(H = 1 \\mid D_1 = 1) = \\frac{P(D_1 = 1 \\mid H = 1)P(H = 1)}{P(D_1 = 1)} = \\frac{1 \\times 0.0015}{0.011485} = 0.1306 \\tag{2.6.8}$$\n",
    "\n",
    "**Conclusion:** There is only a **13.06%** chance the patient actually has HIV, despite testing positive!\n",
    "\n",
    "**Conclusion:** The probability of actually having HIV given a positive test is approximately **13.06%** or about **1 in 8**.\n",
    "\n",
    "**Interpretation:** Despite a positive test, the probability of actually having the disease is relatively low because:\n",
    "1. The disease is rare (low prior: 0.15%)\n",
    "2. The false positive rate (1%) is much higher than the disease prevalence\n",
    "3. Most positive tests are false positives, not true positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Solution - Two Tests\n",
    "\n",
    "**Extended Problem:**\n",
    "A second test is administered with properties:\n",
    "- Sensitivity: $P(D_2 = 1 \\mid H = 1) = 0.98$\n",
    "- False positive rate: $P(D_2 = 1 \\mid H = 0) = 0.03$\n",
    "\n",
    "**Assumption:** The tests are **conditionally independent given $H$**, meaning:\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H) = P(D_1 = 1 \\mid H) P(D_2 = 1 \\mid H)$$\n",
    "\n",
    "**Question:** What is $P(H = 1 \\mid D_1 = 1, D_2 = 1)$?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "**Step 1:** Apply Bayes' theorem:\n",
    "$$\\begin{aligned}\n",
    "P(D_1 = 1, D_2 = 1 \\mid H = 0) &= P(D_1 = 1 \\mid H = 0) P(D_2 = 1 \\mid H = 0) = 0.01 \\times 0.03 = 0.0003 \\\\\n",
    "P(D_1 = 1, D_2 = 1 \\mid H = 1) &= P(D_1 = 1 \\mid H = 1) P(D_2 = 1 \\mid H = 1) = 1 \\times 0.98 = 0.98\n",
    "\\end{aligned} \\tag{2.6.9}$$\n",
    "\n",
    "**Step 2:** Use conditional independence for $H = 1$:\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 1) = (1.0)(0.98) = 0.98$$\n",
    "\n",
    "**Step 3:** Use conditional independence for $H = 0$:\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 0) = (0.01)(0.03) = 0.0003$$\n",
    "\n",
    "**Step 4:** Compute marginal probability:\n",
    "$$\\begin{aligned}\n",
    "P(D_1 = 1, D_2 = 1) &= P(D_1 = 1, D_2 = 1 \\mid H = 0)P(H = 0) + P(D_1 = 1, D_2 = 1 \\mid H = 1)P(H = 1) \\\\\n",
    "&= 0.0003 \\times 0.9985 + 0.98 \\times 0.0015 \\\\\n",
    "&= 0.00029955 + 0.00147 \\\\\n",
    "&= 0.00176955\n",
    "\\end{aligned} \\tag{2.6.10}$$\n",
    "\n",
    "\n",
    "**Step 5:** Compute numerator:\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 1) P(H = 1) = (0.98)(0.0015) = 0.00147$$\n",
    "\n",
    "**Step 6:** Apply Bayes' theorem:\n",
    "$$P(H = 1 \\mid D_1 = 1, D_2 = 1) = \\frac{P(D_1 = 1, D_2 = 1 \\mid H = 1) P(H = 1)}{P(D_1 = 1, D_2 = 1)} = \\frac{0.98 \\times 0.0015}{0.00176955} = 0.8307 \\tag{2.6.11}$$\n",
    "\n",
    "\n",
    "**Conclusion:** With two positive tests, the probability of having HIV increases to approximately **83.07%**.\n",
    "\n",
    "**Interpretation:** The second positive test provides significant additional evidence, increasing confidence from 13.06% to 83.07%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Verify D2L 2.6 HIV Testing Calculations\n",
    "\n",
    "# Given probabilities\n",
    "P_H1 = 0.0015  # P(HIV+)\n",
    "P_H0 = 1 - P_H1  # P(HIV-)\n",
    "\n",
    "# Test 1 parameters\n",
    "P_D1_1_given_H1 = 1.00  # Sensitivity (never misses HIV+)\n",
    "P_D1_1_given_H0 = 0.01  # False positive rate\n",
    "\n",
    "# Test 2 parameters (different from Test 1!)\n",
    "P_D2_1_given_H1 = 0.98\n",
    "P_D2_1_given_H0 = 0.03\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"D2L 2.6 HIV Testing Example Verification\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Equation (2.6.7): P(D1=1)\n",
    "P_D1_1 = P_D1_1_given_H0 * P_H0 + P_D1_1_given_H1 * P_H1\n",
    "print(f\"\\n(2.6.7) P(D₁=1) = {P_D1_1:.6f}\")\n",
    "\n",
    "# Equation (2.6.8): P(H=1|D1=1)\n",
    "P_H1_given_D1_1 = (P_D1_1_given_H1 * P_H1) / P_D1_1\n",
    "print(f\"(2.6.8) P(H=1|D₁=1) = {P_H1_given_D1_1:.4f} ({P_H1_given_D1_1*100:.2f}%)\")\n",
    "\n",
    "# Equation (2.6.9): Joint probabilities for two tests\n",
    "P_D1D2_given_H0 = P_D1_1_given_H0 * P_D2_1_given_H0\n",
    "P_D1D2_given_H1 = P_D1_1_given_H1 * P_D2_1_given_H1\n",
    "print(f\"\\n(2.6.9) P(D₁=1,D₂=1|H=0) = {P_D1D2_given_H0:.4f}\")\n",
    "print(f\"(2.6.9) P(D₁=1,D₂=1|H=1) = {P_D1D2_given_H1:.4f}\")\n",
    "\n",
    "# Equation (2.6.10): P(D1=1, D2=1)\n",
    "P_D1D2 = P_D1D2_given_H0 * P_H0 + P_D1D2_given_H1 * P_H1\n",
    "print(f\"\\n(2.6.10) P(D₁=1,D₂=1) = {P_D1D2:.8f}\")\n",
    "\n",
    "# Equation (2.6.11): P(H=1|D1=1,D2=1)\n",
    "P_H1_given_D1D2 = (P_D1D2_given_H1 * P_H1) / P_D1D2\n",
    "print(f\"(2.6.11) P(H=1|D₁=1,D₂=1) = {P_H1_given_D1D2:.4f} ({P_H1_given_D1D2*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Important Inequalities and Limit Theorems\n",
    "\n",
    "### 7.1 Chebyshev's Inequality\n",
    "\n",
    "**Theorem 7.1 (Chebyshev's Inequality):**\n",
    "For any random variable $X$ with mean $\\mu$ and variance $\\sigma^2$, and for any $k > 0$:\n",
    "$$P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}$$\n",
    "\n",
    "**Equivalently:** For any $\\epsilon > 0$:\n",
    "$$P(|X - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Step 1:** Define an indicator random variable:\n",
    "$$I = \\begin{cases} 1 & \\text{if } |X - \\mu| \\geq \\epsilon \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "**Step 2:** Note that $E[I] = P(|X - \\mu| \\geq \\epsilon)$.\n",
    "\n",
    "**Step 3:** Observe that for all outcomes:\n",
    "$$I \\leq \\frac{(X - \\mu)^2}{\\epsilon^2}$$\n",
    "\n",
    "*Justification:*\n",
    "- If $|X - \\mu| \\geq \\epsilon$: Then $(X - \\mu)^2 \\geq \\epsilon^2$, so $\\frac{(X - \\mu)^2}{\\epsilon^2} \\geq 1 = I$\n",
    "- If $|X - \\mu| < \\epsilon$: Then $I = 0$ and $\\frac{(X - \\mu)^2}{\\epsilon^2} \\geq 0 = I$\n",
    "\n",
    "**Step 4:** Take expectations:\n",
    "$$E[I] \\leq E\\left[\\frac{(X - \\mu)^2}{\\epsilon^2}\\right] = \\frac{\\sigma^2}{\\epsilon^2}$$\n",
    "\n",
    "**Step 5:** Therefore:\n",
    "$$P(|X - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- For $k = 2$: At least 75% of values lie within 2 standard deviations of the mean\n",
    "- For $k = 3$: At least 88.9% of values lie within 3 standard deviations of the mean\n",
    "\n",
    "**Note:** This bound applies to ANY distribution with finite variance, but is often loose for specific distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Law of Large Numbers\n",
    "\n",
    "**Theorem 7.2 (Weak Law of Large Numbers):**\n",
    "Let $X_1, X_2, \\ldots, X_n$ be independent and identically distributed (i.i.d.) random variables with mean $\\mu$ and finite variance $\\sigma^2$. Then the sample average\n",
    "$$\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$$\n",
    "converges in probability to $\\mu$ as $n \\to \\infty$:\n",
    "$$\\lim_{n \\to \\infty} P(|\\bar{X}_n - \\mu| > \\epsilon) = 0 \\quad \\text{for all } \\epsilon > 0$$\n",
    "\n",
    "**Proof Sketch:**\n",
    "\n",
    "**Step 1:** Compute the mean of $\\bar{X}_n$:\n",
    "$$E[\\bar{X}_n] = \\frac{1}{n} \\sum_{i=1}^n E[X_i] = \\frac{1}{n} \\cdot n\\mu = \\mu$$\n",
    "\n",
    "**Step 2:** Compute the variance of $\\bar{X}_n$:\n",
    "$$\\text{Var}[\\bar{X}_n] = \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}[X_i] = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "**Step 3:** Apply Chebyshev's inequality:\n",
    "$$P(|\\bar{X}_n - \\mu| \\geq \\epsilon) \\leq \\frac{\\text{Var}[\\bar{X}_n]}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2}$$\n",
    "\n",
    "**Step 4:** Take the limit:\n",
    "$$\\lim_{n \\to \\infty} P(|\\bar{X}_n - \\mu| \\geq \\epsilon) \\leq \\lim_{n \\to \\infty} \\frac{\\sigma^2}{n\\epsilon^2} = 0$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Empirical averages converge to theoretical means\n",
    "- Foundation for statistical inference\n",
    "- Justifies using sample means to estimate population means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Central Limit Theorem\n",
    "\n",
    "**Theorem 9.3 (Central Limit Theorem):**\n",
    "Let $X_1, X_2, \\ldots, X_n$ be i.i.d. random variables with mean $\\mu$ and variance $\\sigma^2 < \\infty$. Then the standardized sample average\n",
    "$$Z_n = \\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma}$$\n",
    "converges in distribution to a standard normal distribution $N(0,1)$ as $n \\to \\infty$.\n",
    "\n",
    "**Equivalently:**\n",
    "$$\\bar{X}_n \\approx N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ for large } n$$\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Convergence Rate:** The standard error is $\\sigma/\\sqrt{n}$, meaning:\n",
    "   - To halve the error, need 4× more samples\n",
    "   - To reduce error by factor of 10, need 100× more samples\n",
    "\n",
    "2. **Universality:** The CLT applies regardless of the original distribution (as long as variance exists)\n",
    "\n",
    "3. **Rule of Thumb:** CLT approximation is usually good for $n \\geq 30$\n",
    "\n",
    "**Applications:**\n",
    "- Confidence intervals\n",
    "- Hypothesis testing\n",
    "- Monte Carlo estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1]. Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2021). Dive into Deep Learning. https://d2l.ai/\n",
    "\n",
    "[2]. Ross, S. M. (2014). A First Course in Probability (9th ed.). Pearson.\n",
    "\n",
    "[3]. Bertsekas, D. P., & Tsitsiklis, J. N. (2008). Introduction to Probability (2nd ed.). Athena Scientific.\n",
    "\n",
    "[4]. Wikipedia contributors. (2024). Probability axioms. Wikipedia. https://en.wikipedia.org/wiki/Probability_axioms\n",
    "\n",
    "[5]. Wikipedia contributors. (2024). Bayes' theorem. Wikipedia. https://en.wikipedia.org/wiki/Bayes%27_theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "## Exercise 1: Epistemic Uncertainty Reduction\n",
    "\n",
    "**Problem:** Give an example where observing more data can reduce uncertainty to an arbitrarily low level.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Estimating a Coin's Bias\n",
    "\n",
    "**Setup:**\n",
    "- Unknown parameter: $p$ (true probability of heads)\n",
    "- Observable data: $X_1, X_2, \\ldots, X_n$ i.i.d. Bernoulli($p$)\n",
    "- Estimator: $\\hat{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "By the Law of Large Numbers:\n",
    "$$\\hat{p}_n \\xrightarrow{P} p \\quad \\text{as } n \\to \\infty$$\n",
    "\n",
    "The variance of our estimator:\n",
    "$$\\text{Var}[\\hat{p}_n] = \\frac{p(1-p)}{n} \\leq \\frac{1}{4n}$$\n",
    "\n",
    "This decreases as $O(1/n)$, approaching **zero** as $n \\to \\infty$.\n",
    "\n",
    "By Chebyshev's inequality:\n",
    "$$P(|\\hat{p}_n - p| \\geq \\epsilon) \\leq \\frac{1}{4n\\epsilon^2} \\to 0$$\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "As $n \\to \\infty$, uncertainty about $p$ can be made **arbitrarily small**.\n",
    "\n",
    "This is **epistemic uncertainty** - reducible by collecting more data.\n",
    "\n",
    "---\n",
    "\n",
    "### Other Examples\n",
    "- Estimating population mean from samples\n",
    "- Measuring a physical constant with repeated experiments\n",
    "- Estimating parameters of a known distribution family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXERCISE 1: EPISTEMIC UNCERTAINTY REDUCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "p_true = 0.6\n",
    "\n",
    "print(f\"\\nExample: Estimating coin bias (true p = {p_true})\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Show variance decreasing\n",
    "print(\"\\nVariance of estimator Var[p̂_n] ≤ 1/(4n):\")\n",
    "for n in [10, 100, 1000, 10000]:\n",
    "    var_bound = 1 / (4 * n)\n",
    "    print(f\"  n = {n:5d}: Var ≤ {var_bound:.8f}\")\n",
    "\n",
    "# Demonstrate convergence\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Simulation: Running estimate of p\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "for n in [100, 1000, 10000]:\n",
    "    flips = np.random.binomial(1, p_true, n)\n",
    "    estimate = np.mean(flips)\n",
    "    error = abs(estimate - p_true)\n",
    "    print(f\"  n = {n:5d}: p̂ = {estimate:.4f}, |p̂ - p| = {error:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Conclusion:\")\n",
    "print(\"-\"*60)\n",
    "print(\"  As n → ∞, Var[p̂_n] → 0\")\n",
    "print(\"  Uncertainty can be reduced to ARBITRARILY LOW level\")\n",
    "print(\"  This is EPISTEMIC uncertainty (reducible with data)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Aleatoric Uncertainty Limit\n",
    "\n",
    "**Problem:** Give an example where observing more data will only reduce uncertainty up to a point and then no further. Explain why and where this point occurs.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Predicting Individual Coin Flips\n",
    "\n",
    "**Setup:** Coin with bias $p = 0.7$ (probability of heads)\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "Even with infinite data, we can only learn $p$ perfectly. But predicting the **next flip** remains uncertain:\n",
    "\n",
    "$$P(X = \\text{Heads}) = 0.7, \\quad P(X = \\text{Tails}) = 0.3$$\n",
    "\n",
    "**Irreducible Variance:**\n",
    "$$\\text{Var}[X] = p(1-p) = 0.7 \\times 0.3 = 0.21$$\n",
    "\n",
    "This variance **cannot be reduced** by more data - it's intrinsic to the random process.\n",
    "\n",
    "---\n",
    "\n",
    "### The Stopping Point\n",
    "\n",
    "| Uncertainty Type | Reducible? | Limit |\n",
    "|------------------|------------|-------|\n",
    "| **Epistemic** (about $p$) | Yes, with more data | → 0 |\n",
    "| **Aleatoric** (about next flip) | No, intrinsic | $\\text{Var}[X] = 0.21$ |\n",
    "\n",
    "**Stopping point:** When $\\hat{p}_n \\to p$ (estimate converges to true value).\n",
    "\n",
    "Beyond this point, uncertainty is **aleatoric** (irreducible randomness).\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Happens\n",
    "\n",
    "- **Epistemic uncertainty:** \"We don't know $p$\" → reducible with data\n",
    "- **Aleatoric uncertainty:** \"Even knowing $p$, outcome is random\" → irreducible\n",
    "\n",
    "**Other examples:**\n",
    "- Quantum measurement outcomes\n",
    "- Thermal noise in physical systems\n",
    "- Individual customer purchase decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXERCISE 2: ALEATORIC UNCERTAINTY LIMIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "p_true = 0.7\n",
    "\n",
    "# Epistemic uncertainty (about p) - reduces with more data\n",
    "print(\"\\nEpistemic Uncertainty (about p):\")\n",
    "print(\"-\"*60)\n",
    "for n in [10, 100, 1000, 10000]:\n",
    "    std_error = np.sqrt(p_true * (1 - p_true) / n)\n",
    "    print(f\"  n = {n:5d}: Std Error of p̂ = {std_error:.6f}\")\n",
    "\n",
    "# Aleatoric uncertainty (about next flip) - constant\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Aleatoric Uncertainty (about next outcome):\")\n",
    "print(\"-\"*60)\n",
    "aleatoric_var = p_true * (1 - p_true)\n",
    "print(f\"  Var[X] = p(1-p) = {p_true} × {1-p_true} = {aleatoric_var:.2f}\")\n",
    "print(f\"  This is CONSTANT regardless of sample size!\")\n",
    "\n",
    "# Stopping point\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Stopping Point:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  Epistemic uncertainty → 0 as n → ∞\")\n",
    "print(f\"  Aleatoric uncertainty remains at Var[X] = {aleatoric_var:.2f}\")\n",
    "print(f\"\\n  The stopping point occurs when p̂_n ≈ p\")\n",
    "print(f\"  Beyond this, only irreducible randomness remains.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Variance Analysis for Coin Tosses\n",
    "\n",
    "**Problem:** Calculate variance of estimate $\\hat{p}_n$ after $n$ samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (a): Variance Scaling\n",
    "\n",
    "**Setup:** $X_i \\sim \\text{Bernoulli}(p)$, estimator $\\hat{p}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$\n",
    "\n",
    "$$\\text{Var}[\\hat{p}_n] = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}[X_i] = \\frac{n \\cdot p(1-p)}{n^2} = \\frac{p(1-p)}{n}$$\n",
    "\n",
    "**Answer:** Variance scales as $O(1/n)$.\n",
    "\n",
    "For fair coin ($p = 0.5$): $\\text{Var}[\\hat{p}_n] = \\frac{0.25}{n} = \\frac{1}{4n}$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (b): Chebyshev Bound\n",
    "\n",
    "$$P(|\\hat{p}_n - p| \\geq \\epsilon) \\leq \\frac{\\text{Var}[\\hat{p}_n]}{\\epsilon^2} = \\frac{p(1-p)}{n\\epsilon^2}$$\n",
    "\n",
    "For $p = 0.5$: $P(|\\hat{p}_n - 0.5| \\geq \\epsilon) \\leq \\frac{1}{4n\\epsilon^2}$\n",
    "\n",
    "**Example:** For $\\epsilon = 0.01$, 95% confidence:\n",
    "$$\\frac{1}{4n(0.01)^2} \\leq 0.05 \\implies n \\geq \\frac{1}{4 \\times 0.05 \\times 0.0001} = 50{,}000$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (c): Relation to CLT\n",
    "\n",
    "By CLT: $\\frac{\\hat{p}_n - p}{\\sqrt{p(1-p)/n}} \\xrightarrow{d} N(0,1)$\n",
    "\n",
    "For $\\epsilon = 0.01$, 95% confidence using CLT:\n",
    "$$n \\approx \\left(\\frac{z_{0.975} \\cdot 0.5}{0.01}\\right)^2 = \\left(\\frac{1.96 \\times 0.5}{0.01}\\right)^2 \\approx 9{,}604$$\n",
    "\n",
    "| Approach | Required $n$ (95% conf, $\\epsilon=0.01$) |\n",
    "|----------|------------------------------------------|\n",
    "| Chebyshev | $n \\geq 50{,}000$ |\n",
    "| CLT | $n \\approx 9{,}604$ |\n",
    "\n",
    "CLT provides **tighter bounds** by using distributional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXERCISE 3: VARIANCE ANALYSIS FOR COIN TOSSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "p = 0.5\n",
    "epsilon = 0.01\n",
    "confidence = 0.95\n",
    "alpha = 1 - confidence\n",
    "\n",
    "# Part (a): Variance scaling\n",
    "print(\"\\nPart (a): Variance Scaling\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Var[p̂_n] = p(1-p)/n = {p*(1-p)}/n = 0.25/n\")\n",
    "print(\"\\nExamples:\")\n",
    "for n in [100, 1000, 10000]:\n",
    "    var = p * (1 - p) / n\n",
    "    print(f\"  n = {n:5d}: Var = {var:.6f}\")\n",
    "\n",
    "# Part (b): Chebyshev bound\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part (b): Chebyshev Bound\")\n",
    "print(\"-\"*60)\n",
    "print(f\"P(|p̂_n - p| ≥ ε) ≤ 1/(4nε²)\")\n",
    "print(f\"\\nFor ε = {epsilon}, {confidence:.0%} confidence:\")\n",
    "n_chebyshev = int(np.ceil(1 / (4 * alpha * epsilon**2)))\n",
    "print(f\"  n ≥ 1/(4 × {alpha} × {epsilon}²) = {n_chebyshev:,}\")\n",
    "\n",
    "# Part (c): CLT comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part (c): CLT Comparison\")\n",
    "print(\"-\"*60)\n",
    "z = stats.norm.ppf(1 - alpha/2)\n",
    "n_clt = int(np.ceil((z * 0.5 / epsilon)**2))\n",
    "print(f\"Using CLT with z_{1-alpha/2} = {z:.3f}:\")\n",
    "print(f\"  n ≈ (z × σ / ε)² = ({z:.2f} × 0.5 / {epsilon})² ≈ {n_clt:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary (95% confidence, ε = 0.01):\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  Chebyshev: n ≥ {n_chebyshev:,}\")\n",
    "print(f\"  CLT:       n ≈ {n_clt:,}\")\n",
    "print(f\"  CLT needs {n_chebyshev/n_clt:.1f}x fewer samples!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Independence of Sample Averages\n",
    "\n",
    "**Problem:** Draw $m$ samples $x_i$ from distribution with zero mean and unit variance. Compute averages $z_m = m^{-1} \\sum_{i=1}^{m} x_i$. Can we apply Chebyshev's inequality for every $z_m$ independently? Why or why not?\n",
    "\n",
    "---\n",
    "\n",
    "### Solution\n",
    "\n",
    "**Properties of $z_m$:**\n",
    "\n",
    "$$E[z_m] = \\frac{1}{m} \\sum_{i=1}^{m} E[x_i] = 0$$\n",
    "\n",
    "$$\\text{Var}[z_m] = \\frac{1}{m^2} \\sum_{i=1}^{m} \\text{Var}[x_i] = \\frac{m}{m^2} = \\frac{1}{m}$$\n",
    "\n",
    "**Chebyshev for single average:**\n",
    "\n",
    "$$P(|z_m| \\geq \\epsilon) \\leq \\frac{1}{m\\epsilon^2}$$\n",
    "\n",
    "This is valid for a **single** average.\n",
    "\n",
    "---\n",
    "\n",
    "### Can We Apply to Multiple Averages Independently?\n",
    "\n",
    "**Answer: NO**\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "1. **Dependence Through Shared Data:**\n",
    "   - If we compute overlapping averages (e.g., $z_{m_1}, z_{m_2}$) from same data, they share samples\n",
    "   - Shared samples create **correlation** between averages\n",
    "   - They are **NOT independent**\n",
    "\n",
    "2. **Union Bound Issue:**\n",
    "   - Naive: $P(\\text{any } |z_j| \\geq \\epsilon) \\leq \\sum_j P(|z_j| \\geq \\epsilon)$\n",
    "   - This only works for **mutually exclusive** events, not correlated ones\n",
    "\n",
    "3. **Correct Approach:**\n",
    "   - Use **Bonferroni correction**: require each average at level $1 - \\delta/k$\n",
    "   - Or use **independent** (non-overlapping) samples for each average\n",
    "\n",
    "---\n",
    "\n",
    "### Key Lesson\n",
    "\n",
    "Independence of random variables $x_i$ does **not** imply independence of statistics computed from overlapping subsets of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXERCISE 4: INDEPENDENCE OF SAMPLE AVERAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Setup\n",
    "m = 100  # sample size\n",
    "epsilon = 0.2\n",
    "\n",
    "print(f\"\\nSetup: m = {m} samples, E[X] = 0, Var[X] = 1\")\n",
    "\n",
    "# Properties of z_m\n",
    "print(f\"\\nProperties of z_m = (1/m) Σ x_i:\")\n",
    "print(f\"  E[z_m] = 0\")\n",
    "print(f\"  Var[z_m] = 1/m = {1/m:.4f}\")\n",
    "\n",
    "# Chebyshev bound\n",
    "chebyshev_bound = 1 / (m * epsilon**2)\n",
    "print(f\"\\nChebyshev bound for single average:\")\n",
    "print(f\"  P(|z_m| ≥ {epsilon}) ≤ 1/(m·ε²) = {chebyshev_bound:.4f}\")\n",
    "\n",
    "# Demonstrate dependence\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Demonstration: Overlapping Averages are DEPENDENT\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_trials = 10000\n",
    "\n",
    "# Two overlapping averages sharing m-1 samples\n",
    "correlations = []\n",
    "for _ in range(n_trials):\n",
    "    data = np.random.normal(0, 1, m + 1)\n",
    "    z1 = np.mean(data[:m])      # samples 0 to m-1\n",
    "    z2 = np.mean(data[1:m+1])   # samples 1 to m\n",
    "    correlations.append((z1, z2))\n",
    "\n",
    "correlations = np.array(correlations)\n",
    "corr_coef = np.corrcoef(correlations[:, 0], correlations[:, 1])[0, 1]\n",
    "\n",
    "print(f\"\\nTwo averages sharing {m-1} of {m} samples:\")\n",
    "print(f\"  Correlation coefficient: {corr_coef:.4f}\")\n",
    "print(f\"  Expected if independent: 0.0000\")\n",
    "print(f\"  → They are HIGHLY CORRELATED, not independent!\")\n",
    "\n",
    "# Answer\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "NO, we cannot apply Chebyshev independently to every z_m.\n",
    "\n",
    "Reason: If averages share data, they are DEPENDENT.\n",
    "- Shared samples create correlation\n",
    "- Union bound with independence assumption fails\n",
    "- Must use Bonferroni correction or non-overlapping samples\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Probability Bounds for Union and Intersection\n",
    "\n",
    "**Problem:** Given $P(A)$ and $P(B)$, compute upper and lower bounds on $P(A \\cup B)$ and $P(A \\cap B)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (a): Bounds on $P(A \\cup B)$\n",
    "\n",
    "**Inclusion-Exclusion:** $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n",
    "\n",
    "**Upper Bound:** Since $P(A \\cap B) \\geq 0$:\n",
    "$$P(A \\cup B) \\leq P(A) + P(B)$$\n",
    "\n",
    "**Lower Bound:** Since $A \\cup B$ contains both $A$ and $B$:\n",
    "$$P(A \\cup B) \\geq \\max\\{P(A), P(B)\\}$$\n",
    "\n",
    "$$\\boxed{\\max\\{P(A), P(B)\\} \\leq P(A \\cup B) \\leq \\min\\{1, P(A) + P(B)\\}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (b): Bounds on $P(A \\cap B)$\n",
    "\n",
    "**Upper Bound:** Since $A \\cap B \\subseteq A$ and $A \\cap B \\subseteq B$:\n",
    "$$P(A \\cap B) \\leq \\min\\{P(A), P(B)\\}$$\n",
    "\n",
    "**Lower Bound:** Since $P(A \\cup B) \\leq 1$:\n",
    "$$P(A \\cap B) = P(A) + P(B) - P(A \\cup B) \\geq P(A) + P(B) - 1$$\n",
    "\n",
    "$$\\boxed{\\max\\{0, P(A) + P(B) - 1\\} \\leq P(A \\cap B) \\leq \\min\\{P(A), P(B)\\}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXERCISE 5: PROBABILITY BOUNDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Summary formulas\n",
    "print(\"\\nBounds on P(A ∪ B):\")\n",
    "print(\"  max{P(A), P(B)} ≤ P(A ∪ B) ≤ min{1, P(A) + P(B)}\")\n",
    "\n",
    "print(\"\\nBounds on P(A ∩ B):\")\n",
    "print(\"  max{0, P(A) + P(B) - 1} ≤ P(A ∩ B) ≤ min{P(A), P(B)}\")\n",
    "\n",
    "# Examples\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Examples:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "examples = [(0.6, 0.4), (0.8, 0.7)]\n",
    "\n",
    "for p_a, p_b in examples:\n",
    "    union_lower = max(p_a, p_b)\n",
    "    union_upper = min(1, p_a + p_b)\n",
    "    inter_lower = max(0, p_a + p_b - 1)\n",
    "    inter_upper = min(p_a, p_b)\n",
    "    \n",
    "    print(f\"\\nP(A) = {p_a}, P(B) = {p_b}\")\n",
    "    print(f\"  Union:        {union_lower:.1f} ≤ P(A∪B) ≤ {union_upper:.1f}\")\n",
    "    print(f\"  Intersection: {inter_lower:.1f} ≤ P(A∩B) ≤ {inter_upper:.1f}\")\n",
    "\n",
    "# Venn Diagram\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "titles = ['Disjoint (A∩B = 0)', 'Partial Overlap', 'A ⊂ B (A∩B = A)']\n",
    "overlaps = [0, 0.5, 1]  # relative overlap\n",
    "\n",
    "for idx, (ax, title, overlap) in enumerate(zip(axes, titles, overlaps)):\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw circles\n",
    "    if overlap == 0:  # Disjoint\n",
    "        circle_a = patches.Circle((3, 3), 1.5, fill=False, edgecolor='blue', linewidth=2)\n",
    "        circle_b = patches.Circle((7, 3), 1.5, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.text(3, 3, 'A', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        ax.text(7, 3, 'B', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    elif overlap == 1:  # A ⊂ B\n",
    "        circle_b = patches.Circle((5, 3), 2, fill=False, edgecolor='red', linewidth=2)\n",
    "        circle_a = patches.Circle((5, 3), 1, fill=False, edgecolor='blue', linewidth=2)\n",
    "        ax.text(5, 3, 'A', ha='center', va='center', fontsize=14, fontweight='bold', color='blue')\n",
    "        ax.text(5, 4.5, 'B', ha='center', va='center', fontsize=14, fontweight='bold', color='red')\n",
    "    else:  # Partial overlap\n",
    "        circle_a = patches.Circle((4, 3), 1.5, fill=False, edgecolor='blue', linewidth=2)\n",
    "        circle_b = patches.Circle((6, 3), 1.5, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.text(3.2, 3, 'A', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        ax.text(6.8, 3, 'B', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        ax.text(5, 3, 'A∩B', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    ax.add_patch(circle_a if overlap != 1 else circle_b)\n",
    "    ax.add_patch(circle_b if overlap != 1 else circle_a)\n",
    "    if overlap == 1:\n",
    "        ax.add_patch(patches.Circle((5, 3), 1, fill=False, edgecolor='blue', linewidth=2))\n",
    "\n",
    "plt.suptitle('Venn Diagram: Different Overlap Scenarios', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Graphical Interpretation:\")\n",
    "print(\"  - Maximum overlap: A∩B = min{A, B} (one contains other)\")\n",
    "print(\"  - Minimum overlap: A∩B = max{0, P(A)+P(B)-1}\")\n",
    "print(\"  - Maximum union: A∪B approaches 1 when disjoint\")\n",
    "print(\"  - Minimum union: A∪B = max{A, B} when one contains other\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Markov Chain Simplification\n",
    "\n",
    "**Problem:** Assume we have random variables $A$, $B$, $C$ where $B$ only depends on $A$, and $C$ only depends on $B$. Simplify $P(A, B, C)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Solution\n",
    "\n",
    "**Given Structure:**\n",
    "- $B$ only depends on $A$: $B \\perp \\text{(past)} \\mid A$\n",
    "- $C$ only depends on $B$: $C \\perp A \\mid B$\n",
    "\n",
    "This is a **Markov chain**: $A \\to B \\to C$\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "**Step 1:** Start with chain rule:\n",
    "$$P(A, B, C) = P(C \\mid A, B) \\cdot P(B \\mid A) \\cdot P(A)$$\n",
    "\n",
    "**Step 2:** Apply Markov property ($C$ only depends on $B$, so given $B$, $C$ is independent of $A$):\n",
    "$$P(C \\mid A, B) = P(C \\mid B)$$\n",
    "\n",
    "**Step 3:** Substitute:\n",
    "\n",
    "$$\\boxed{P(A, B, C) = P(A) \\cdot P(B \\mid A) \\cdot P(C \\mid B)}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "- Start at $A$ with probability $P(A)$\n",
    "- Transition to $B$ with probability $P(B \\mid A)$\n",
    "- Transition to $C$ with probability $P(C \\mid B)$\n",
    "\n",
    "The joint probability is the product of these transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXERCISE 6: MARKOV CHAIN SIMPLIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nAnswer: P(A,B,C) = P(A) · P(B|A) · P(C|B)\")\n",
    "\n",
    "# Example: Weather model A → B → C\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Example: Weather Model\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "p_A = 0.7           # P(A = Sunny)\n",
    "p_B_given_A = 0.8   # P(B = Sunny | A = Sunny)\n",
    "p_C_given_B = 0.75  # P(C = Sunny | B = Sunny)\n",
    "\n",
    "p_ABC = p_A * p_B_given_A * p_C_given_B\n",
    "\n",
    "print(f\"P(A = Sunny) = {p_A}\")\n",
    "print(f\"P(B = Sunny | A = Sunny) = {p_B_given_A}\")\n",
    "print(f\"P(C = Sunny | B = Sunny) = {p_C_given_B}\")\n",
    "print(f\"\\nP(All Sunny) = {p_A} × {p_B_given_A} × {p_C_given_B} = {p_ABC}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Medical Testing with Conditional Dependence\n",
    "\n",
    "**Problem (D2L 2.6.5):** Assume the two tests are not independent.\n",
    "\n",
    "**Given:**\n",
    "- False positive rate: $P(D = 1 \\mid H = 0) = 0.10$ (10%)\n",
    "- False negative rate: $P(D = 0 \\mid H = 1) = 0.01$ (1%)\n",
    "- Sensitivity: $P(D = 1 \\mid H = 1) = 0.99$ (99%)\n",
    "- For **infected** ($H = 1$): tests are conditionally **independent**\n",
    "- For **healthy** ($H = 0$): $P(D_2 = 1 \\mid D_1 = 1, H = 0) = 0.5$\n",
    "- Baseline: $P(H = 1) = 0.0015$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (a): Joint Probability Table for $D_1, D_2$ given $H = 0$\n",
    "\n",
    "**Step 1:** Marginal probabilities for $D_1$ given $H = 0$:\n",
    "$$P(D_1 = 1 \\mid H = 0) = 0.10, \\quad P(D_1 = 0 \\mid H = 0) = 0.90$$\n",
    "\n",
    "**Step 2:** Conditional probabilities for $D_2$:\n",
    "- $P(D_2 = 1 \\mid D_1 = 1, H = 0) = 0.50$ (given)\n",
    "- $P(D_2 = 1 \\mid D_1 = 0, H = 0) = 0.10$ (assume marginal when $D_1 = 0$)\n",
    "\n",
    "**Step 3:** Compute joint probabilities:\n",
    "$$P(D_1 = 0, D_2 = 0 \\mid H = 0) = 0.90 \\times 0.90 = 0.81$$\n",
    "$$P(D_1 = 0, D_2 = 1 \\mid H = 0) = 0.90 \\times 0.10 = 0.09$$\n",
    "$$P(D_1 = 1, D_2 = 0 \\mid H = 0) = 0.10 \\times 0.50 = 0.05$$\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 0) = 0.10 \\times 0.50 = 0.05$$\n",
    "\n",
    "**Joint Probability Table:**\n",
    "\n",
    "|  | $D_2 = 0$ | $D_2 = 1$ | Marginal |\n",
    "|--|-----------|-----------|----------|\n",
    "| $D_1 = 0$ | 0.81 | 0.09 | 0.90 |\n",
    "| $D_1 = 1$ | 0.05 | 0.05 | 0.10 |\n",
    "| **Marginal** | 0.86 | 0.14 | 1.00 |\n",
    "\n",
    "---\n",
    "\n",
    "### Part (b): $P(H = 1 \\mid D_1 = 1)$\n",
    "\n",
    "Using Bayes' Theorem:\n",
    "$$P(H = 1 \\mid D_1 = 1) = \\frac{P(D_1 = 1 \\mid H = 1) \\cdot P(H = 1)}{P(D_1 = 1)}$$\n",
    "\n",
    "**Compute $P(D_1 = 1)$:**\n",
    "$$P(D_1 = 1) = 0.99 \\times 0.0015 + 0.10 \\times 0.9985 = 0.001485 + 0.09985 = 0.101335$$\n",
    "\n",
    "**Apply Bayes:**\n",
    "$$P(H = 1 \\mid D_1 = 1) = \\frac{0.99 \\times 0.0015}{0.101335} = \\frac{0.001485}{0.101335} \\approx 0.0147 = 1.47\\%$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (c): $P(H = 1 \\mid D_1 = 1, D_2 = 1)$\n",
    "\n",
    "**For $H = 1$ (infected, tests independent):**\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 1) = 0.99 \\times 0.99 = 0.9801$$\n",
    "\n",
    "**For $H = 0$ (healthy, from table):**\n",
    "$$P(D_1 = 1, D_2 = 1 \\mid H = 0) = 0.05$$\n",
    "\n",
    "**Compute $P(D_1 = 1, D_2 = 1)$:**\n",
    "$$P(D_1 = 1, D_2 = 1) = 0.9801 \\times 0.0015 + 0.05 \\times 0.9985 = 0.00147 + 0.04993 = 0.0514$$\n",
    "\n",
    "**Apply Bayes:**\n",
    "$$P(H = 1 \\mid D_1 = 1, D_2 = 1) = \\frac{0.00147}{0.0514} \\approx 0.0286 = 2.86\\%$$\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison: Dependent vs Independent\n",
    "\n",
    "| Scenario | P(Disease \\| Evidence) |\n",
    "|----------|------------------------|\n",
    "| One positive test | 1.47% |\n",
    "| Both positive (DEPENDENT) | **2.86%** |\n",
    "| Both positive (INDEPENDENT) | ~12.9% |\n",
    "\n",
    "**Insight:** Conditional dependence significantly reduces the evidential value of the second test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXERCISE 7: MEDICAL TESTING WITH CONDITIONAL DEPENDENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Given data (from D2L 2.6)\n",
    "p_disease = 0.0015\n",
    "p_healthy = 1 - p_disease\n",
    "\n",
    "# Test characteristics\n",
    "false_positive = 0.10  # P(D=1 | H=0)\n",
    "false_negative = 0.01  # P(D=0 | H=1)\n",
    "sensitivity = 1 - false_negative  # P(D=1 | H=1) = 0.99\n",
    "\n",
    "# Conditional dependence for healthy patients\n",
    "p_d2_given_d1_h0 = 0.50  # P(D2=1 | D1=1, H=0)\n",
    "p_d2_given_not_d1_h0 = false_positive  # P(D2=1 | D1=0, H=0) = 0.10\n",
    "\n",
    "# Part (a): Joint probability table\n",
    "print(\"\\nPart (a): Joint Probability Table P(D1, D2 | H=0)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "p_d1_0_h0 = 1 - false_positive  # 0.90\n",
    "p_d1_1_h0 = false_positive       # 0.10\n",
    "\n",
    "p_00 = p_d1_0_h0 * (1 - p_d2_given_not_d1_h0)  # 0.90 * 0.90 = 0.81\n",
    "p_01 = p_d1_0_h0 * p_d2_given_not_d1_h0         # 0.90 * 0.10 = 0.09\n",
    "p_10 = p_d1_1_h0 * (1 - p_d2_given_d1_h0)       # 0.10 * 0.50 = 0.05\n",
    "p_11 = p_d1_1_h0 * p_d2_given_d1_h0             # 0.10 * 0.50 = 0.05\n",
    "\n",
    "print(f\"           D2=0      D2=1      Marginal\")\n",
    "print(f\"D1=0      {p_00:.2f}      {p_01:.2f}      {p_d1_0_h0:.2f}\")\n",
    "print(f\"D1=1      {p_10:.2f}      {p_11:.2f}      {p_d1_1_h0:.2f}\")\n",
    "print(f\"Marginal  {p_00+p_10:.2f}      {p_01+p_11:.2f}      1.00\")\n",
    "\n",
    "# Part (b): P(H=1 | D1=1)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part (b): P(H=1 | D1=1)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "p_d1_1 = sensitivity * p_disease + false_positive * p_healthy\n",
    "p_h1_given_d1 = (sensitivity * p_disease) / p_d1_1\n",
    "\n",
    "print(f\"P(D1=1) = {p_d1_1:.6f}\")\n",
    "print(f\"P(H=1 | D1=1) = {p_h1_given_d1:.4f} = {p_h1_given_d1*100:.2f}%\")\n",
    "\n",
    "# Part (c): P(H=1 | D1=1, D2=1)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part (c): P(H=1 | D1=1, D2=1)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "p_d1_d2_given_h1 = sensitivity * sensitivity  # 0.99 * 0.99 = 0.9801\n",
    "p_d1_d2_given_h0 = p_11  # 0.05\n",
    "\n",
    "p_d1_d2 = p_d1_d2_given_h1 * p_disease + p_d1_d2_given_h0 * p_healthy\n",
    "p_h1_given_both = (p_d1_d2_given_h1 * p_disease) / p_d1_d2\n",
    "\n",
    "print(f\"P(D1=1, D2=1 | H=1) = {p_d1_d2_given_h1:.4f}\")\n",
    "print(f\"P(D1=1, D2=1 | H=0) = {p_d1_d2_given_h0:.4f}\")\n",
    "print(f\"P(D1=1, D2=1) = {p_d1_d2:.6f}\")\n",
    "print(f\"\\nP(H=1 | D1=1, D2=1) = {p_h1_given_both:.4f} = {p_h1_given_both*100:.2f}%\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison: Dependent vs Independent\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "p_d1_d2_given_h0_indep = false_positive * false_positive  # 0.01\n",
    "p_d1_d2_indep = p_d1_d2_given_h1 * p_disease + p_d1_d2_given_h0_indep * p_healthy\n",
    "p_h1_given_both_indep = (p_d1_d2_given_h1 * p_disease) / p_d1_d2_indep\n",
    "\n",
    "print(f\"P(H=1 | both positive, DEPENDENT):   {p_h1_given_both:.4f} = {p_h1_given_both*100:.2f}%\")\n",
    "print(f\"P(H=1 | both positive, INDEPENDENT): {p_h1_given_both_indep:.4f} = {p_h1_given_both_indep*100:.2f}%\")\n",
    "print(f\"\\n→ Conditional dependence REDUCES evidential value!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 8: Portfolio Optimization\n",
    "\n",
    "> \"Portfolio optimization problem with stocks, returns, and covariance.\"\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "Consider a portfolio with three assets:\n",
    "- Asset A\n",
    "- Asset B\n",
    "- Asset C\n",
    "\n",
    "**Given Data:**\n",
    "\n",
    "**Expected Returns:**\n",
    "- $\\mu_A = 0.10$ (10% expected return)\n",
    "- $\\mu_B = 0.08$ (8% expected return)\n",
    "- $\\mu_C = 0.12$ (12% expected return)\n",
    "\n",
    "**Covariance Matrix:**\n",
    "$$\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n",
    "0.04 & 0.01 & 0.02 \\\\\n",
    "0.01 & 0.02 & 0.005 \\\\\n",
    "0.02 & 0.005 & 0.06\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Portfolio Weights:** $\\mathbf{w} = (w_A, w_B, w_C)^T$ where $w_A + w_B + w_C = 1$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (a): Expected Return for a Given Portfolio\n",
    "\n",
    "For a portfolio with weights $\\mathbf{w}$, the expected return is:\n",
    "$$E[R_p] = \\mathbf{w}^T \\boldsymbol{\\mu} = w_A \\mu_A + w_B \\mu_B + w_C \\mu_C$$\n",
    "\n",
    "**Example:** For equal weights $\\mathbf{w} = (1/3, 1/3, 1/3)^T$:\n",
    "$$E[R_p] = \\frac{1}{3}(0.10) + \\frac{1}{3}(0.08) + \\frac{1}{3}(0.12)$$\n",
    "$$= \\frac{1}{3}(0.30) = 0.10 = 10\\%$$\n",
    "\n",
    "**Answer:** Expected return is **10%** for equal-weighted portfolio.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (b): Maximum Return Portfolio\n",
    "\n",
    "To maximize return without constraints on risk:\n",
    "$$\\max_{\\mathbf{w}} \\quad \\mathbf{w}^T \\boldsymbol{\\mu}$$\n",
    "$$\\text{subject to} \\quad \\mathbf{w}^T \\mathbf{1} = 1$$\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Since Asset C has the highest expected return (12%), allocate all weight to Asset C:\n",
    "$$\\mathbf{w}^* = (0, 0, 1)^T$$\n",
    "\n",
    "**Maximum Expected Return:**\n",
    "$$E[R_p^*] = 0 \\cdot 0.10 + 0 \\cdot 0.08 + 1 \\cdot 0.12 = 0.12 = 12\\%$$\n",
    "\n",
    "**Answer:** Invest 100% in Asset C for maximum return of **12%**.\n",
    "\n",
    "**Caveat:** This ignores risk completely - Asset C may have high variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (c): Variance of the Portfolio\n",
    "\n",
    "For portfolio with weights $\\mathbf{w}$, variance is:\n",
    "$$\\text{Var}[R_p] = \\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w}$$\n",
    "\n",
    "Expanding:\n",
    "$$= \\sum_{i,j} w_i w_j \\Sigma_{ij}$$\n",
    "\n",
    "**Example 1: Equal-weighted portfolio** $\\mathbf{w} = (1/3, 1/3, 1/3)^T$:\n",
    "\n",
    "$$\\text{Var}[R_p] = \\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w}$$\n",
    "\n",
    "Computing step-by-step:\n",
    "$$\\boldsymbol{\\Sigma} \\mathbf{w} = \\begin{bmatrix}\n",
    "0.04 & 0.01 & 0.02 \\\\\n",
    "0.01 & 0.02 & 0.005 \\\\\n",
    "0.02 & 0.005 & 0.06\n",
    "\\end{bmatrix} \\begin{bmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{bmatrix} = \\begin{bmatrix} 0.0233 \\\\ 0.0117 \\\\ 0.0283 \\end{bmatrix}$$\n",
    "\n",
    "$$\\mathbf{w}^T (\\boldsymbol{\\Sigma} \\mathbf{w}) = \\frac{1}{3}(0.0233 + 0.0117 + 0.0283) = \\frac{0.0633}{3} = 0.0211$$\n",
    "\n",
    "**Answer:** Variance = **0.0211** (standard deviation = **14.5%**)\n",
    "\n",
    "**Example 2: Maximum return portfolio** $\\mathbf{w} = (0, 0, 1)^T$:\n",
    "\n",
    "$$\\text{Var}[R_p] = 1^2 \\cdot \\Sigma_{CC} = 0.06$$\n",
    "\n",
    "**Answer:** Variance = **0.06** (standard deviation = **24.5%**)\n",
    "\n",
    "The maximum return portfolio has much higher risk!\n",
    "\n",
    "---\n",
    "\n",
    "### Part (d): Constrained Optimization Problem\n",
    "\n",
    "**Objective:** Maximize return while limiting risk.\n",
    "\n",
    "**Formulation:**\n",
    "$$\\max_{\\mathbf{w}} \\quad \\mathbf{w}^T \\boldsymbol{\\mu}$$\n",
    "$$\\text{subject to} \\quad \\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w} \\leq \\sigma_{\\max}^2$$\n",
    "$$\\quad \\quad \\quad \\quad \\quad \\mathbf{w}^T \\mathbf{1} = 1$$\n",
    "$$\\quad \\quad \\quad \\quad \\quad w_i \\geq 0 \\quad \\forall i$$\n",
    "\n",
    "where $\\sigma_{\\max}^2$ is the maximum acceptable variance.\n",
    "\n",
    "**Alternative Formulation (Risk-Adjusted Return):**\n",
    "$$\\max_{\\mathbf{w}} \\quad \\mathbf{w}^T \\boldsymbol{\\mu} - \\frac{\\lambda}{2} \\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w}$$\n",
    "$$\\text{subject to} \\quad \\mathbf{w}^T \\mathbf{1} = 1$$\n",
    "\n",
    "where $\\lambda > 0$ is the risk aversion parameter.\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\lambda = 0$: No risk aversion (maximize return only)\n",
    "- $\\lambda \\to \\infty$: Extreme risk aversion (minimize variance only)\n",
    "- Intermediate $\\lambda$: Trade-off between return and risk\n",
    "\n",
    "**Solution Method:**\n",
    "\n",
    "This is a **quadratic programming** problem. The Lagrangian is:\n",
    "$$\\mathcal{L}(\\mathbf{w}, \\nu) = \\mathbf{w}^T \\boldsymbol{\\mu} - \\frac{\\lambda}{2} \\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w} - \\nu(\\mathbf{w}^T \\mathbf{1} - 1)$$\n",
    "\n",
    "Taking derivative with respect to $\\mathbf{w}$ and setting to zero:\n",
    "$$\\boldsymbol{\\mu} - \\lambda \\boldsymbol{\\Sigma} \\mathbf{w} - \\nu \\mathbf{1} = 0$$\n",
    "\n",
    "Solving:\n",
    "$$\\mathbf{w}^* = \\frac{1}{\\lambda} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu} - \\nu \\mathbf{1})$$\n",
    "\n",
    "where $\\nu$ is chosen to satisfy $\\mathbf{w}^T \\mathbf{1} = 1$.\n",
    "\n",
    "**Efficient Frontier:**\n",
    "\n",
    "The set of portfolios that maximize return for each level of risk forms the **efficient frontier**. Rational investors choose portfolios on this frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Portfolio Optimization\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Given data\n",
    "mu = np.array([0.10, 0.08, 0.12])  # Expected returns\n",
    "Sigma = np.array([\n",
    "    [0.04, 0.01, 0.02],\n",
    "    [0.01, 0.02, 0.005],\n",
    "    [0.02, 0.005, 0.06]\n",
    "])\n",
    "\n",
    "asset_names = ['Asset A', 'Asset B', 'Asset C']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXERCISE 8: PORTFOLIO OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nGiven Data:\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Expected Returns: {mu}\")\n",
    "print(f\"\\nCovariance Matrix:\")\n",
    "print(Sigma)\n",
    "\n",
    "# Part (a): Equal-weighted portfolio\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Part (a): Equal-Weighted Portfolio\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "w_equal = np.array([1/3, 1/3, 1/3])\n",
    "return_equal = w_equal @ mu\n",
    "variance_equal = w_equal @ Sigma @ w_equal\n",
    "std_equal = np.sqrt(variance_equal)\n",
    "\n",
    "print(f\"Weights: {w_equal}\")\n",
    "print(f\"Expected Return: {return_equal:.4f} = {return_equal*100:.2f}%\")\n",
    "print(f\"Variance: {variance_equal:.4f}\")\n",
    "print(f\"Standard Deviation: {std_equal:.4f} = {std_equal*100:.2f}%\")\n",
    "\n",
    "# Part (b): Maximum return portfolio\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Part (b): Maximum Return Portfolio (No Risk Constraint)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "max_return_idx = np.argmax(mu)\n",
    "w_max_return = np.zeros(3)\n",
    "w_max_return[max_return_idx] = 1.0\n",
    "\n",
    "return_max = w_max_return @ mu\n",
    "variance_max = w_max_return @ Sigma @ w_max_return\n",
    "std_max = np.sqrt(variance_max)\n",
    "\n",
    "print(f\"Optimal Allocation: 100% to {asset_names[max_return_idx]}\")\n",
    "print(f\"Weights: {w_max_return}\")\n",
    "print(f\"Expected Return: {return_max:.4f} = {return_max*100:.2f}%\")\n",
    "print(f\"Variance: {variance_max:.4f}\")\n",
    "print(f\"Standard Deviation: {std_max:.4f} = {std_max*100:.2f}%\")\n",
    "\n",
    "# Part (c): Variance for different portfolios\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Part (c): Variance Analysis for Different Portfolios\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "portfolios = {\n",
    "    'Equal-weighted': w_equal,\n",
    "    'Max Return': w_max_return,\n",
    "    'Conservative (50% B, 50% A)': np.array([0.5, 0.5, 0.0]),\n",
    "    'Aggressive (50% C, 30% A, 20% B)': np.array([0.3, 0.2, 0.5])\n",
    "}\n",
    "\n",
    "print(f\"{'Portfolio':<35} {'Return':<10} {'Std Dev':<10}\")\n",
    "print(\"-\"*70)\n",
    "for name, weights in portfolios.items():\n",
    "    ret = weights @ mu\n",
    "    std = np.sqrt(weights @ Sigma @ weights)\n",
    "    print(f\"{name:<35} {ret:8.2%}   {std:8.2%}\")\n",
    "\n",
    "# Part (d): Optimization with risk constraint\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Part (d): Constrained Optimization - Efficient Frontier\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def portfolio_return(w):\n",
    "    return -(w @ mu)  # Negative because we minimize\n",
    "\n",
    "def portfolio_variance(w):\n",
    "    return w @ Sigma @ w\n",
    "\n",
    "# Constraint: weights sum to 1\n",
    "constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "\n",
    "# Bounds: weights between 0 and 1 (no short selling)\n",
    "bounds = tuple((0, 1) for _ in range(3))\n",
    "\n",
    "# Compute efficient frontier\n",
    "target_returns = np.linspace(mu.min(), mu.max(), 50)\n",
    "efficient_frontier_std = []\n",
    "efficient_frontier_weights = []\n",
    "\n",
    "for target_return in target_returns:\n",
    "    # Minimize variance subject to target return\n",
    "    constraints_with_return = constraints + [\n",
    "        {'type': 'eq', 'fun': lambda w, tr=target_return: w @ mu - tr}\n",
    "    ]\n",
    "    \n",
    "    result = minimize(portfolio_variance, x0=w_equal, method='SLSQP',\n",
    "                     bounds=bounds, constraints=constraints_with_return)\n",
    "    \n",
    "    if result.success:\n",
    "        efficient_frontier_std.append(np.sqrt(result.fun))\n",
    "        efficient_frontier_weights.append(result.x)\n",
    "    else:\n",
    "        efficient_frontier_std.append(np.nan)\n",
    "        efficient_frontier_weights.append(None)\n",
    "\n",
    "efficient_frontier_std = np.array(efficient_frontier_std)\n",
    "\n",
    "# Optimize for different risk aversion parameters\n",
    "lambdas = [0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "optimal_portfolios = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    # Maximize risk-adjusted return: return - (lambda/2)*variance\n",
    "    def objective(w):\n",
    "        return -(w @ mu - (lam/2) * (w @ Sigma @ w))\n",
    "    \n",
    "    result = minimize(objective, x0=w_equal, method='SLSQP',\n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    if result.success:\n",
    "        w_opt = result.x\n",
    "        ret_opt = w_opt @ mu\n",
    "        std_opt = np.sqrt(w_opt @ Sigma @ w_opt)\n",
    "        optimal_portfolios.append((lam, w_opt, ret_opt, std_opt))\n",
    "\n",
    "print(\"\\nOptimal Portfolios for Different Risk Aversion Levels:\")\n",
    "print(f\"{'Lambda':<8} {'Return':<10} {'Std Dev':<10} {'Weights (A, B, C)'}\")\n",
    "print(\"-\"*70)\n",
    "for lam, w, ret, std in optimal_portfolios:\n",
    "    weights_str = f\"({w[0]:.3f}, {w[1]:.3f}, {w[2]:.3f})\"\n",
    "    print(f\"{lam:<8.1f} {ret:8.2%}   {std:8.2%}   {weights_str}\")\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Efficient Frontier\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "\n",
    "# Plot efficient frontier\n",
    "valid_mask = ~np.isnan(efficient_frontier_std)\n",
    "ax1.plot(efficient_frontier_std[valid_mask], target_returns[valid_mask], \n",
    "         'b-', linewidth=2, label='Efficient Frontier')\n",
    "\n",
    "# Plot individual assets\n",
    "for i, name in enumerate(asset_names):\n",
    "    w_single = np.zeros(3)\n",
    "    w_single[i] = 1.0\n",
    "    ret_single = mu[i]\n",
    "    std_single = np.sqrt(Sigma[i, i])\n",
    "    ax1.plot(std_single, ret_single, 'ro', markersize=10)\n",
    "    ax1.annotate(name, (std_single, ret_single), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# Plot example portfolios\n",
    "for lam, w, ret, std in optimal_portfolios:\n",
    "    ax1.plot(std, ret, 'g^', markersize=8)\n",
    "\n",
    "ax1.plot(std_equal, return_equal, 'ks', markersize=10, label='Equal-weighted')\n",
    "\n",
    "ax1.set_xlabel('Standard Deviation (Risk)')\n",
    "ax1.set_ylabel('Expected Return')\n",
    "ax1.set_title('Efficient Frontier')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Portfolio weights vs lambda\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "\n",
    "lambdas_plot = [p[0] for p in optimal_portfolios]\n",
    "weights_A = [p[1][0] for p in optimal_portfolios]\n",
    "weights_B = [p[1][1] for p in optimal_portfolios]\n",
    "weights_C = [p[1][2] for p in optimal_portfolios]\n",
    "\n",
    "ax2.plot(lambdas_plot, weights_A, 'o-', label='Asset A', linewidth=2)\n",
    "ax2.plot(lambdas_plot, weights_B, 's-', label='Asset B', linewidth=2)\n",
    "ax2.plot(lambdas_plot, weights_C, '^-', label='Asset C', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Risk Aversion (λ)')\n",
    "ax2.set_ylabel('Portfolio Weight')\n",
    "ax2.set_title('Optimal Weights vs Risk Aversion')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Return vs Risk tradeoff\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "\n",
    "returns_plot = [p[2] for p in optimal_portfolios]\n",
    "stds_plot = [p[3] for p in optimal_portfolios]\n",
    "\n",
    "ax3.plot(lambdas_plot, returns_plot, 'b-o', label='Expected Return', linewidth=2)\n",
    "ax3.set_xlabel('Risk Aversion (λ)')\n",
    "ax3.set_ylabel('Expected Return', color='b')\n",
    "ax3.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax3_twin = ax3.twinx()\n",
    "ax3_twin.plot(lambdas_plot, stds_plot, 'r-s', label='Standard Deviation', linewidth=2)\n",
    "ax3_twin.set_ylabel('Standard Deviation', color='r')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "ax3.set_title('Return-Risk Tradeoff')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Key Insights:\")\n",
    "print(\"- Higher risk aversion (λ) → Lower risk, lower return\")\n",
    "print(\"- Efficient frontier shows optimal return for each risk level\")\n",
    "print(\"- Diversification reduces risk compared to single-asset portfolios\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
