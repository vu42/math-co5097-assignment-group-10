{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='urllib3 v2 only supports OpenSSL')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a fundamental statistical method [1] used to estimate the parameters of a statistical model. Given observed data, the core idea behind MLE is to find the parameter values that maximize the likelihood, meaning the probability of the observed data under those parameters.\n",
    "\n",
    "**Why do we need Maximum Likelihood?**\n",
    "\n",
    "In real-world applications, we often observe data generated by some unknown process. We want to model this process using a probabilistic distribution, but we don't know the parameters of that distribution. MLE provides a principled, systematic way [2] to estimate these parameters directly from the observed data.\n",
    "\n",
    "### Applications in Computer Science\n",
    "\n",
    "MLE is extensively used across various Computer Science domains:\n",
    "\n",
    "**1. Machine Learning:**\n",
    "- Training neural networks (weight optimization)\n",
    "- Logistic regression for binary classification\n",
    "- Gaussian Mixture Models for clustering\n",
    "- Hidden Markov Models for sequence modeling\n",
    "\n",
    "**2. Natural Language Processing:**\n",
    "- Language model parameter estimation\n",
    "- N-gram probability estimation\n",
    "- Word embedding training\n",
    "- Machine translation models\n",
    "\n",
    "**3. Computer Vision:**\n",
    "- Image noise reduction (Gaussian noise modeling)\n",
    "- Object detection and recognition\n",
    "- Image segmentation\n",
    "- Feature extraction\n",
    "\n",
    "**4. Data Science and Statistics:**\n",
    "- A/B testing and hypothesis testing\n",
    "- Time series forecasting\n",
    "- Anomaly detection\n",
    "- Recommendation systems\n",
    "\n",
    "**5. Bioinformatics:**\n",
    "- Gene expression analysis\n",
    "- Evolutionary parameter estimation\n",
    "- Protein structure prediction\n",
    "\n",
    "### Computer Science Problems Solved by MLE\n",
    "\n",
    "1. **Parameter Estimation:** Finding optimal model parameters from data\n",
    "2. **Model Selection:** Comparing different probabilistic models\n",
    "3. **Prediction:** Making informed predictions about unseen data\n",
    "4. **Classification:** Distinguishing between different classes (spam vs. not spam)\n",
    "5. **Regression:** Predicting continuous values based on input features\n",
    "6. **Density Estimation:** Estimating probability distributions from data\n",
    "7. **Missing Data Imputation:** Filling in missing values using probabilistic models\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The fundamental principle of MLE is to find parameter $\\theta$ that maximizes:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\max_{\\theta} P(X|\\theta)\n",
    "$$\n",
    "\n",
    "where $X$ represents our observed data, and $P(X|\\theta)$ is the likelihood function.\n",
    "\n",
    "This approach transforms the problem of parameter estimation into an optimization problem, which can be solved using calculus (setting derivatives to zero) or numerical optimization methods (gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Foundations & Implementations\n",
    "\n",
    "This section presents the core concepts of Maximum Likelihood Estimation, organized from fundamental definitions to specific distribution estimators. Each formula includes:\n",
    "- **Detailed Explanation:** Intuitive understanding of the concept\n",
    "- **Detailed Derivation:** Mathematical proof and reasoning\n",
    "- **Concrete Example:** Real-world application with numerical calculations\n",
    "- **Python Implementation:** Verification through code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Likelihood Function\n",
    "\n",
    "The likelihood function for a parameter $\\theta$, given observed data $X$, is defined as [1]:\n",
    "\n",
    "$$\n",
    "L(\\theta) = P(X|\\theta)\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "This formula expresses the probability of observing the given data $X$, assuming the parameter $\\theta$ is known. The goal of Maximum Likelihood Estimation (MLE) is to find the parameter value that maximizes this likelihood function.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "To better understand the likelihood function, we can start from **Bayes' Rule** [1]:\n",
    "\n",
    "$$\n",
    "P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}\n",
    "$$\n",
    "\n",
    "This equation gives the **posterior probability** of the parameter $\\theta$ after observing data $X$, in terms of:\n",
    "\n",
    "- **Likelihood**: $P(X|\\theta)$ — how probable the data is given the parameter.\n",
    "- **Prior**: $P(\\theta)$ — our prior belief about the parameter before seeing the data.\n",
    "- **Marginal likelihood**: $P(X)$ — the total probability of the data (a normalizing constant).\n",
    "\n",
    "Now, suppose we want to **find the value of $\\theta$ that is most probable after observing $X$**. That means:\n",
    "\n",
    "$$\n",
    "\\arg\\max_{\\theta} P(\\theta|X)\n",
    "$$\n",
    "\n",
    "By Bayes' rule, this is equivalent to:\n",
    "\n",
    "$$\n",
    "\\arg\\max_{\\theta} \\frac{P(X|\\theta)P(\\theta)}{P(X)}\n",
    "$$\n",
    "\n",
    "Since $P(X)$ **does not depend on $\\theta$**, it can be treated as a constant during optimization:\n",
    "\n",
    "$$\n",
    "\\arg\\max_{\\theta} P(\\theta|X) = \\arg\\max_{\\theta} P(X|\\theta)P(\\theta)\n",
    "$$\n",
    "\n",
    "At this point, if we assume that we have **no prior preference** about $\\theta$ (i.e. $P(\\theta)$ is uniform — a so-called **uninformative prior**), then $P(\\theta)$ is also constant, and we can drop it too:\n",
    "\n",
    "$$\n",
    "\\arg\\max_{\\theta} P(X|\\theta)\n",
    "$$\n",
    "\n",
    "This gives us the **Maximum Likelihood Estimation (MLE)** objective:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} P(X|\\theta)\n",
    "$$\n",
    "\n",
    "Therefore, we define the **likelihood function** as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = P(X|\\theta)\n",
    "$$\n",
    "\n",
    "This forms the foundation of Maximum Likelihood Estimation [1][2].\n",
    "\n",
    "### Concrete Example and Calculation:\n",
    "\n",
    "Suppose we roll a fair 6-sided die twice and observe the outcomes: 2 and 4.\n",
    "\n",
    "Let:\n",
    "\n",
    "- $x_1 = 2$ be the result of the first roll\n",
    "- $x_2 = 4$ be the result of the second roll\n",
    "- $X = (x_1, x_2) = (2, 4) $ be the data vector containing both outcomes\n",
    "- $\\theta = (\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5, \\theta_6)$ be the probability vector, where each $\\theta_i$ represents the probability of rolling side $i$\n",
    "\n",
    "Assume the dice is fair, each side has equal probability:\n",
    "$$\n",
    "(\\theta_i = \\frac{1}{6}) \\text{ for all } i = 1,\\dots,6\n",
    "$$\n",
    "\n",
    "Then, assuming independence between rolls, the likelihood of observing $X = (2, 4)$\n",
    "\n",
    "* detailed calculation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L\\left(\\theta\\right) &= P(X|\\theta) \\\\\n",
    "&= P(x_1, x_2|\\theta) \\\\\n",
    "&= P(x_1|\\theta) \\times P(x_2|\\theta) \\\\\n",
    "&= \\frac{1}{6} \\times \\frac{1}{6} \\\\\n",
    "&= \\frac{1}{36}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Python Verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Observed data: rolled 2, then 4 on a fair die\n",
    "observed_outcomes = torch.tensor([1, 3])  # 0-indexed\n",
    "\n",
    "# Fair die probabilities\n",
    "probs = torch.ones(6) / 6.0\n",
    "\n",
    "categorical_dist = torch.distributions.Categorical(probs=probs)\n",
    "log_likelihood = categorical_dist.log_prob(observed_outcomes).sum()\n",
    "likelihood = torch.exp(log_likelihood)\n",
    "\n",
    "print(f\"Likelihood of observing (2,4) with fair die (theta=1/6): {likelihood.item():.6f}\")\n",
    "print(f\"Expected value (manual): {1/36:.6f}\")\n",
    "print(f\"Log-likelihood: {log_likelihood.item():.6f}\")\n",
    "\n",
    "# Compute likelihood for different theta values\n",
    "theta_values = torch.linspace(0.001, 0.3, 300)\n",
    "likelihoods = []\n",
    "for theta in theta_values:\n",
    "    probs_theta = torch.ones(6) * theta / (6 * theta)\n",
    "    dist = torch.distributions.Categorical(probs=probs_theta)\n",
    "    log_like = dist.log_prob(observed_outcomes).sum()\n",
    "    likelihoods.append(torch.exp(log_like).item())\n",
    "likelihoods = torch.tensor(likelihoods)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(theta_values.numpy(), likelihoods.numpy(), 'b-', linewidth=2)\n",
    "plt.axvline(1/6, color='r', linestyle='--', linewidth=2, label=f'Fair die θ=1/6')\n",
    "plt.xlabel('θ (Probability)', fontsize=12)\n",
    "plt.ylabel('Likelihood L(θ)', fontsize=12)\n",
    "plt.title('Likelihood Function for Observing Two Outcomes (PyTorch)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "observed_outcomes = torch.tensor([1, 3])\n",
    "theta_range = torch.linspace(0.001, 0.3, 300)\n",
    "\n",
    "likelihoods = []\n",
    "for theta in theta_range:\n",
    "    probs = torch.ones(6) * theta\n",
    "    probs = probs / probs.sum()\n",
    "    \n",
    "    dist = torch.distributions.Categorical(probs=probs)\n",
    "    log_like = dist.log_prob(observed_outcomes).sum()\n",
    "    likelihoods.append(torch.exp(log_like).item())\n",
    "\n",
    "likelihoods = torch.tensor(likelihoods)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(theta_range.numpy(), likelihoods.numpy(), 'g-', linewidth=2, label='PyTorch Categorical')\n",
    "plt.axvline(1/6, color='r', linestyle='--', linewidth=2, label='Fair die θ=1/6')\n",
    "plt.xlabel('θ (Probability)', fontsize=12)\n",
    "plt.ylabel('Likelihood L(θ)', fontsize=12)\n",
    "plt.title('Likelihood Function (PyTorch Built-in Distribution)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Verify at theta=1/6\n",
    "probs_fair = torch.ones(6) / 6.0\n",
    "dist_fair = torch.distributions.Categorical(probs=probs_fair)\n",
    "log_like_fair = dist_fair.log_prob(observed_outcomes).sum()\n",
    "likelihood_fair = torch.exp(log_like_fair)\n",
    "\n",
    "print(f\"Likelihood at theta=1/6: {likelihood_fair.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "observed_outcomes = [1, 3]\n",
    "theta_range = np.linspace(0.001, 0.3, 300)\n",
    "\n",
    "n_observations = len(observed_outcomes)\n",
    "likelihood_np = np.power(theta_range, n_observations)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(theta_range, likelihood_np, 'c-', linewidth=2, label='NumPy (ndarray ops)')\n",
    "plt.axvline(1/6, color='r', linestyle='--', linewidth=2, label='Fair die θ=1/6')\n",
    "plt.xlabel('θ (Probability)', fontsize=12)\n",
    "plt.ylabel('Likelihood L(θ)', fontsize=12)\n",
    "plt.title('Likelihood Function (NumPy ndarray operations)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Likelihood at theta=1/6: {float(np.power(1/6, 2)):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "observed_outcomes = tf.constant([1, 3])\n",
    "probs = tf.ones(6) / 6.0\n",
    "\n",
    "categorical_dist = tfp.distributions.Categorical(probs=probs)\n",
    "log_likelihood = tf.reduce_sum(categorical_dist.log_prob(observed_outcomes))\n",
    "likelihood = tf.exp(log_likelihood)\n",
    "\n",
    "print(f\"Likelihood of observing (2,4) with fair die (theta=1/6): {likelihood.numpy():.6f}\")\n",
    "print(f\"Expected value (manual): {1/36:.6f}\")\n",
    "print(f\"Log-likelihood: {log_likelihood.numpy():.6f}\")\n",
    "\n",
    "theta_tf = tf.linspace(0.001, 0.3, 300)\n",
    "\n",
    "likelihoods_tf = []\n",
    "for theta in theta_tf:\n",
    "    probs_theta = tf.ones(6) * theta / (6 * theta)\n",
    "    dist = tfp.distributions.Categorical(probs=probs_theta)\n",
    "    log_like = tf.reduce_sum(dist.log_prob(observed_outcomes))\n",
    "    likelihoods_tf.append(tf.exp(log_like).numpy())\n",
    "likelihoods_tf = tf.constant(likelihoods_tf)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(theta_tf.numpy(), likelihoods_tf.numpy(), 'm-', linewidth=2, label='TensorFlow')\n",
    "plt.axvline(1/6, color='r', linestyle='--', linewidth=2, label='Fair die θ=1/6')\n",
    "plt.xlabel('θ (Probability)', fontsize=12)\n",
    "plt.ylabel('Likelihood L(θ)', fontsize=12)\n",
    "plt.title('Likelihood Function (TensorFlow Probability)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Likelihood for Independent Observations\n",
    "\n",
    "When data consists of independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$, the likelihood function factorizes as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = P(X|\\theta) = \\prod_{i=1}^{n} P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "This formula applies when each observation is independent of the others. In practice, independence significantly simplifies calculations, converting complex joint probabilities into simple products of individual probabilities.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "Given the assumption of independence, by definition of joint probability:\n",
    "\n",
    "* For independent events:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\ldots, x_n|\\theta) = P(x_1|\\theta)P(x_2|\\theta) \\ldots P(x_n|\\theta)\n",
    "$$\n",
    "\n",
    "Thus, the likelihood function for independent observations naturally becomes:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "### Concrete Example and Calculation:\n",
    "\n",
    "Suppose we toss a coin 3 times with outcomes (Heads, Tails, Heads), and let $\\theta = 0.5$ represent the probability of Heads:\n",
    "\n",
    "* detailed calculation:\n",
    "\n",
    "$$\n",
    "L(0.5) = P(H|\\theta = 0.5) \\times P(T|\\theta = 0.5) \\times P(H|\\theta = 0.5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.5) \\times (0.5) \\times (0.5) = 0.125\n",
    "$$\n",
    "\n",
    "### Python Verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "outcomes = torch.tensor([1.0, 0.0, 1.0])\n",
    "theta = 0.5\n",
    "\n",
    "bernoulli_dist = torch.distributions.Bernoulli(probs=theta)\n",
    "log_likelihood = bernoulli_dist.log_prob(outcomes).sum()\n",
    "likelihood = torch.exp(log_likelihood)\n",
    "\n",
    "print(\"Likelihood:\", likelihood.item())\n",
    "print(\"Expected:\", 0.125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Log-Likelihood Function\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "The log-likelihood function is defined as:\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The log-likelihood converts the product of probabilities into a sum, simplifying computation.\n",
    "* It also helps avoid numerical underflow that occurs when multiplying very small probabilities.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1.** Starting from likelihood function for independent observations:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "**2.** Taking natural log of both sides to convert product to summation:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\log\\left(\\prod_{i=1}^{n} P(x_i|\\theta)\\right)\n",
    "$$\n",
    "\n",
    "**3.** Applying the logarithm property:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i=1}^{n} \\log P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "Thus, we have clearly derived the log-likelihood function.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Consider flipping a fair coin 3 times, with outcomes: Heads (H), Tails (T), Heads (H). Assume $\\theta = 0.5$:\n",
    "\n",
    "* Compute likelihood first:\n",
    "\n",
    "$$\n",
    "L(0.5) = 0.5 \\times 0.5 \\times 0.5 = 0.125\n",
    "$$\n",
    "\n",
    "* Compute log-likelihood:\n",
    "\n",
    "$$\n",
    "l(0.5) = \\log(0.5) + \\log(0.5) + \\log(0.5) = 3\\log(0.5)\n",
    "$$\n",
    "\n",
    "* Using numerical approximation:\n",
    "\n",
    "$$\n",
    "3\\log(0.5) = 3 \\times (-0.6931) = -2.0794\n",
    "$$\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "log_likelihood = 3 * np.log(0.5)\n",
    "print(\"Log-Likelihood:\", log_likelihood)\n",
    "# Output: Log-Likelihood: -2.0794\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Negative Log-Likelihood (NLL)\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "$$\n",
    "NLL(\\theta) = -\\log L(\\theta) = -\\sum_{i=1}^{n} \\log P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "The Negative Log-Likelihood is often used in practice because:\n",
    "\n",
    "* Taking logs converts multiplication into summation (easier computation).\n",
    "* Taking the negative allows minimization, which is more convenient numerically in optimization algorithms.\n",
    "\n",
    "MLE thus becomes the minimization problem:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\min_{\\theta} NLL(\\theta)\n",
    "$$\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "Start from the likelihood function for independent observations:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "Take the logarithm:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\log \\prod_{i=1}^{n} P(x_i|\\theta) = \\sum_{i=1}^{n} \\log P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "Multiply by -1:\n",
    "\n",
    "$$\n",
    "NLL(\\theta) = -\\log L(\\theta) = -\\sum_{i=1}^{n} \\log P(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "### Concrete Example and Detailed Calculation:\n",
    "\n",
    "Consider tossing a biased coin 4 times, observing the results: H, H, T, H.\n",
    "\n",
    "* Let's assume initially $\\theta = 0.6$, probability of getting heads.\n",
    "\n",
    "Likelihood:\n",
    "\n",
    "$$\n",
    "L(0.6) = 0.6 \\times 0.6 \\times 0.4 \\times 0.6 = 0.0864\n",
    "$$\n",
    "\n",
    "Log-likelihood:\n",
    "\n",
    "$$\n",
    "\\log L(0.6) = \\log(0.6) + \\log(0.6) + \\log(0.4) + \\log(0.6)\n",
    "$$\n",
    "\n",
    "Compute detailed (approx):\n",
    "\n",
    "* $\\log(0.6) \\approx -0.5108, \\log(0.4) \\approx -0.9163$\n",
    "* Thus:\n",
    "\n",
    "$$\n",
    "\\log L(0.6) = (-0.5108) + (-0.5108) + (-0.9163) + (-0.5108) = -2.4487\n",
    "$$\n",
    "\n",
    "Negative log-likelihood (NLL):\n",
    "\n",
    "$$\n",
    "NLL(0.6) = -(-2.4487) = 2.4487\n",
    "$$\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.6\n",
    "log_likelihood = 3*np.log(theta) + np.log(1-theta)\n",
    "nll = -log_likelihood\n",
    "\n",
    "print(\"Likelihood:\", theta**3 * (1-theta))\n",
    "print(\"Log-Likelihood:\", log_likelihood)\n",
    "print(\"Negative Log-Likelihood:\", nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "n_H, n_T = 7, 3\n",
    "n_total = n_H + n_T\n",
    "theta_range = np.linspace(0.01, 0.99, 200)\n",
    "\n",
    "# NumPy (scipy)\n",
    "likelihood_np = [binom.pmf(n_H, n_total, theta) for theta in theta_range]\n",
    "\n",
    "# PyTorch\n",
    "likelihood_torch = []\n",
    "for theta in theta_range:\n",
    "    binomial_dist = torch.distributions.Binomial(total_count=n_total, probs=theta)\n",
    "    prob = torch.exp(binomial_dist.log_prob(torch.tensor(float(n_H))))\n",
    "    likelihood_torch.append(prob.item())\n",
    "\n",
    "# TensorFlow\n",
    "likelihood_tf = []\n",
    "for theta in theta_range:\n",
    "    binomial_dist = tfp.distributions.Binomial(total_count=float(n_total), probs=theta)\n",
    "    prob = tf.exp(binomial_dist.log_prob(float(n_H)))\n",
    "    likelihood_tf.append(prob.numpy())\n",
    "\n",
    "# NumPy (manual calculation)\n",
    "likelihood_np_manual = np.power(theta_range, n_H) * np.power(1 - theta_range, n_T)\n",
    "\n",
    "theta_mle = n_H / n_total\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0,0].plot(theta_range, likelihood_np, 'b-', linewidth=2)\n",
    "axes[0,0].axvline(theta_mle, color='r', linestyle='--', linewidth=2, label=f'MLE θ={theta_mle:.2f}')\n",
    "axes[0,0].set_title('NumPy + scipy.stats.binom.pmf()', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_xlabel('θ (Probability of Heads)', fontsize=10)\n",
    "axes[0,0].set_ylabel('Likelihood L(θ)', fontsize=10)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].plot(theta_range, likelihood_torch, 'g-', linewidth=2)\n",
    "axes[0,1].axvline(theta_mle, color='r', linestyle='--', linewidth=2, label=f'MLE θ={theta_mle:.2f}')\n",
    "axes[0,1].set_title('PyTorch (torch.distributions.Binomial)', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_xlabel('θ (Probability of Heads)', fontsize=10)\n",
    "axes[0,1].set_ylabel('Likelihood L(θ)', fontsize=10)\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,0].plot(theta_range, likelihood_tf, 'm-', linewidth=2)\n",
    "axes[1,0].axvline(theta_mle, color='r', linestyle='--', linewidth=2, label=f'MLE θ={theta_mle:.2f}')\n",
    "axes[1,0].set_title('TensorFlow (tfp.distributions.Binomial)', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_xlabel('θ (Probability of Heads)', fontsize=10)\n",
    "axes[1,0].set_ylabel('Likelihood L(θ)', fontsize=10)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,1].plot(theta_range, likelihood_np_manual, 'c-', linewidth=2)\n",
    "axes[1,1].axvline(theta_mle, color='r', linestyle='--', linewidth=2, label=f'MLE θ={theta_mle:.2f}')\n",
    "axes[1,1].set_title('NumPy (manual calculation)', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_xlabel('θ (Probability of Heads)', fontsize=10)\n",
    "axes[1,1].set_ylabel('Likelihood L(θ)', fontsize=10)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Binomial Likelihood: {n_H} Heads, {n_T} Tails (Using Built-in Distributions)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Binomial Distribution MLE Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Observed: {n_H} Heads, {n_T} Tails\")\n",
    "print(f\"MLE theta = {n_H}/{n_H+n_T} = {theta_mle:.4f}\")\n",
    "print(f\"\\nUsing Built-in Distributions:\")\n",
    "print(f\"  - NumPy:      scipy.stats.binom.pmf()\")\n",
    "print(f\"  - PyTorch:    torch.distributions.Binomial\")\n",
    "print(f\"  - TensorFlow: tfp.distributions.Binomial\")  \n",
    "print(f\"  - NumPy:      np.power() operations\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Binomial Likelihood\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Likelihood function for binomially distributed data [1][2] (success probability $\\theta$, number of successes $n_H$, failures $n_T$):\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\theta^{n_H}(1-\\theta)^{n_T}\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* This formula expresses the likelihood of observing exactly $n_H$ successes in $n_H + n_T$ independent Bernoulli trials.\n",
    "* Commonly used when data represents counts of successes/failures.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "The binomial probability formula (excluding the combinatorial coefficient, as it doesn't affect maximization w.r.t. $\\theta$) is:\n",
    "\n",
    "$$\n",
    "P(X|\\theta) \\propto \\theta^{n_H}(1-\\theta)^{n_T}\n",
    "$$\n",
    "\n",
    "Thus, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\theta^{n_H}(1-\\theta)^{n_T}\n",
    "$$\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose a biased coin is flipped 10 times, yielding exactly 7 heads and 3 tails. Assume $\\theta = 0.7$:\n",
    "\n",
    "* Compute likelihood:\n",
    "\n",
    "$$\n",
    "L(0.7) = (0.7)^7 \\times (0.3)^3\n",
    "$$\n",
    "\n",
    "* detailed numeric calculation:\n",
    "\n",
    "$$\n",
    "(0.7)^7 \\approx 0.0823543, \\quad (0.3)^3 = 0.027\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "L(0.7) \\approx 0.0823543 \\times 0.027 = 0.00222357\n",
    "$$\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu_true = 2.0\n",
    "sigma_sq_true = 1.0\n",
    "sigma_true = np.sqrt(sigma_sq_true)\n",
    "x_data = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# NumPy\n",
    "likelihood_np = np.prod([norm.pdf(x, mu_true, sigma_true) for x in x_data])\n",
    "\n",
    "# PyTorch\n",
    "x_torch = torch.tensor(x_data)\n",
    "normal_dist_torch = torch.distributions.Normal(mu_true, sigma_true)\n",
    "log_likelihood_torch = normal_dist_torch.log_prob(x_torch).sum()\n",
    "likelihood_torch = torch.exp(log_likelihood_torch)\n",
    "\n",
    "# TensorFlow\n",
    "x_tf = tf.constant(x_data)\n",
    "normal_dist_tf = tfp.distributions.Normal(mu_true, sigma_true)\n",
    "log_likelihood_tf = tf.reduce_sum(normal_dist_tf.log_prob(x_tf))\n",
    "likelihood_tf = tf.exp(log_likelihood_tf)\n",
    "\n",
    "# NumPy (manual calculation)\n",
    "pdf_np = (1 / np.sqrt(2 * np.pi * sigma_sq_true)) * np.exp(-(x_data - mu_true)**2 / (2 * sigma_sq_true))\n",
    "likelihood_np_manual = np.prod(pdf_np)\n",
    "\n",
    "mu_range = np.linspace(-1, 5, 200)\n",
    "likelihoods_np = [np.prod([norm.pdf(x, mu, sigma_true) for x in x_data]) for mu in mu_range]\n",
    "\n",
    "likelihoods_torch = []\n",
    "for mu in mu_range:\n",
    "    dist = torch.distributions.Normal(mu, sigma_true)\n",
    "    log_like = dist.log_prob(x_torch).sum()\n",
    "    likelihoods_torch.append(torch.exp(log_like).item())\n",
    "\n",
    "likelihoods_tf = []\n",
    "for mu in mu_range:\n",
    "    dist = tfp.distributions.Normal(mu, sigma_true)\n",
    "    log_like = tf.reduce_sum(dist.log_prob(x_tf))\n",
    "    likelihoods_tf.append(tf.exp(log_like).numpy())\n",
    "\n",
    "likelihoods_np_manual = []\n",
    "for mu in mu_range:\n",
    "    pdf = (1 / np.sqrt(2 * np.pi * sigma_sq_true)) * np.exp(-(x_data - mu)**2 / (2 * sigma_sq_true))\n",
    "    likelihoods_np_manual.append(float(np.prod(pdf)))\n",
    "\n",
    "mu_mle = np.mean(x_data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0,0].plot(mu_range, likelihoods_np, 'b-', linewidth=2)\n",
    "axes[0,0].axvline(mu_mle, color='r', linestyle='--', linewidth=2, label=f'MLE μ={mu_mle:.2f}')\n",
    "axes[0,0].set_title('NumPy + scipy.stats.norm.pdf()', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_xlabel('μ (Mean)', fontsize=10)\n",
    "axes[0,0].set_ylabel('Likelihood L(μ)', fontsize=10)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].plot(mu_range, likelihoods_torch, 'g-', linewidth=2)\n",
    "axes[0,1].axvline(mu_mle, color='r', linestyle='--', linewidth=2, label=f'MLE μ={mu_mle:.2f}')\n",
    "axes[0,1].set_title('PyTorch (torch.distributions.Normal)', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_xlabel('μ (Mean)', fontsize=10)\n",
    "axes[0,1].set_ylabel('Likelihood L(μ)', fontsize=10)\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,0].plot(mu_range, likelihoods_tf, 'm-', linewidth=2)\n",
    "axes[1,0].axvline(mu_mle, color='r', linestyle='--', linewidth=2, label=f'MLE μ={mu_mle:.2f}')\n",
    "axes[1,0].set_title('TensorFlow (tfp.distributions.Normal)', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_xlabel('μ (Mean)', fontsize=10)\n",
    "axes[1,0].set_ylabel('Likelihood L(μ)', fontsize=10)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,1].plot(mu_range, likelihoods_np_manual, 'c-', linewidth=2)\n",
    "axes[1,1].axvline(mu_mle, color='r', linestyle='--', linewidth=2, label=f'MLE μ={mu_mle:.2f}')\n",
    "axes[1,1].set_title('NumPy (manual calculation)', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_xlabel('μ (Mean)', fontsize=10)\n",
    "axes[1,1].set_ylabel('Likelihood L(μ)', fontsize=10)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Gaussian Likelihood for Data: {x_data}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Gaussian Distribution MLE Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Observed data: {x_data}\")\n",
    "print(f\"True parameters: mu={mu_true}, sigma^2={sigma_sq_true}\")\n",
    "print(f\"MLE mu = {mu_mle:.4f}\")\n",
    "print(f\"\\nLikelihood at true parameters (mu={mu_true}, sigma^2={sigma_sq_true}):\")\n",
    "print(f\"  NumPy + scipy:  {likelihood_np:.6f}\")\n",
    "print(f\"  PyTorch:        {likelihood_torch.item():.6f}\")\n",
    "print(f\"  TensorFlow:     {likelihood_tf.numpy():.6f}\")\n",
    "print(f\"  NumPy (manual): {float(likelihood_np_manual):.6f}\")\n",
    "print(f\"\\nUsing Built-in Distributions:\")\n",
    "print(f\"  - NumPy:      scipy.stats.norm.pdf()\")\n",
    "print(f\"  - PyTorch:    torch.distributions.Normal\")\n",
    "print(f\"  - TensorFlow: tfp.distributions.Normal\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "theta = 0.7\n",
    "n_H = 7\n",
    "n_T = 3\n",
    "n_total = n_H + n_T\n",
    "\n",
    "binomial_dist = torch.distributions.Binomial(total_count=n_total, probs=theta)\n",
    "log_prob = binomial_dist.log_prob(torch.tensor(float(n_H)))\n",
    "likelihood = torch.exp(log_prob)\n",
    "\n",
    "print(\"Likelihood:\", likelihood.item())\n",
    "print(\"Expected:\", theta**n_H * (1 - theta)**n_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Gaussian Likelihood (Continuous Variables)\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Likelihood function for data drawn from a Gaussian (normal) distribution [1][2] with mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* This formula arises naturally from the Gaussian probability density function (pdf):\n",
    "\n",
    "$$\n",
    "f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "* If we assume each observation $x_i$ is independently drawn from a Gaussian distribution, the joint probability (likelihood) is the product of the individual probabilities.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "Given independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$:\n",
    "\n",
    "**1.** The joint probability (likelihood) is:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(x_i|\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "**2.** Substitute the Gaussian pdf for each $f(x_i|\\mu, \\sigma^2)$:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "**3.** Simplifying further is usually done by taking logarithms (Log-Likelihood) to ease calculations, but the above formula clearly defines the likelihood function.\n",
    "\n",
    "### Concrete Example and Calculation:\n",
    "\n",
    "**Suppose:** we have 3 observations drawn from a Gaussian distribution with mean $\\mu = 2$, and variance $\\sigma^2 = 1$:\n",
    "\n",
    "$$\n",
    "X = \\{1, 2, 3\\}\n",
    "$$\n",
    "\n",
    "* Compute each probability density individually:\n",
    "\n",
    "$$\n",
    "f(1|2, 1) = \\frac{1}{\\sqrt{2\\pi}} e^{-(1-2)^2/2} = \\frac{1}{\\sqrt{2\\pi}} e^{-0.5} \\approx 0.24197\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(2|2, 1) = \\frac{1}{\\sqrt{2\\pi}} e^{-(2-2)^2/2} = \\frac{1}{\\sqrt{2\\pi}} \\approx 0.39894\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(3|2, 1) = \\frac{1}{\\sqrt{2\\pi}} e^{-(3-2)^2/2} = \\frac{1}{\\sqrt{2\\pi}} e^{-0.5} \\approx 0.24197\n",
    "$$\n",
    "\n",
    "* Compute the likelihood (product):\n",
    "\n",
    "$$\n",
    "L(2, 1) = (0.24197) \\times (0.39894) \\times (0.24197) \\approx 0.02336\n",
    "$$\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu = 2\n",
    "sigma = np.sqrt(1)\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "likelihood = np.prod(norm.pdf(x, mu, sigma))\n",
    "\n",
    "print(\"Gaussian Likelihood:\", likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Maximum Likelihood Estimator for Discrete Probability\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "For discrete outcomes, the Maximum Likelihood Estimator (MLE) of the probability $\\theta_i$ of outcome $i$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_i = \\frac{n_i}{N}\n",
    "$$\n",
    "\n",
    "* $n_i$ is the number of occurrences of outcome $i$.\n",
    "* $N$ is the total number of observations.\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "This formula expresses the intuitive notion that the best estimate of the probability of observing a particular discrete outcome $i$ is simply the proportion of times this outcome occurred in all observed data.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "Given a set of independent discrete observations with $k$ possible outcomes, the likelihood is a multinomial distribution (without the constant combinatorial factor):\n",
    "\n",
    "$$\n",
    "L(\\theta_1, \\ldots, \\theta_k) \\propto \\prod_{i=1}^{k} \\theta_i^{n_i}\n",
    "$$\n",
    "\n",
    "Under the constraint:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} \\theta_i = 1\n",
    "$$\n",
    "\n",
    "We find MLE by maximizing this likelihood subject to the constraint above:\n",
    "\n",
    "* Using Lagrange multipliers, set up the Lagrangian:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\lambda) = \\sum_{i=1}^{k} n_i \\log(\\theta_i) - \\lambda\\left(\\sum_{i=1}^{k} \\theta_i - 1\\right)\n",
    "$$\n",
    "\n",
    "* Taking partial derivatives and setting equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_i} = \\frac{n_i}{\\theta_i} - \\lambda = 0 \\quad \\Rightarrow \\quad \\theta_i = \\frac{n_i}{\\lambda}\n",
    "$$\n",
    "\n",
    "* Summing over all outcomes $i$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} \\theta_i = 1 \\quad \\Rightarrow \\quad \\sum_{i=1}^{k} \\frac{n_i}{\\lambda} = \\frac{N}{\\lambda} = 1 \\quad \\Rightarrow \\quad \\lambda = N\n",
    "$$\n",
    "\n",
    "* Substitute back:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_i = \\frac{n_i}{N}\n",
    "$$\n",
    "\n",
    "Thus, we've rigorously derived the MLE for discrete probabilities.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we have a 6-sided dice rolled 20 times, resulting in the following frequencies:\n",
    "\n",
    "| Face | Frequency ($n_i$) |\n",
    "| --- | --- |\n",
    "| 1    | 2                 |\n",
    "| 2    | 5                 |\n",
    "| 3    | 3                 |\n",
    "| 4    | 4                 |\n",
    "| 5    | 4                 |\n",
    "| 6    | 2                 |\n",
    "\n",
    "* Total $N = 20$\n",
    "\n",
    "Calculate the MLE for face \"2\":\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_2 = \\frac{n_2}{N} = \\frac{5}{20} = 0.25\n",
    "$$\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frequencies = np.array([2, 5, 3, 4, 4, 2])\n",
    "N = np.sum(frequencies)\n",
    "theta_estimates = frequencies / N\n",
    "\n",
    "print(\"MLE estimates:\", theta_estimates)\n",
    "print(\"Sum of probabilities:\", np.sum(theta_estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Maximum Likelihood Estimator for Poisson Distribution\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given data $X = \\{x_1, x_2, \\ldots, x_n\\}$ independently drawn from a Poisson distribution with parameter $\\lambda$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) of $\\lambda$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The Poisson distribution is frequently used to model count-based data [2][3] (number of events occurring in a fixed interval).\n",
    "* The MLE intuitively corresponds to the average number of events observed, as the parameter $\\lambda$ represents the expected number of events.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Write the Likelihood function** for the Poisson distribution clearly:\n",
    "\n",
    "$$\n",
    "L(\\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\n",
    "$$\n",
    "\n",
    "**2. Take the natural logarithm** to simplify calculations (log-likelihood):\n",
    "\n",
    "$$\n",
    "l(\\lambda) = \\log L(\\lambda) = \\log\\left(\\prod_{i=1}^{n} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{n} \\log\\left(\\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\\right)\n",
    "$$\n",
    "\n",
    "**3. Simplify further:**\n",
    "\n",
    "$$\n",
    "l(\\lambda) = \\sum_{i=1}^{n} [x_i \\log(\\lambda) - \\lambda - \\log(x_i!)]\n",
    "$$\n",
    "\n",
    "**4. To find the MLE, take the derivative** with respect to $\\lambda$, set equal to zero, and solve for $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\lambda} l(\\lambda) = \\sum_{i=1}^{n} \\frac{x_i}{\\lambda} - n = 0\n",
    "$$\n",
    "\n",
    "**5. Solve for $\\lambda$:**\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\lambda}\\sum_{i=1}^{n} x_i - n = 0 \\quad \\Rightarrow \\quad \\hat{\\lambda} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the Poisson distribution is derived clearly and rigorously.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we have observed the number of accidents per day over a period of 5 days:\n",
    "\n",
    "| Day | Number of Accidents ($x_i$) |\n",
    "| --- | --- |\n",
    "| 1   | 2                           |\n",
    "| 2   | 4                           |\n",
    "| 3   | 1                           |\n",
    "| 4   | 3                           |\n",
    "| 5   | 0                           |\n",
    "\n",
    "Compute the MLE for $\\lambda$:\n",
    "\n",
    "* First, sum the observations:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{5} x_i = 2 + 4 + 1 + 3 + 0 = 10\n",
    "$$\n",
    "\n",
    "* Then divide by the number of observations $n = 5$:\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda} = \\frac{10}{5} = 2\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the Poisson parameter $\\lambda$ is 2.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([2, 4, 1, 3, 0])\n",
    "lambda_hat = np.mean(data)\n",
    "\n",
    "print(\"MLE for Poisson lambda:\", lambda_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Maximum Likelihood Estimator for Exponential Distribution\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$ drawn from an exponential distribution with parameter $\\lambda$, the likelihood function is defined as:\n",
    "\n",
    "$$\n",
    "L(\\lambda) = \\prod_{i=1}^{n} \\lambda e^{-\\lambda x_i}, \\quad x_i \\geq 0, \\lambda > 0\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) of $\\lambda$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} x_i}\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The exponential distribution models waiting times between events [2] (e.g., time until the next occurrence of an event).\n",
    "* The MLE intuitively corresponds to the inverse of the mean waiting time observed, reflecting the average rate at which events occur.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Write the likelihood function clearly:**\n",
    "\n",
    "Given exponential distribution pdf:\n",
    "\n",
    "$$\n",
    "f(x|\\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\n",
    "$$\n",
    "\n",
    "Thus, for independent observations:\n",
    "\n",
    "$$\n",
    "L(\\lambda) = \\prod_{i=1}^{n} \\lambda e^{-\\lambda x_i} = \\lambda^n e^{-\\lambda \\sum_{i=1}^{n} x_i}\n",
    "$$\n",
    "\n",
    "**2. Take the log-likelihood** to simplify the calculations:\n",
    "\n",
    "$$\n",
    "l(\\lambda) = \\log L(\\lambda) = \\log\\left(\\lambda^n e^{-\\lambda \\sum x_i}\\right)\n",
    "$$\n",
    "\n",
    "Simplify further:\n",
    "\n",
    "$$\n",
    "l(\\lambda) = n\\log(\\lambda) - \\lambda\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "**3. Differentiate the log-likelihood** w.r.t. $\\lambda$ and set equal to zero to find maximum:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\lambda} l(\\lambda) = \\frac{n}{\\lambda} - \\sum_{i=1}^{n} x_i = 0\n",
    "$$\n",
    "\n",
    "**4. Solve for $\\lambda$:**\n",
    "\n",
    "$$\n",
    "\\frac{n}{\\lambda} - \\sum_{i=1}^{n} x_i = 0 \\quad \\Rightarrow \\quad \\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} x_i}\n",
    "$$\n",
    "\n",
    "Thus, we clearly derived the MLE for the exponential distribution.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we have recorded waiting times (in minutes) for a customer to arrive at a store across 4 observations:\n",
    "\n",
    "| Observation | Waiting Time $x_i$ |\n",
    "| --- | --- |\n",
    "| 1           | 2                   |\n",
    "| 2           | 5                   |\n",
    "| 3           | 1                   |\n",
    "| 4           | 4                   |\n",
    "\n",
    "Compute MLE for $\\lambda$:\n",
    "\n",
    "* First, sum the observations:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{4} x_i = 2 + 5 + 1 + 4 = 12\n",
    "$$\n",
    "\n",
    "* Then divide the number of observations $n = 4$ by the sum:\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda} = \\frac{n}{\\sum x_i} = \\frac{4}{12} = \\frac{1}{3} \\approx 0.3333\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the exponential parameter $\\lambda$ is 0.3333.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "observations = np.array([2, 5, 1, 4])\n",
    "n = len(observations)\n",
    "sum_x = np.sum(observations)\n",
    "lambda_hat = n / sum_x\n",
    "\n",
    "print(\"MLE for Exponential lambda:\", lambda_hat)\n",
    "print(\"Computed as: n / sum(x) = {}/{} = {:.4f}\".format(n, sum_x, lambda_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. Maximum Likelihood Estimator for Mean of Gaussian Distribution with Known Variance\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "For independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$ drawn from a Gaussian distribution with unknown mean $\\mu$ and **known variance** $\\sigma^2$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) of $\\mu$ is:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* When the variance is known, the MLE for the Gaussian mean is simply the arithmetic average of the observed data.\n",
    "* Intuitively, the best estimate for the center of the data is the observed mean.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Write the likelihood function clearly:**\n",
    "\n",
    "For Gaussian observations with known variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "L(\\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "**2. Take the logarithm to simplify:**\n",
    "\n",
    "$$\n",
    "l(\\mu) = \\log L(\\mu) = \\sum_{i=1}^{n} \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\sum_{i=1}^{n} \\frac{(x_i - \\mu)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "The first term is constant (does not depend on $\\mu$), thus the optimization problem becomes minimizing:\n",
    "\n",
    "$$\n",
    "-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "**3. Differentiate the expression with respect to $\\mu$, set equal to zero:**\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\mu}\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (x_i - \\mu)^2\\right] = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(x_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} (x_i - \\mu) = 0\n",
    "$$\n",
    "\n",
    "**4. Solve for $\\mu$:**\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (x_i - \\mu) = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{n} x_i - n\\mu = 0 \\quad \\Rightarrow \\quad \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Thus, we derived clearly the MLE estimator for Gaussian mean with known variance.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Assume we have 5 observations drawn from a Gaussian distribution with unknown mean $\\mu$, known variance $\\sigma^2 = 4$:\n",
    "\n",
    "| Observation | $x_i$ |\n",
    "| --- | --- |\n",
    "| 1           | 10    |\n",
    "| 2           | 14    |\n",
    "| 3           | 9     |\n",
    "| 4           | 11    |\n",
    "| 5           | 16    |\n",
    "\n",
    "Calculate the MLE for the mean $\\mu$:\n",
    "\n",
    "* First, sum all observations:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{5} x_i = 10 + 14 + 9 + 11 + 16 = 60\n",
    "$$\n",
    "\n",
    "* Divide by the total number of observations $n = 5$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{60}{5} = 12\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the mean is 12.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "observations = np.array([10, 14, 9, 11, 16])\n",
    "mu_hat = np.mean(observations)\n",
    "\n",
    "print(\"MLE for Gaussian mean (mu):\", mu_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11. Maximum Likelihood Estimator for Variance of Gaussian Distribution with Known Mean\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$ drawn from a Gaussian distribution with **known mean** $\\mu$ and unknown variance $\\sigma^2$, the likelihood function is defined as:\n",
    "\n",
    "$$\n",
    "L(\\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) of the variance $\\sigma^2$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The MLE for variance with known mean represents the average squared deviation of observations from the known mean.\n",
    "* It provides the best estimate for variability or dispersion of the observed data.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Start from likelihood function:**\n",
    "\n",
    "For Gaussian distribution with known mean $\\mu$:\n",
    "\n",
    "$$\n",
    "L(\\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "**2. Take the natural logarithm** to simplify (log-likelihood):\n",
    "\n",
    "$$\n",
    "l(\\sigma^2) = \\log L(\\sigma^2) = \\sum_{i=1}^{n} \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\sum_{i=1}^{n} \\frac{(x_i - \\mu)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "Simplify further:\n",
    "\n",
    "$$\n",
    "l(\\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "**3. Differentiate log-likelihood** w.r.t. $\\sigma^2$ and set equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\sigma^2} l(\\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n} (x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "**4. Solve clearly for $\\sigma^2$:**\n",
    "\n",
    "$$\n",
    "-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n} (x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "Multiply both sides by $2(\\sigma^2)^2$:\n",
    "\n",
    "$$\n",
    "-n\\sigma^2 + \\sum_{i=1}^{n} (x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "Thus, we obtain:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we have observed data with a known mean $\\mu = 10$, and recorded 4 observations:\n",
    "\n",
    "| Observation | $x_i$ |\n",
    "| --- | --- |\n",
    "| 1           | 8     |\n",
    "| 2           | 9     |\n",
    "| 3           | 11    |\n",
    "| 4           | 12    |\n",
    "\n",
    "Compute the MLE for $\\sigma^2$:\n",
    "\n",
    "* Compute squared deviations from the known mean $\\mu = 10$:\n",
    "\n",
    "$$\n",
    "(8 - 10)^2 = 4, \\quad (9 - 10)^2 = 1, \\quad (11 - 10)^2 = 1, \\quad (12 - 10)^2 = 4\n",
    "$$\n",
    "\n",
    "* Sum these squared deviations:\n",
    "\n",
    "$$\n",
    "4 + 1 + 1 + 4 = 10\n",
    "$$\n",
    "\n",
    "* Divide by the number of observations ($n = 4$):\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{10}{4} = 2.5\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the variance is 2.5.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "observations = np.array([8, 9, 11, 12])\n",
    "mu_known = 10\n",
    "\n",
    "sigma_sq_hat = np.mean((observations - mu_known)**2)\n",
    "\n",
    "print(\"MLE for Gaussian variance (sigma^2):\", sigma_sq_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12. Maximum Likelihood Estimator for Mean of Gaussian Distribution with Unknown Variance\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$ from a Gaussian (normal) distribution with unknown mean $\\mu$ and unknown variance $\\sigma^2$, the joint likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) for the mean $\\mu$ (with unknown variance) is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "(This estimator is identical to the known-variance scenario for the mean.)\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* Even when variance is unknown, the MLE for the mean remains the arithmetic average of the observations.\n",
    "* The unknown variance does not affect the MLE for the mean, as the mean and variance estimates are independent in a Gaussian distribution.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Write clearly the joint likelihood function:**\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "**2. Take natural logarithm** to simplify (log-likelihood):\n",
    "\n",
    "$$\n",
    "l(\\mu, \\sigma^2) = \\log L(\\mu, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "**3. Differentiate log-likelihood with respect to $\\mu$**, set equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} l(\\mu, \\sigma^2) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(x_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} (x_i - \\mu) = 0\n",
    "$$\n",
    "\n",
    "**4. Solve clearly for $\\mu$:**\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (x_i - \\mu) = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{n} x_i - n\\mu = 0 \\quad \\Rightarrow \\quad \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Thus, we explicitly derived the MLE for the mean in the unknown-variance scenario.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we have recorded heights (in cm) of 4 individuals:\n",
    "\n",
    "| Observation | $x_i$ |\n",
    "| --- | --- |\n",
    "| 1           | 165   |\n",
    "| 2           | 170   |\n",
    "| 3           | 175   |\n",
    "| 4           | 160   |\n",
    "\n",
    "Compute the MLE for the mean $\\mu$:\n",
    "\n",
    "* First, sum all observations:\n",
    "\n",
    "$$\n",
    "165 + 170 + 175 + 160 = 670\n",
    "$$\n",
    "\n",
    "* Then divide by number of observations $n = 4$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{670}{4} = 167.5\n",
    "$$\n",
    "\n",
    "Thus, the MLE estimate for the mean height is 167.5 cm.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "observations = np.array([165, 170, 175, 160])\n",
    "mu_hat = np.mean(observations)\n",
    "\n",
    "print(\"MLE for Gaussian mean (mu):\", mu_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13. Maximum Likelihood Estimator for Variance of Gaussian Distribution with Unknown Mean\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$ drawn from a Gaussian (normal) distribution with unknown mean $\\mu$ and unknown variance $\\sigma^2$, the joint likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) for the variance $\\sigma^2$ with unknown mean is:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2, \\quad \\text{where } \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The MLE of variance when the mean is also unknown is computed using the sample mean $\\hat{\\mu}$, and it represents the average squared deviations from this estimated mean.\n",
    "* This provides the best statistical estimate of data variability around the observed mean.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Clearly write down the joint likelihood function:**\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "**2. Take the natural logarithm** (log-likelihood) to simplify calculations:\n",
    "\n",
    "$$\n",
    "l(\\mu, \\sigma^2) = \\log L(\\mu, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "**3. First, find the estimator for the mean $\\mu$** (as done previously):\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "**4. Substitute this $\\hat{\\mu}$ into the log-likelihood and differentiate with respect to $\\sigma^2$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\sigma^2} l(\\mu, \\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2 = 0\n",
    "$$\n",
    "\n",
    "**5. Solve explicitly for $\\sigma^2$:**\n",
    "\n",
    "Multiply by $2(\\sigma^2)^2$:\n",
    "\n",
    "$$\n",
    "-n\\sigma^2 + \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2 = 0\n",
    "$$\n",
    "\n",
    "Thus, the estimator is explicitly obtained:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2\n",
    "$$\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we have observed the exam scores of 5 students (out of 100):\n",
    "\n",
    "| Student | Score ($x_i$) |\n",
    "| --- | --- |\n",
    "| 1       | 75            |\n",
    "| 2       | 80            |\n",
    "| 3       | 85            |\n",
    "| 4       | 70            |\n",
    "| 5       | 90            |\n",
    "\n",
    "* First, calculate sample mean $\\hat{\\mu}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{75 + 80 + 85 + 70 + 90}{5} = \\frac{400}{5} = 80\n",
    "$$\n",
    "\n",
    "* Then, calculate squared deviations from this mean:\n",
    "\n",
    "$$\n",
    "(75 - 80)^2 = 25, \\quad (80 - 80)^2 = 0, \\quad (85 - 80)^2 = 25, \\quad (70 - 80)^2 = 100, \\quad (90 - 80)^2 = 100\n",
    "$$\n",
    "\n",
    "* Sum these squared deviations:\n",
    "\n",
    "$$\n",
    "25 + 0 + 25 + 100 + 100 = 250\n",
    "$$\n",
    "\n",
    "* Finally, compute variance MLE ($\\hat{\\sigma}^2$):\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{250}{5} = 50\n",
    "$$\n",
    "\n",
    "Thus, the MLE for variance is 50.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores = np.array([75, 80, 85, 70, 90])\n",
    "mu_hat = np.mean(scores)\n",
    "sigma_sq_hat = np.var(scores, ddof=0)\n",
    "sigma_sq_hat_manual = np.mean((scores - mu_hat)**2)\n",
    "\n",
    "print(\"MLE for Gaussian variance (sigma^2):\", sigma_sq_hat)\n",
    "print(\"Manual calculation:\", sigma_sq_hat_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14. Maximum Likelihood Estimator for Bernoulli Distribution\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given independent Bernoulli trials (e.g., coin flips) with outcomes $X = \\{x_1, x_2, \\ldots, x_n\\}$, where each $x_i \\in \\{0, 1\\}$ and parameter $\\theta$ is the probability of success ($x_i = 1$), the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} \\theta^{x_i}(1-\\theta)^{1-x_i}\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) for the parameter $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The MLE for a Bernoulli parameter intuitively corresponds to the proportion of successes observed in the trials.\n",
    "* This estimator is a straightforward and intuitive measure of the likelihood of success.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Clearly write down the likelihood function** for Bernoulli distribution:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} \\theta^{x_i}(1-\\theta)^{1-x_i}\n",
    "$$\n",
    "\n",
    "**2. Convert to log-likelihood** for easier differentiation:\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} [x_i \\log(\\theta) + (1-x_i) \\log(1-\\theta)]\n",
    "$$\n",
    "\n",
    "**3. Differentiate the log-likelihood** w.r.t. $\\theta$ and set equal to zero to maximize:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} l(\\theta) = \\sum_{i=1}^{n} \\frac{x_i}{\\theta} - \\sum_{i=1}^{n} \\frac{1-x_i}{1-\\theta} = 0\n",
    "$$\n",
    "\n",
    "**4. Solve explicitly for $\\theta$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^{n} x_i}{\\theta} = \\frac{\\sum_{i=1}^{n} (1-x_i)}{1-\\theta}\n",
    "$$\n",
    "\n",
    "Cross multiply:\n",
    "\n",
    "$$\n",
    "(1-\\theta)\\sum_{i=1}^{n} x_i = \\theta \\sum_{i=1}^{n} (1-x_i)\n",
    "$$\n",
    "\n",
    "Simplify the right-hand side:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_i - \\theta \\sum_{i=1}^{n} x_i = \\theta(n - \\sum_{i=1}^{n} x_i)\n",
    "$$\n",
    "\n",
    "Rearrange terms clearly:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_i = \\theta n \\quad \\Rightarrow \\quad \\hat{\\theta} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n",
    "$$\n",
    "\n",
    "Thus, we've explicitly derived the MLE estimator for a Bernoulli distribution.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we flip a coin 8 times, getting the following results (1 = Heads, 0 = Tails):\n",
    "\n",
    "| Flip | Outcome ($x_i$) |\n",
    "| --- | --- |\n",
    "| 1    | 1               |\n",
    "| 2    | 0               |\n",
    "| 3    | 1               |\n",
    "| 4    | 1               |\n",
    "| 5    | 0               |\n",
    "| 6    | 1               |\n",
    "| 7    | 0               |\n",
    "| 8    | 1               |\n",
    "\n",
    "Calculate the MLE for the probability of Heads ($\\theta$):\n",
    "\n",
    "* Sum of successes:\n",
    "\n",
    "$$\n",
    "1 + 0 + 1 + 1 + 0 + 1 + 0 + 1 = 5\n",
    "$$\n",
    "\n",
    "* Divide by total number of flips $n = 8$:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{5}{8} = 0.625\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the Bernoulli parameter is 0.625.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "outcomes = np.array([1, 0, 1, 1, 0, 1, 0, 1])\n",
    "theta_hat = np.mean(outcomes)\n",
    "\n",
    "print(\"MLE for Bernoulli theta:\", theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.15. Maximum Likelihood Estimator for Multinomial Distribution\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given observations from a multinomial distribution with $k$ possible outcomes and parameters $\\theta = \\{\\theta_1, \\theta_2, \\ldots, \\theta_k\\}$, where each outcome $i$ has probability $\\theta_i$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{n!}{n_1!n_2!\\ldots n_k!} \\theta_1^{n_1}\\theta_2^{n_2}\\ldots\\theta_k^{n_k}, \\quad \\text{where} \\quad \\sum_{i=1}^{k} \\theta_i = 1\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) for each parameter $\\theta_i$ is:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_i = \\frac{n_i}{n}, \\quad \\text{where} \\quad n = \\sum_{i=1}^{k} n_i\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The multinomial distribution generalizes the binomial distribution to cases where each trial has more than two possible outcomes.\n",
    "* The MLE estimator for each probability is simply the proportion of times each outcome was observed.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Clearly write down the multinomial likelihood function:**\n",
    "\n",
    "Given multinomial outcomes with counts $n_1, n_2, \\ldots, n_k$:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{n!}{n_1!n_2!\\ldots n_k!} \\theta_1^{n_1}\\theta_2^{n_2}\\ldots\\theta_k^{n_k}\n",
    "$$\n",
    "\n",
    "**2. Convert to log-likelihood** for simpler differentiation (constants removed as they don't affect the optimization):\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\sum_{i=1}^{k} n_i \\log(\\theta_i)\n",
    "$$\n",
    "\n",
    "**3. To account for the constraint** $\\sum_{i=1}^{k} \\theta_i = 1$, introduce a Lagrange multiplier $\\lambda$ and form the Lagrangian clearly:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\lambda) = \\sum_{i=1}^{k} n_i \\log(\\theta_i) - \\lambda\\left(\\sum_{i=1}^{k} \\theta_i - 1\\right)\n",
    "$$\n",
    "\n",
    "**4. Differentiate w.r.t. each $\\theta_i$** and set equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_i} = \\frac{n_i}{\\theta_i} - \\lambda = 0 \\quad \\Rightarrow \\quad \\theta_i = \\frac{n_i}{\\lambda}\n",
    "$$\n",
    "\n",
    "**5. Using the constraint** $\\sum_{i=1}^{k} \\theta_i = 1$, find $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} \\theta_i = 1 \\quad \\Rightarrow \\quad \\sum_{i=1}^{k} \\frac{n_i}{\\lambda} = \\frac{n}{\\lambda} = 1 \\quad \\Rightarrow \\quad \\lambda = n\n",
    "$$\n",
    "\n",
    "**6. Substitute back** to get MLE clearly:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_i = \\frac{n_i}{n}\n",
    "$$\n",
    "\n",
    "Thus, explicitly derived the MLE estimators for the multinomial distribution.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we survey 50 people on their favorite fruits, obtaining the following responses:\n",
    "\n",
    "| Fruit  | Count ($n_i$) |\n",
    "| --- | --- |\n",
    "| Apple  | 15            |\n",
    "| Banana | 10            |\n",
    "| Orange | 20            |\n",
    "| Grape  | 5             |\n",
    "\n",
    "Calculate the MLE estimates for each fruit category:\n",
    "\n",
    "* Total count $n = 15 + 10 + 20 + 5 = 50$\n",
    "\n",
    "Then clearly calculate proportions:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\text{Apple} = \\frac{15}{50} = 0.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\text{Banana} = \\frac{10}{50} = 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\text{Orange} = \\frac{20}{50} = 0.4\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\text{Grape} = \\frac{5}{50} = 0.1\n",
    "$$\n",
    "\n",
    "Thus, we clearly calculated the MLE probabilities for each outcome.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "counts = np.array([15, 10, 20, 5])\n",
    "n_total = np.sum(counts)\n",
    "theta_hat = counts / n_total\n",
    "\n",
    "fruits = [\"Apple\", \"Banana\", \"Orange\", \"Grape\"]\n",
    "for fruit, theta in zip(fruits, theta_hat):\n",
    "    print(f\"MLE for {fruit}: {theta:.2f}\")\n",
    "print(f\"\\nSum of probabilities: {np.sum(theta_hat):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.16. Maximum Likelihood Estimator for Uniform Distribution\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$ drawn from a uniform distribution defined over the interval $[0, \\theta]$, where $0 \\leq x_i \\leq \\theta$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\begin{cases}\n",
    "\\frac{1}{\\theta^n}, & \\text{if } 0 \\leq x_i \\leq \\theta \\text{ for all } i \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) for the parameter $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\max\\{x_1, x_2, \\ldots, x_n\\}\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The MLE for the uniform distribution intuitively corresponds to the largest observed value.\n",
    "* This makes sense because the likelihood of observing all data is maximized when the interval just barely covers all observed data points, making it as small as possible but still valid.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Clearly define the likelihood function:**\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\begin{cases}\n",
    "\\frac{1}{\\theta^n}, & 0 \\leq x_i \\leq \\theta \\quad \\forall i \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**2. For the likelihood to be non-zero**, the parameter $\\theta$ must be at least as large as the maximum observed value, $x_{(n)} = \\max(x_1, x_2, \\ldots, x_n)$.\n",
    "\n",
    "**3. Given this constraint, clearly rewrite the likelihood function as:**\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{\\theta^n}, \\quad \\theta \\geq x_{(n)}\n",
    "$$\n",
    "\n",
    "**4. To maximize $L(\\theta)$, minimize the denominator** ($\\theta^n$), meaning choosing the smallest possible $\\theta$ that satisfies the constraint. Thus:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = x_{(n)} = \\max(x_1, x_2, \\ldots, x_n)\n",
    "$$\n",
    "\n",
    "We clearly derived the MLE estimator for the uniform distribution.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we have measured lengths (in cm) of 5 randomly selected metal rods produced in a factory, known to follow a uniform distribution over $[0, \\theta]$:\n",
    "\n",
    "| Rod | Length ($x_i$) |\n",
    "| --- | --- |\n",
    "| 1   | 10             |\n",
    "| 2   | 12             |\n",
    "| 3   | 8              |\n",
    "| 4   | 9              |\n",
    "| 5   | 11             |\n",
    "\n",
    "* Determine the maximum length observed:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\max\\{10, 12, 8, 9, 11\\} = 12\n",
    "$$\n",
    "\n",
    "Thus, the MLE estimate for $\\theta$ is clearly 12 cm.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "measurements = np.array([10, 12, 8, 9, 11])\n",
    "theta_hat = np.max(measurements)\n",
    "\n",
    "print(\"MLE for Uniform distribution (theta):\", theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.17. Maximum Likelihood Estimator for Geometric Distribution\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$, each following a geometric distribution with parameter $\\theta$ (probability of success), the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} (1-\\theta)^{x_i-1}\\theta, \\quad 0 < \\theta \\leq 1\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) for parameter $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{n}{\\sum_{i=1}^{n} x_i}\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The geometric distribution describes the number of trials until the first success.\n",
    "* The MLE intuitively corresponds to the reciprocal of the average number of trials needed for success.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Clearly define the geometric likelihood function:**\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} (1-\\theta)^{x_i-1}\\theta\n",
    "$$\n",
    "\n",
    "**2. Take the natural logarithm** (log-likelihood) for easier computation:\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} [(x_i - 1) \\log(1-\\theta) + \\log(\\theta)]\n",
    "$$\n",
    "\n",
    "**3. Differentiate the log-likelihood** with respect to $\\theta$, set equal to zero for maximization:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} l(\\theta) = \\sum_{i=1}^{n} \\frac{-(x_i - 1)}{1-\\theta} + \\frac{n}{\\theta} = 0\n",
    "$$\n",
    "\n",
    "**4. Solve explicitly for $\\theta$:**\n",
    "\n",
    "$$\n",
    "\\frac{n}{\\theta} = \\frac{\\sum_{i=1}^{n} (x_i - 1)}{1-\\theta}\n",
    "$$\n",
    "\n",
    "Cross-multiply and simplify:\n",
    "\n",
    "$$\n",
    "n(1-\\theta) = \\theta\\sum_{i=1}^{n} (x_i - 1)\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "n - n\\theta = \\theta\\sum_{i=1}^{n} x_i - n\\theta\n",
    "$$\n",
    "\n",
    "Simplify (subtracting $-n\\theta$ on both sides):\n",
    "\n",
    "$$\n",
    "n = \\theta\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Thus, clearly obtaining the MLE estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{n}{\\sum_{i=1}^{n} x_i}\n",
    "$$\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we perform an experiment repeatedly and observe the following numbers of trials until the first success occurs:\n",
    "\n",
    "| Observation | Trials until Success ($x_i$) |\n",
    "| --- | --- |\n",
    "| 1           | 3                            |\n",
    "| 2           | 4                            |\n",
    "| 3           | 2                            |\n",
    "| 4           | 5                            |\n",
    "| 5           | 1                            |\n",
    "\n",
    "* Sum these observations:\n",
    "\n",
    "$$\n",
    "3 + 4 + 2 + 5 + 1 = 15\n",
    "$$\n",
    "\n",
    "* Calculate MLE clearly:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{n}{\\sum_{i=1}^{n} x_i} = \\frac{5}{15} = \\frac{1}{3} \\approx 0.3333\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the geometric parameter $\\theta$ is 0.3333.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "trials = np.array([3, 4, 2, 5, 1])\n",
    "n = len(trials)\n",
    "sum_trials = np.sum(trials)\n",
    "theta_hat = n / sum_trials\n",
    "\n",
    "print(\"MLE for Geometric theta:\", theta_hat)\n",
    "print(\"Computed as: n / sum(trials) = {}/{} = {:.4f}\".format(n, sum_trials, theta_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.18. Maximum Likelihood Estimator for Negative Binomial Distribution\n",
    "\n",
    "### Full Formula:\n",
    "\n",
    "Given independent observations $X = \\{x_1, x_2, \\ldots, x_n\\}$ from a negative binomial distribution, defined as the number of failures ($x_i$) until $r$ successes are reached, with success probability $\\theta$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} \\binom{x_i + r - 1}{x_i} (1-\\theta)^{x_i}\\theta^r\n",
    "$$\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) for parameter $\\theta$ (probability of success) is:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{nr}{nr + \\sum_{i=1}^{n} x_i}\n",
    "$$\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "* The negative binomial distribution models the number of failures before a fixed number of successes ($r$) occurs.\n",
    "* The MLE estimator intuitively represents the proportion of successes relative to the total number of trials (successes + failures).\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "**1. Clearly define the negative binomial likelihood function:**\n",
    "\n",
    "Given independent observations:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} \\binom{x_i + r - 1}{x_i} (1-\\theta)^{x_i}\\theta^r\n",
    "$$\n",
    "\n",
    "**2. Take natural logarithm** to simplify (ignoring constant binomial coefficients as they don't affect maximization w.r.t $\\theta$):\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} [x_i \\log(1-\\theta) + r \\log(\\theta)] + (\\text{constant terms})\n",
    "$$\n",
    "\n",
    "**3. Differentiate log-likelihood clearly** w.r.t. $\\theta$, set equal to zero to maximize:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} l(\\theta) = \\sum_{i=1}^{n} \\frac{-x_i}{1-\\theta} + \\frac{nr}{\\theta} = 0\n",
    "$$\n",
    "\n",
    "**4. Solve explicitly for $\\theta$:**\n",
    "\n",
    "$$\n",
    "\\frac{nr}{\\theta} = \\frac{\\sum_{i=1}^{n} x_i}{1-\\theta}\n",
    "$$\n",
    "\n",
    "Cross multiply to solve:\n",
    "\n",
    "$$\n",
    "nr(1-\\theta) = \\theta\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Expand clearly:\n",
    "\n",
    "$$\n",
    "nr - nr\\theta = \\theta\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Collect terms with $\\theta$:\n",
    "\n",
    "$$\n",
    "nr = \\theta\\sum_{i=1}^{n} x_i + nr\\theta\n",
    "$$\n",
    "\n",
    "Factor $\\theta$ clearly:\n",
    "\n",
    "$$\n",
    "nr = \\theta\\left(\\sum_{i=1}^{n} x_i + nr\\right)\n",
    "$$\n",
    "\n",
    "Solve explicitly for $\\theta$:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{nr}{nr + \\sum_{i=1}^{n} x_i}\n",
    "$$\n",
    "\n",
    "Thus, explicitly derived the MLE for negative binomial distribution.\n",
    "\n",
    "### Concrete Example & Calculation:\n",
    "\n",
    "**Example:** Suppose we observe the number of failed attempts to make exactly $r = 2$ successful shots in basketball across 4 trials:\n",
    "\n",
    "| Trial | Failures until 2 successes ($x_i$) |\n",
    "| --- | --- |\n",
    "| 1     | 3                                   |\n",
    "| 2     | 2                                   |\n",
    "| 3     | 4                                   |\n",
    "| 4     | 1                                   |\n",
    "\n",
    "* First, calculate the total number of failures:\n",
    "\n",
    "$$\n",
    "3 + 2 + 4 + 1 = 10\n",
    "$$\n",
    "\n",
    "* Compute clearly MLE for $\\theta$:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{nr}{nr + \\sum_{i=1}^{n} x_i} = \\frac{4 \\times 2}{(4 \\times 2) + 10} = \\frac{8}{18} = 0.4444\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the success probability $\\theta$ is 0.4444.\n",
    "\n",
    "### Python Verification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_i = np.array([3, 2, 4, 1])\n",
    "r = 2\n",
    "n = len(x_i)\n",
    "sum_x = np.sum(x_i)\n",
    "\n",
    "theta_hat = (n * r) / (n * r + sum_x)\n",
    "\n",
    "print(\"MLE for Negative Binomial theta:\", theta_hat)\n",
    "print(\"Computed as: (n*r) / (n*r + sum(x_i)) = ({0}*{1}) / ({0}*{1} + {2}) = {3:.4f}\".format(n, r, sum_x, theta_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise Solutions\n",
    "\n",
    "This section provides detailed solutions to all exercises from Section 22.7 of the D2L textbook [1], including:\n",
    "- **Detailed reasoning:** Clear mathematical derivations\n",
    "- **Computations:** All calculation steps shown explicitly\n",
    "- **Python implementation:** Numerical verification of analytical results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Exercise 1\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Given a probability density function (PDF):\n",
    "\n",
    "$$\n",
    "p(x|\\alpha) = \\alpha e^{-\\alpha x}, \\quad x \\geq 0, \\alpha > 0\n",
    "$$\n",
    "\n",
    "We have a single observed data point $x = 3$.\n",
    "\n",
    "Find the Maximum Likelihood Estimator (MLE) for the parameter $\\alpha$.\n",
    "\n",
    "\n",
    "### Solution:\n",
    "\n",
    "**Step 1: Write the Likelihood Function Clearly**\n",
    "\n",
    "Since we have only one observation $x = 3$:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = p(3|\\alpha) = \\alpha e^{-3\\alpha}\n",
    "$$\n",
    "\n",
    "**Step 2: Convert to Log-Likelihood**\n",
    "\n",
    "Take the natural logarithm to simplify calculations:\n",
    "\n",
    "$$\n",
    "l(\\alpha) = \\log(\\alpha e^{-3\\alpha}) = \\log(\\alpha) + \\log(e^{-3\\alpha})\n",
    "$$\n",
    "\n",
    "Simplify clearly:\n",
    "\n",
    "$$\n",
    "l(\\alpha) = \\log(\\alpha) - 3\\alpha\n",
    "$$\n",
    "\n",
    "**Step 3: Differentiate and Set to Zero to Maximize**\n",
    "\n",
    "Differentiate $l(\\alpha)$ with respect to $\\alpha$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\alpha}l(\\alpha) = \\frac{1}{\\alpha} - 3\n",
    "$$\n",
    "\n",
    "Set this derivative equal to zero to find the maximum clearly:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\alpha} - 3 = 0\n",
    "$$\n",
    "\n",
    "**Step 4: Solve explicitly for α**\n",
    "\n",
    "Solve the equation detailed:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\alpha} - 3 = 0 \\quad \\Rightarrow \\quad \\frac{1}{\\alpha} = 3 \\quad \\Rightarrow \\quad \\hat{\\alpha} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "Thus, clearly we have obtained the MLE:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "**Interpretation:** For an exponential distribution, the MLE for the rate parameter $\\alpha$ is the reciprocal of the observed value. This makes intuitive sense: if we observe $x=3$, we estimate the average rate as $1/3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import expon\n",
    "\n",
    "x_observed = 3.0\n",
    "alpha_mle = 1 / x_observed\n",
    "alpha_range = np.linspace(0.01, 2, 200)\n",
    "\n",
    "# NumPy (scipy.expon uses scale=1/rate)\n",
    "likelihoods_np = [expon.pdf(x_observed, scale=1/alpha) for alpha in alpha_range]\n",
    "\n",
    "# PyTorch\n",
    "likelihoods_torch = []\n",
    "for alpha in alpha_range:\n",
    "    exp_dist = torch.distributions.Exponential(rate=alpha)\n",
    "    prob = torch.exp(exp_dist.log_prob(torch.tensor(x_observed)))\n",
    "    likelihoods_torch.append(prob.item())\n",
    "\n",
    "# TensorFlow\n",
    "likelihoods_tf = []\n",
    "for alpha in alpha_range:\n",
    "    exp_dist = tfp.distributions.Exponential(rate=alpha)\n",
    "    prob = tf.exp(exp_dist.log_prob(x_observed))\n",
    "    likelihoods_tf.append(prob.numpy())\n",
    "\n",
    "# NumPy (manual calculation)\n",
    "likelihoods_np_manual = alpha_range * np.exp(-alpha_range * x_observed)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# NumPy\n",
    "axes[0,0].plot(alpha_range, likelihoods_np, 'b-', linewidth=2)\n",
    "axes[0,0].axvline(alpha_mle, color='red', linestyle='--', linewidth=2, label=f'MLE alpha={alpha_mle:.4f}')\n",
    "axes[0,0].set_xlabel('α', fontsize=12)\n",
    "axes[0,0].set_ylabel('Likelihood L(α)', fontsize=12)\n",
    "axes[0,0].set_title('NumPy + scipy.stats.expon.pdf()', fontsize=12, fontweight='bold')\n",
    "axes[0,0].legend(fontsize=10)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# PyTorch\n",
    "axes[0,1].plot(alpha_range, likelihoods_torch, 'g-', linewidth=2)\n",
    "axes[0,1].axvline(alpha_mle, color='red', linestyle='--', linewidth=2, label=f'MLE alpha={alpha_mle:.4f}')\n",
    "axes[0,1].set_xlabel('α', fontsize=12)\n",
    "axes[0,1].set_ylabel('Likelihood L(α)', fontsize=12)\n",
    "axes[0,1].set_title('PyTorch (torch.distributions.Exponential)', fontsize=12, fontweight='bold')\n",
    "axes[0,1].legend(fontsize=10)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# TensorFlow\n",
    "axes[1,0].plot(alpha_range, likelihoods_tf, 'm-', linewidth=2)\n",
    "axes[1,0].axvline(alpha_mle, color='red', linestyle='--', linewidth=2, label=f'MLE alpha={alpha_mle:.4f}')\n",
    "axes[1,0].set_xlabel('α', fontsize=12)\n",
    "axes[1,0].set_ylabel('Likelihood L(α)', fontsize=12)\n",
    "axes[1,0].set_title('TensorFlow (tfp.distributions.Exponential)', fontsize=12, fontweight='bold')\n",
    "axes[1,0].legend(fontsize=10)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# NumPy (manual)\n",
    "axes[1,1].plot(alpha_range, likelihoods_np_manual, 'c-', linewidth=2)\n",
    "axes[1,1].axvline(alpha_mle, color='red', linestyle='--', linewidth=2, label=f'MLE alpha={alpha_mle:.4f}')\n",
    "axes[1,1].set_xlabel('α', fontsize=12)\n",
    "axes[1,1].set_ylabel('Likelihood L(α)', fontsize=12)\n",
    "axes[1,1].set_title('NumPy (manual calculation)', fontsize=12, fontweight='bold')\n",
    "axes[1,1].legend(fontsize=10)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Exponential Distribution Likelihood (x={x_observed})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Exponential Distribution MLE - Exercise 1\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Observed: x = {x_observed}\")\n",
    "print(f\"MLE: alpha = 1/x = {alpha_mle:.4f}\")\n",
    "print(f\"\\nAll implementations confirm: alpha = 1/3 ~ 0.3333\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Exercise 2\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Given observations $X = \\{x_1, x_2, \\ldots, x_n\\}$ from a Gaussian distribution with unknown mean $\\mu$ and variance = 1, the likelihood is:\n",
    "\n",
    "$$\n",
    "L(\\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i-\\mu)^2}{2}\\right)\n",
    "$$\n",
    "\n",
    "Find the Maximum Likelihood Estimator (MLE) for the mean $\\mu$.\n",
    "\n",
    "### Solution:\n",
    "\n",
    "**Step 1: Write the Likelihood Clearly**\n",
    "\n",
    "$$\n",
    "L(\\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i-\\mu)^2}{2}\\right)\n",
    "$$\n",
    "\n",
    "**Step 2: Convert to Log-Likelihood to Simplify**\n",
    "\n",
    "Taking the natural logarithm:\n",
    "\n",
    "$$\n",
    "l(\\mu) = \\log L(\\mu) = \\sum_{i=1}^{n} \\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{1}{2}\\sum_{i=1}^{n}(x_i-\\mu)^2\n",
    "$$\n",
    "\n",
    "Note that the first term (constant) is independent of $\\mu$, thus to maximize we only focus on minimizing:\n",
    "\n",
    "$$\n",
    "-\\frac{1}{2}\\sum_{i=1}^{n}(x_i-\\mu)^2\n",
    "$$\n",
    "\n",
    "**Step 3: Differentiate and Set Equal to Zero**\n",
    "\n",
    "Differentiate clearly:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\mu}\\left(-\\frac{1}{2}\\sum_{i=1}^{n}(x_i-\\mu)^2\\right) = -\\frac{1}{2}\\sum_{i=1}^{n}2(x_i-\\mu)(-1) = \\sum_{i=1}^{n}(x_i-\\mu)\n",
    "$$\n",
    "\n",
    "Set equal to zero to find the maximum:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}(x_i-\\mu) = 0\n",
    "$$\n",
    "\n",
    "**Step 4: Solve Explicitly for μ**\n",
    "\n",
    "Solve explicitly for $\\mu$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_i - n\\mu = 0 \\quad \\Rightarrow \\quad \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Thus, we clearly obtain the MLE for mean as the arithmetic average:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "**Interpretation:** The MLE for the mean of a Gaussian distribution is simply the sample mean. This is an intuitive and well-known result in statistics.\n",
    "\n",
    "\n",
    "### Concrete Numerical Example:\n",
    "\n",
    "Suppose we have 4 observations from a Gaussian distribution (variance=1):\n",
    "\n",
    "| Observation | Value $x_i$ |\n",
    "| --- | --- |\n",
    "| 1           | 5           |\n",
    "| 2           | 7           |\n",
    "| 3           | 9           |\n",
    "| 4           | 11          |\n",
    "\n",
    "Calculate the MLE for $\\mu$:\n",
    "\n",
    "- Clearly sum observations:\n",
    "\n",
    "$$\n",
    "5 + 7 + 9 + 11 = 32\n",
    "$$\n",
    "\n",
    "- Divide by $n = 4$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{32}{4} = 8\n",
    "$$\n",
    "\n",
    "Thus, the MLE for the mean clearly is **8**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "observations = np.array([5.0, 7.0, 9.0, 11.0])\n",
    "known_std = 1.0\n",
    "\n",
    "# NumPy\n",
    "mu_hat_np = np.mean(observations)\n",
    "gaussian_np = norm(mu_hat_np, known_std)\n",
    "log_likelihood_np = gaussian_np.logpdf(observations).sum()\n",
    "\n",
    "mu_range = np.linspace(5, 11, 200)\n",
    "likelihoods_np = np.array([np.exp(norm(mu, known_std).logpdf(observations).sum()) \n",
    "                           for mu in mu_range])\n",
    "\n",
    "# PyTorch\n",
    "observations_torch = torch.tensor(observations, dtype=torch.float32)\n",
    "mu_hat_torch = observations_torch.mean()\n",
    "gaussian_torch = torch.distributions.Normal(mu_hat_torch, known_std)\n",
    "log_likelihood_torch = gaussian_torch.log_prob(observations_torch).sum()\n",
    "\n",
    "mu_range_torch = torch.linspace(5, 11, 200)\n",
    "likelihoods_torch = torch.zeros(200)\n",
    "for i, mu in enumerate(mu_range_torch):\n",
    "    dist = torch.distributions.Normal(mu, known_std)\n",
    "    likelihoods_torch[i] = torch.exp(dist.log_prob(observations_torch).sum())\n",
    "\n",
    "# TensorFlow\n",
    "observations_tf = tf.constant(observations, dtype=tf.float32)\n",
    "mu_hat_tf = tf.reduce_mean(observations_tf)\n",
    "gaussian_tf = tfp.distributions.Normal(mu_hat_tf, known_std)\n",
    "log_likelihood_tf = tf.reduce_sum(gaussian_tf.log_prob(observations_tf))\n",
    "\n",
    "mu_range_tf = tf.linspace(5.0, 11.0, 200)\n",
    "likelihoods_tf = []\n",
    "for mu in mu_range_tf:\n",
    "    dist = tfp.distributions.Normal(mu, known_std)\n",
    "    lik = tf.exp(tf.reduce_sum(dist.log_prob(observations_tf)))\n",
    "    likelihoods_tf.append(lik.numpy())\n",
    "likelihoods_tf = np.array(likelihoods_tf)\n",
    "\n",
    "# NumPy (manual calculation)\n",
    "likelihoods_np_manual = np.zeros(200)\n",
    "for i, mu in enumerate(mu_range):\n",
    "    pdf_values = (1 / np.sqrt(2 * np.pi * known_std**2)) * \\\n",
    "                 np.exp(-((observations - mu)**2) / (2 * known_std**2))\n",
    "    likelihoods_np_manual[i] = np.prod(pdf_values)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# NumPy + SciPy\n",
    "axes[0,0].plot(mu_range, likelihoods_np, 'b-', linewidth=2)\n",
    "axes[0,0].axvline(mu_hat_np, color='red', linestyle='--', linewidth=2, \n",
    "                  label=f'MLE mu={mu_hat_np:.1f}')\n",
    "axes[0,0].scatter(observations, [0]*len(observations), color='green', s=100, \n",
    "                  zorder=5, label='Observations')\n",
    "axes[0,0].set_xlabel('μ (mean)', fontsize=12)\n",
    "axes[0,0].set_ylabel('Likelihood L(μ)', fontsize=12)\n",
    "axes[0,0].set_title('NumPy + scipy.stats.norm', fontsize=12, fontweight='bold')\n",
    "axes[0,0].legend(fontsize=10)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# PyTorch\n",
    "axes[0,1].plot(mu_range_torch.numpy(), likelihoods_torch.numpy(), 'g-', linewidth=2)\n",
    "axes[0,1].axvline(mu_hat_torch.item(), color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'MLE mu={mu_hat_torch.item():.1f}')\n",
    "axes[0,1].scatter(observations, [0]*len(observations), color='green', s=100,\n",
    "                  zorder=5, label='Observations')\n",
    "axes[0,1].set_xlabel('μ (mean)', fontsize=12)\n",
    "axes[0,1].set_ylabel('Likelihood L(μ)', fontsize=12)\n",
    "axes[0,1].set_title('PyTorch (torch.distributions.Normal)', fontsize=12, fontweight='bold')\n",
    "axes[0,1].legend(fontsize=10)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# TensorFlow\n",
    "axes[1,0].plot(mu_range_tf.numpy(), likelihoods_tf, 'm-', linewidth=2)\n",
    "axes[1,0].axvline(mu_hat_tf.numpy(), color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'MLE mu={mu_hat_tf.numpy():.1f}')\n",
    "axes[1,0].scatter(observations, [0]*len(observations), color='green', s=100,\n",
    "                  zorder=5, label='Observations')\n",
    "axes[1,0].set_xlabel('μ (mean)', fontsize=12)\n",
    "axes[1,0].set_ylabel('Likelihood L(μ)', fontsize=12)\n",
    "axes[1,0].set_title('TensorFlow (tfp.distributions.Normal)', fontsize=12, fontweight='bold')\n",
    "axes[1,0].legend(fontsize=10)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# NumPy (manual)\n",
    "axes[1,1].plot(mu_range, likelihoods_np_manual, 'c-', linewidth=2)\n",
    "axes[1,1].axvline(mu_hat_np, color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'MLE mu={mu_hat_np:.1f}')\n",
    "axes[1,1].scatter(observations, [0]*len(observations), color='green', s=100,\n",
    "                  zorder=5, label='Observations')\n",
    "axes[1,1].set_xlabel('μ (mean)', fontsize=12)\n",
    "axes[1,1].set_ylabel('Likelihood L(μ)', fontsize=12)\n",
    "axes[1,1].set_title('NumPy (manual calculation)', fontsize=12, fontweight='bold')\n",
    "axes[1,1].legend(fontsize=10)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Gaussian Distribution MLE: σ={known_std}, Observations={observations}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Gaussian Distribution MLE - Exercise 2\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Observations: {observations}\")\n",
    "print(f\"Known std: sigma = {known_std}\")\n",
    "print(f\"\\nMLE Formula: mu = (1/n)*sum(xi) = (1/4)(5+7+9+11) = 8\")\n",
    "print(f\"\\nMLE Results:\")\n",
    "print(f\"  NumPy:      {mu_hat_np:.4f}\")\n",
    "print(f\"  PyTorch:    {mu_hat_torch.item():.4f}\")\n",
    "print(f\"  TensorFlow: {mu_hat_tf.numpy():.4f}\")\n",
    "print(f\"\\nLog-Likelihood at MLE:\")\n",
    "print(f\"  NumPy:      {log_likelihood_np:.4f}\")\n",
    "print(f\"  PyTorch:    {log_likelihood_torch.item():.4f}\")\n",
    "print(f\"  TensorFlow: {log_likelihood_tf.numpy():.4f}\")\n",
    "print(f\"\\nAll frameworks confirm: mu = 8.0\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. Cambridge University Press. Retrieved from https://d2l.ai\n",
    "   - Chapter 22.7: Maximum Likelihood Estimation\n",
    "   - Chapter 22.9: Naive Bayes\n",
    "\n",
    "[2] Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.\n",
    "   - Chapter 1: Introduction (pp. 21-24)\n",
    "   - Chapter 2: Probability Distributions (pp. 67-88)\n",
    "\n",
    "[3] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.\n",
    "   - Chapter 2: Probability (pp. 35-59)\n",
    "   - Chapter 9: Generative Models for Discrete Data (pp. 259-287)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codem irror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
