{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c0a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.11/site-packages (3.10.7)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (2.3.5)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./venv/lib/python3.11/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./venv/lib/python3.11/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.11/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.11/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19a6d6",
   "metadata": {},
   "source": [
    "# 22.8. Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecbb8a",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "In Computer Science, we rarely operate in a perfect, predictable world. Real-world data is noisy, hardware components have inherent failure rates, and user behavior fluctuates randomly. To build robust systems—whether it is a neural network recognizing images or a distributed database ensuring consistency—we cannot simply guess. We need probability distributions.\n",
    "\n",
    "Probability distributions provide the essential framework to quantify uncertainty. They allow us to model variables ranging from discrete binary choices (like a \"yes/no\" click) to continuous signals (like audio frequencies or pixel intensities). By understanding the underlying distribution of our data, we can calculate risks, optimize performance, and train algorithms to make accurate predictions even in the presence of noise.\n",
    "\n",
    "### Applications in Computer Science\n",
    "\n",
    "This document bridges the gap between statistical theory and code, exploring how fundamental distributions are applied to solve core computing challenges:\n",
    "\n",
    "- **Binary Classification (Spam Filter):** We utilize the Bernoulli distribution as the foundation for modeling binary outcomes. This is the core mechanism behind binary classifiers, such as spam detection algorithms that categorize emails as either \"Spam\" (1) or \"Not Spam\" (0).\n",
    "    \n",
    "- **Load Balancing:** We apply the Uniform distribution to algorithms requiring fairness and unpredictability. In distributed systems, random server selection ensures that incoming traffic is spread evenly across resources, preventing bottlenecks.\n",
    "    \n",
    "- **System Reliability (Server requests):** We use the Poisson distribution to model the rate at which events occur over a fixed period. This is critical for tasks like analyzing server log traffic, predicting request arrivals, or detecting anomalies like DDoS attacks where event frequency deviates from the norm.\n",
    "    \n",
    "- **Noise Modeling & Weight Initialization:** We examine the Gaussian (Normal) distribution, the most common distribution in nature. In Machine Learning, it is essential for modeling symmetric noise in data and is frequently used to initialize the weights of Neural Networks to ensure effective training dynamics.\n",
    "\n",
    "From the discrete simplicity of a Bernoulli trial to the continuous complexity of the Gaussian bell curve, and the rare-event modeling of the Poisson, this guide provides the mathematical definitions, step-by-step proofs, and practical Python implementations for each. Let us dive into the probability theories that power intelligent computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de0b41",
   "metadata": {},
   "source": [
    "## 2.1. Bernoulli Distribution\n",
    "\n",
    "### Definition \n",
    "\n",
    "The Bernoulli distribution models a random experiment that has only two possible outcomes: **Success** (denoted as 1) or **Failure** (denoted as 0).\n",
    "\n",
    "A classic example is tossing a coin: heads is 1, tails is 0.\n",
    "\n",
    "If a random variable $X$ follows this distribution with success probability $p$ (where $0 \\le p \\le 1$), we denote:\n",
    "\n",
    "$$X \\sim \\text{Bernoulli}(p)$$\n",
    "\n",
    "**Probability Mass Function (PMF):**\n",
    "\n",
    "The probability that $X$ takes the value $x$ is:\n",
    "\n",
    "$$P(X=x) = \\begin{cases} p & \\text{if } x = 1 \\\\ 1-p & \\text{if } x = 0 \\end{cases}$$\n",
    "\n",
    "\n",
    "\n",
    "**Cumulative Distribution Function (CDF):**\n",
    "\n",
    "$$F(x) = \\begin{cases}\n",
    "\n",
    "0 & x < 0 \\\\\n",
    "\n",
    "1-p & 0 \\le x < 1 \\\\\n",
    "\n",
    "1 & x \\ge 1\n",
    "\n",
    "\\end{cases}$$\n",
    "\n",
    "**Mean:**\n",
    "\n",
    "$$\\mu_X = p$$\n",
    "\n",
    "**Variance:**\n",
    "\n",
    "$$\\sigma_X^2 = p(1-p)$$\n",
    "\n",
    "### Step-by-step Computation \n",
    "\n",
    "**Probability Mass Function (PMF)**\n",
    "\n",
    "_Original Definition:_\n",
    "\n",
    "The Bernoulli random variable $X$ has only two outcomes:\n",
    "\n",
    "- $X=1$ (Success) with probability $p$.\n",
    "    \n",
    "- $X=0$ (Failure) with probability $1-p$.\n",
    "    \n",
    "\n",
    "So it can be seen that the p.m.f formula is correct.\n",
    "\n",
    "**Cumulative Distribution Function (CDF)**\n",
    "\n",
    "_Original Definition:_\n",
    "\n",
    "The cumulative distribution function is the probability that the random variable $X$ takes a value less than or equal to a certain level $x$:\n",
    "\n",
    "$$F(x) = P(X \\le x)$$\n",
    "\n",
    "_Formula to be proved:_\n",
    "\n",
    "$$F(x) = \\begin{cases} 0 & x < 0 \\\\ 1-p & 0 \\le x < 1 \\\\ 1 & x \\ge 1 \\end{cases}$$\n",
    "\n",
    "_Consider each interval:_\n",
    "\n",
    "- $x < 0$\n",
    "    \n",
    "    Random variable $X$ only takes values 0 or 1. There is no value of $X$ smaller than a negative number (e.g., $x = -0.5$). Therefore, the event $\\{X \\le x\\}$ is an impossible event. Thus: $F(x) = P(\\emptyset) = 0$\n",
    "    \n",
    "- $0 \\le x < 1$ (Example: $x = 0.5$)\n",
    "    \n",
    "    Among the possible values of $X$ ($0$ and $1$), only the value $0$ satisfies the condition of being less than or equal to $x$ (e.g., $\\le 0.5$). Thus $P(X \\le x) = P(X=0)$.\n",
    "    \n",
    "    According to the PMF section above, $P(X=0) = 1-p$. Thus: $F(x) = 1-p$\n",
    "    \n",
    "- $x \\ge 1$ (Example: $x = 1.5$)\n",
    "    \n",
    "    The values of $X$ are $0$ and $1$, which satisfy $\\le 1.5$. Thus $P(X \\le x) = P(X=0) + P(X=1)$.\n",
    "    \n",
    "    Thus: $F(x) = (1-p) + p = 1$\n",
    "    \n",
    "\n",
    "**Expectation ($\\mu_X$):**\n",
    "\n",
    "According to the definition of expectation for discrete variables:\n",
    "\n",
    "$$\\begin{aligned} \\mathbb{E}[X] &= \\sum x \\cdot P(X=x) \\\\ &= 1 \\cdot P(X=1) + 0 \\cdot P(X=0) \\\\ &= 1 \\cdot p + 0 \\cdot (1-p) \\quad \\text{(by p.m.f definition)} \\\\ &= p \\end{aligned}$$\n",
    "\n",
    "**Variance ($\\sigma_X^2$):**\n",
    "\n",
    "According to the definition of expectation: $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n",
    "\n",
    "We have the second moment:\n",
    "\n",
    "$$\\begin{aligned} \\mathbb{E}[X^2] &= \\sum x^2 \\cdot P(X=x) \\\\ &= 1^2 \\cdot P(X=1) + 0^2 \\cdot P(X=0) \\\\ &= 1 \\cdot p + 0 \\cdot (1-p) \\\\ &= p \\end{aligned}$$\n",
    "\n",
    "Therefore: $\\text{Var}(X) = p - p^2 = p(1 - p)$\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "In CS, Bernoulli appears everywhere in the form of **Binary Classification**.\n",
    "\n",
    "- **Example:** An email arrives in your inbox. It can be **Spam (1)** or **Not Spam (0)**.\n",
    "    \n",
    "- **Model:** An AI model (like Logistic Regression) will predict the probability $p$ that the email is Spam. If $p > 0.5$, we classify it as 1, otherwise 0.\n",
    "    \n",
    "- Each data label ($y$) in the training set of a binary classification problem is a Bernoulli random variable.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390234bb",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b1ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterblockman/bachkhoa/math-for-cs/venv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities (P): tensor([0.9500, 0.1000, 0.8000, 0.0500, 0.6000])\n",
      "Classification labels (y_hat): tensor([1., 0., 1., 0., 1.])\n",
      "Email 1: SPAM (p=0.95)\n",
      "Email 2: NORMAL (p=0.10)\n",
      "Email 3: SPAM (p=0.80)\n",
      "Email 4: NORMAL (p=0.05)\n",
      "Email 5: SPAM (p=0.60)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Suppose we have 5 incoming emails\n",
    "# This is the probability P(Spam) that AI predicts based on the content of each email\n",
    "# (Email 1 is very spam-like, Email 2 is very safe...)\n",
    "predicted_probs = torch.tensor([0.95, 0.10, 0.80, 0.05, 0.60])\n",
    "\n",
    "print(\"Predicted probabilities (P):\", predicted_probs)\n",
    "\n",
    "# Classification threshold, usually 0.5\n",
    "threshold = 0.5\n",
    "\n",
    "# DECISION (Classification)\n",
    "# If p > 0.5 then 1 (Spam), otherwise 0 (Normal)\n",
    "# .float() to convert True/False to 1.0/0.0\n",
    "predictions = (predicted_probs > threshold).float()\n",
    "\n",
    "print(\"Classification labels (y_hat):\", predictions)\n",
    "\n",
    "# Explain results\n",
    "for i, pred in enumerate(predictions):\n",
    "    label = \"SPAM\" if pred == 1 else \"NORMAL\"\n",
    "    print(f\"Email {i+1}: {label} (p={predicted_probs[i]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345dc642",
   "metadata": {},
   "source": [
    "## 2.2. Discrete Uniform Distribution\n",
    "\n",
    "### Definition\n",
    "\n",
    "This distribution is used to describe the case where we randomly select an integer from the set $\\{1, 2, ..., n\\}$ where every value is equally likely to occur (e.g., rolling a 6-sided die, so $n=6$).\n",
    "\n",
    "**Notation:** $X \\sim U(n)$.\n",
    "\n",
    "**Probability Mass Function (PMF):**\n",
    "\n",
    "Since there are $n$ values and equal probability, the probability of each value is $\\frac{1}{n}$.\n",
    "\n",
    "$$p_i = P(X=i) = \\frac{1}{n} \\quad \\text{for } i \\in \\{1, 2, ..., n\\}$$\n",
    "\n",
    "\n",
    "**Cumulative Distribution Function (CDF):**\n",
    "\n",
    "This function accumulates probability. For any integer value $k$ in the range $[1, n]$:\n",
    "\n",
    "$$F(k) = P(X \\le k) = \\sum_{i=1}^k P(X=i) = \\sum_{i=1}^k \\frac{1}{n} = \\frac{k}{n}$$\n",
    "\n",
    "**Mean (Expectation):**\n",
    "\n",
    "$$\\mu_X = \\frac{n+1}{2}$$\n",
    "\n",
    "**Variance:**\n",
    "\n",
    "$$\\sigma_X^2 = \\frac{n^2-1}{12}$$\n",
    "\n",
    "### Step-by-step Computation \n",
    "\n",
    "**Expectation ($\\mu_X$):**\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{x_i=1}^n x_i \\cdot P(X=x_i)$$\n",
    "\n",
    "Substituting $P(X=i) = \\frac{1}{n}$, we get:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{x_i=1}^n x_i \\cdot \\frac{1}{n} = \\frac{1}{n} \\sum_{x_i=1}^n x_i$$\n",
    "\n",
    "Applying the sum of natural numbers formula: $1+2+...+n = \\frac{n(n+1)}{2}$, we have:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\frac{1}{n} \\cdot \\frac{n(n+1)}{2} = \\frac{n+1}{2}$$\n",
    "\n",
    "**Variance ($\\sigma_X^2$):**\n",
    "\n",
    "According to the definition of variance: $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n",
    "\n",
    "We have the second moment ($\\mathbb{E}[X^2]$):\n",
    "\n",
    "$$\\begin{aligned} \\mathbb{E}[X^2] &= \\sum_{x_i=1}^n x_i^2 \\cdot P(X=x_i) \\\\[8pt] &= \\sum_{x_i=1}^n x_i^2 \\cdot \\frac{1}{n} \\\\[8pt] &= \\frac{1}{n} \\sum_{x_i=1}^n x_i^2 \\end{aligned}$$\n",
    "\n",
    "Applying the sum of squares formula: $1^2 + ... + n^2 = \\frac{n(n+1)(2n+1)}{6}$, we have:\n",
    "\n",
    "$$\\mathbb{E}[X^2] = \\frac{1}{n} \\cdot \\frac{n(n+1)(2n+1)}{6} = \\frac{(n+1)(2n+1)}{6}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\begin{aligned} \\text{Var}(X) &= \\frac{(n+1)(2n+1)}{6} - \\left( \\frac{n+1}{2} \\right)^2 \\\\[8pt] &= \\frac{(n+1)(2n+1)}{6} - \\frac{(n+1)^2}{4} \\\\[8pt] &= \\frac{2(n+1)(2n+1)}{12} - \\frac{3(n+1)^2}{12} \\\\[8pt] &= \\frac{n+1}{12} \\left[ 2(2n+1) - 3(n+1) \\right] \\\\[8pt] &= \\frac{n+1}{12} \\left[ 4n + 2 - 3n - 3 \\right] \\\\[8pt] &= \\frac{n+1}{12} [n - 1] \\\\[8pt] &= \\frac{n^2 - 1}{12} \\end{aligned}$$\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "### Load balancing\n",
    "Suppose you have $n$ servers to serve users. When a new request arrives, you need to decide which server to route it to for processing.\n",
    "\n",
    "The simplest but most effective strategy is **Random Selection**:\n",
    "\n",
    "- Randomly select a server from the list $\\{1, 2, ..., n\\}$.\n",
    "    \n",
    "- The probability of choosing each server is equal: $p = \\frac{1}{n}$.\n",
    "    \n",
    "\n",
    "This is exactly a random variable following the **Discrete Uniform** $U(n)$ distribution.\n",
    "\n",
    "- **Benefits:** Stateless, extremely fast, and mathematically (Law of Large Numbers), when the request volume is large enough, the load will be distributed evenly across the machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1789788",
   "metadata": {},
   "source": [
    "### Python Implementation \n",
    "\n",
    "We will simulate the situation:\n",
    "\n",
    "- The system has **5 Servers**.\n",
    "    \n",
    "- Receives **1,000,000 Requests**.\n",
    "    \n",
    "- We will use `torch.randint` to route requests according to a uniform distribution and check if the load is truly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc384e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Requests: 1000000\n",
      "------------------------------\n",
      "Server 1: 199904 requests (19.99%)\n",
      "Server 2: 199621 requests (19.96%)\n",
      "Server 3: 200390 requests (20.04%)\n",
      "Server 4: 199860 requests (19.99%)\n",
      "Server 5: 200225 requests (20.02%)\n",
      "------------------------------\n",
      "Difference between busiest and freest server: 769 requests\n",
      "Percentage deviation: 0.0769%\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "import torch\n",
    "\n",
    "# 1. System Configuration\n",
    "num_servers = 5\n",
    "num_requests = 1000000 # 1 million requests\n",
    "\n",
    "# 2. Load Balancer: Route requests\n",
    "# Randomly select a server index from 0 to 4 for each request\n",
    "# This is the Discrete Uniform U(0, 4) distribution\n",
    "# torch.randint generates integers in [low, high), so we set high=num_servers\n",
    "server_indices = torch.randint(0, num_servers, size=(num_requests,))\n",
    "\n",
    "# 3. Load statistics on each server\n",
    "# torch.bincount counts the occurrences of each index\n",
    "server_loads = torch.bincount(server_indices)\n",
    "\n",
    "print(f\"Total Requests: {num_requests}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i in range(num_servers):\n",
    "    load = server_loads[i].item()\n",
    "    percent = (load / num_requests) * 100\n",
    "    print(f\"Server {i+1}: {load} requests ({percent:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 4. Evaluate deviation\n",
    "# Ideal: Each server receives exactly 20% (200,000 requests)\n",
    "max_load = server_loads.max().item()\n",
    "min_load = server_loads.min().item()\n",
    "diff = max_load - min_load\n",
    "\n",
    "print(f\"Difference between busiest and freest server: {diff} requests\")\n",
    "print(f\"Percentage deviation: {diff / num_requests * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee430b",
   "metadata": {},
   "source": [
    "## 2.3. Continuous Uniform Distribution\n",
    "\n",
    "### Definition\n",
    "\n",
    "The continuous uniform distribution is a distribution where probability is equally likely for all outcomes of a continuous random variable.\n",
    "\n",
    "This distribution appears when we take the discrete uniform distribution (choosing 1 of $n$ numbers), increase $n$ to infinity, and stretch it to fit in an interval $[a, b]$. It represents choosing a random value arbitrarily in the interval $[a, b]$ with equal probability.\n",
    "\n",
    "**Notation:** $X \\sim U(a, b)$.\n",
    "\n",
    "**Probability Density Function (PDF):**\n",
    "\n",
    "Unlike the discrete case (PMF), for continuous variables we use \"density\". Since the probability of picking an exact specific point is 0, we only care about the probability falling into an _interval_.\n",
    "\n",
    "To make the total area under the graph equal to 1 (total probability = 1), the height of the rectangle on the segment $[a, b]$ must be $\\frac{1}{b-a}$.\n",
    "\n",
    "$$p(x) = \\begin{cases} \\frac{1}{b-a} & \\text{if } a \\le x \\le b \\\\ 0 & \\text{if } x < a \\text{ or } x > b \\end{cases}$$\n",
    "\n",
    "**Cumulative Distribution Function (CDF):**\n",
    "\n",
    "This is the area calculated from the left to point $x$.\n",
    "\n",
    "$$F(x) = \\begin{cases} 0 & \\text{for } x < a, \\\\ \\frac{x-a}{b-a} & \\text{for } a \\le x \\le b, \\\\ 1 & \\text{for } x > b. \\end{cases}$$\n",
    "\n",
    "**Expectation:**\n",
    "\n",
    "$$\\mu_X = \\frac{a+b}{2}$$\n",
    "\n",
    "**Variance:**\n",
    "\n",
    "$$\\sigma_X^2 = \\frac{(b-a)^2}{12}$$\n",
    "\n",
    "### Step-by-step Computation\n",
    "\n",
    "**Probability Density Function**\n",
    "\n",
    "The total probability of the entire sample space must be **1**. For a continuous variable, this means the area under the PDF curve must equal 1.\n",
    "\n",
    "$$\\int_{-\\infty}^{+\\infty} p(x) \\, dx = 1$$\n",
    "\n",
    "Since $p(x) = 0$ when $x$ is outside $[a, b]$, we only need to integrate within the interval $[a, b]$:\n",
    "\n",
    "$$\\begin{aligned} & \\int_{a}^{b} k \\, dx = 1 \\\\ \\Leftrightarrow \\quad & k \\cdot \\int_{a}^{b} 1 \\, dx = 1 \\\\ \\Leftrightarrow \\quad & k \\cdot [x]_{a}^{b} = 1 \\\\ \\Leftrightarrow \\quad & k \\cdot (b - a) = 1 \\\\ \\Leftrightarrow \\quad & k = \\frac{1}{b-a} \\end{aligned}$$\n",
    "\n",
    "Thus we have the formula: $p(x) = \\frac{1}{b-a}$ in the interval $[a, b]$.\n",
    "\n",
    "**Cumulative Distribution Function (CDF)**\n",
    "\n",
    "_Assumption (Probability Density Function - PDF):_\n",
    "\n",
    "The probability density function of the continuous uniform distribution $U(a, b)$ is:\n",
    "\n",
    "$$p(t) = \\begin{cases} \\frac{1}{b-a} & a \\le t \\le b \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "_CDF Definition:_\n",
    "\n",
    "The cumulative distribution function is the probability that the random variable $X$ takes a value less than or equal to $x$, calculated by the integral of the density function:\n",
    "\n",
    "$$F(x) = P(X \\le x) = \\int_{-\\infty}^{x} p(t) \\, dt$$\n",
    "\n",
    "_Formula to be proved:_\n",
    "\n",
    "$$F(x) = \\begin{cases} 0 & x < a \\\\ \\frac{x-a}{b-a} & a \\le x \\le b \\\\ 1 & x > b \\end{cases}$$\n",
    "\n",
    "_Consider each interval:_\n",
    "\n",
    "- $x < a$ (Left of the domain)\n",
    "    \n",
    "    When $x$ is completely to the left of the interval $[a, b]$, the density function $p(t)$ is always 0 on the segment $(-\\infty, x]$. Thus: $F(x) = \\int_{-\\infty}^{x} 0 \\, dt = 0$\n",
    "    \n",
    "- $a \\le x \\le b$ (Possible value range)\n",
    "    \n",
    "    We split the integration domain into 2 parts: the part with no probability $(-\\infty, a)$ and the part with uniform density $(a, x)$. $F(x) = \\int_{-\\infty}^{a} 0 \\, dt + \\int_{a}^{x} \\frac{1}{b-a} \\, dt$. Applying the antiderivative $\\int C \\, dt = Ct$:\n",
    "    \n",
    "    $$F(x) = 0 + \\left[ \\frac{t}{b-a} \\right]_{t=a}^{t=x}= \\frac{x}{b-a} - \\frac{a}{b-a} = \\frac{x-a}{b-a} $$\n",
    "    \n",
    "- $x > b$ (Right of the domain)\n",
    "    \n",
    "    We split the integration domain into 3 parts covering the entire possible value range $[a, b]$.\n",
    "    \n",
    "    $$F(x) = \\underbrace{\\int_{-\\infty}^{a} 0 , dt}{=0} + \\underbrace{\\int{a}^{b} \\frac{1}{b-a} , dt}{\\text{Total Probability}} + \\underbrace{\\int{b}^{x} 0 , dt}{=0} = \\left[ \\frac{t}{b-a} \\right]{a}^{b} = \\frac{b}{b-a} - \\frac{a}{b-a} = \\frac{b-a}{b-a} = 1 $$\n",
    "    \n",
    "\n",
    "**Expectation ($\\mu_X$):**\n",
    "\n",
    "According to the definition of expectation for continuous variables: $\\mathbb{E}[X] = \\int_{-\\infty}^{+\\infty} x \\cdot p(x) dx$.\n",
    "\n",
    "Since $p(x)$ is non-zero only in the interval $[a, b]$:\n",
    "\n",
    "$$\\begin{aligned} \\mathbb{E}[X] &= \\int_{a}^{b} x \\cdot \\frac{1}{b-a} \\, dx \\\\[8pt] &= \\frac{1}{b-a} \\int_{a}^{b} x \\, dx \\\\[8pt] &= \\frac{1}{b-a} \\left[ \\frac{x^2}{2} \\right]_{a}^{b} \\quad \\text{(Antiderivative of $x$ is $\\frac{x^2}{2}$)} \\\\[8pt] &= \\frac{1}{b-a} \\left( \\frac{b^2}{2} - \\frac{a^2}{2} \\right) \\\\[8pt] &= \\frac{1}{b-a} \\cdot \\frac{(b-a)(b+a)}{2} \\\\[8pt] &= \\frac{a+b}{2} \\end{aligned}$$\n",
    "\n",
    "**Variance ($\\sigma_X^2$):**\n",
    "\n",
    "According to the definition of variance: $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n",
    "\n",
    "We have the Second Moment ($\\mathbb{E}[X^2]$):\n",
    "\n",
    "$$\\begin{aligned} \\mathbb{E}[X^2] &= \\int_{-\\infty}^{+\\infty} x^2 \\cdot p(x) \\, dx \\\\[8pt] &= \\int_{a}^{b} x^2 \\cdot \\frac{1}{b-a} \\, dx \\\\[8pt] &= \\frac{1}{b-a} \\left[ \\frac{x^3}{3} \\right]_{a}^{b} \\quad \\text{(Antiderivative of $x^2$ is $\\frac{x^3}{3}$)} \\\\[8pt] &= \\frac{1}{b-a} \\cdot \\frac{b^3 - a^3}{3} \\\\[8pt] &= \\frac{1}{b-a} \\cdot \\frac{(b-a)(b^2 + ab + a^2)}{3} \\\\[8pt] &= \\frac{a^2 + ab + b^2}{3} \\end{aligned}$$\n",
    "\n",
    "Substituting into the variance formula:\n",
    "\n",
    "$$\\begin{aligned} \\text{Var}(X) &= \\frac{a^2 + ab + b^2}{3} - \\left( \\frac{a+b}{2} \\right)^2 \\\\[8pt] &= \\frac{4(a^2 + ab + b^2)}{12} - \\frac{3(a^2 + 2ab + b^2)}{12} \\\\[8pt] &= \\frac{4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2}{12} \\\\[8pt] &= \\frac{a^2 - 2ab + b^2}{12} \\\\[8pt] &= \\frac{(b-a)^2}{12} \\end{aligned}$$\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "Weight Initialization\n",
    "\n",
    "In Deep Learning (AI), the most important application of the continuous uniform distribution is Weight Initialization.\n",
    "\n",
    "**Concept: Symmetry Breaking**\n",
    "\n",
    "When building a Neural Network, there are millions of parameters (weights) that need to be learned.\n",
    "\n",
    "- **Problem:** If we initialize all weights to 0 (or equal values), all neurons will calculate the exact same result. During backpropagation, they will receive the same gradients and \"learn\" identically. The Neural Network becomes useless.\n",
    "    \n",
    "- **Solution:** We need to initialize weights randomly to \"break symmetry\".\n",
    "    \n",
    "- **Why Uniform?** Because initially, we **do not know** which weights should be large or small. The \"fairest\" way (most conservative/unbiased) is to assume every value in a small range $[-\\epsilon, \\epsilon]$ is equally likely to occur.\n",
    "    \n",
    "\n",
    "The famous **Xavier Initialization** (or Glorot Initialization) technique often initializes weights from the distribution:\n",
    "\n",
    "$$W \\sim U\\left(-\\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70490e05",
   "metadata": {},
   "source": [
    "### Python Implementation\n",
    "\n",
    "We will simulate the initialization of a Linear Layer and check if the weights are truly distributed evenly (\"flat\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506c21b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m nn.init.uniform_(layer.weight, a=a, b=b)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 3. Extract weight data for inspection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m weights = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitialization range: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mActual Min: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights.min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# 1. Simulate a Linear layer in a Neural Network\n",
    "# Input: 100 features, Output: 100 features -> Total 10,000 weights\n",
    "layer = nn.Linear(in_features=100, out_features=100)\n",
    "\n",
    "# 2. Initialize weights using Uniform distribution\n",
    "# Suppose we want weights in range [-0.5, 0.5]\n",
    "a, b = -0.5, 0.5\n",
    "nn.init.uniform_(layer.weight, a=a, b=b)\n",
    "\n",
    "# 3. Extract weight data for inspection\n",
    "weights = layer.weight.data.flatten().numpy()\n",
    "\n",
    "print(f\"Initialization range: [{a}, {b}]\")\n",
    "print(f\"Actual Min: {weights.min():.4f}\")\n",
    "print(f\"Actual Max: {weights.max():.4f}\")\n",
    "print(f\"Actual Mean (expected is 0): {weights.mean():.4f}\")\n",
    "\n",
    "# 4. Plot Histogram to see \"flatness\"\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(weights, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Distribution of Weights after Uniform Initialization\")\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.axvline(a, color='red', linestyle='dashed', linewidth=2, label='a (Lower bound)')\n",
    "plt.axvline(b, color='red', linestyle='dashed', linewidth=2, label='b (Upper bound)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c2629",
   "metadata": {},
   "source": [
    "## 2.4. Binomial Distribution\n",
    "\n",
    "### Definition\n",
    "\n",
    "is a discrete probability distribution with two parameters $n$ and $p$, denoting the number of successes in $n$ independent trials seeking a yes/no success result. (wikipedia)\n",
    "\n",
    "Imagine you don't just toss a coin once (Bernoulli), but toss $n$ times consecutively and independently. You count how many times heads appear.\n",
    "\n",
    "Let $X_i$ be the result of the $i$-th toss (1 if heads, 0 if tails), then $X_i \\sim \\text{Bernoulli}(p)$.\n",
    "\n",
    "The Binomial random variable $X$ is the total number of successes:\n",
    "\n",
    "$$X = \\sum_{i=1}^n X_i$$\n",
    "\n",
    "Notation: $X \\sim \\text{Binomial}(n, p)$.\n",
    "\n",
    "**Probability Mass Function (PMF):**\n",
    "\n",
    "The probability of having exactly $k$ successes in $n$ trials is:\n",
    "\n",
    "$$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "Where $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the number of ways to choose $k$ successes from $n$ trials.\n",
    "\n",
    "**Cumulative Distribution Function (CDF):**\n",
    "\n",
    "$$F(x) = \\begin{cases} 0 & \\text{if } x < 0 \\\\ \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1-p)^{n-k} & \\text{if } 0 \\le x < n \\\\ 1 & \\text{if } x \\ge n \\end{cases}$$\n",
    "\n",
    "Expectation:\n",
    "\n",
    "$$\\mu_X = np$$\n",
    "\n",
    "Variance:\n",
    "\n",
    "$$\\sigma_X^2 = np(1-p)$$\n",
    "\n",
    "### Step-by-step Computation\n",
    "\n",
    "**Probability Mass Function (PMF)**\n",
    "\n",
    "Need to prove:\n",
    "\n",
    "$$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "**Step 1: Probability of a specific scenario**\n",
    "\n",
    "Suppose you toss a coin $n$ times. You want exactly $k$ heads (Success - $S$) and $n-k$ tails (Failure - $F$).\n",
    "\n",
    "Imagine the simplest scenario: the first $k$ times are all heads, and all remaining $n-k$ times are tails.\n",
    "\n",
    "The sequence of results will look like this:\n",
    "\n",
    "$$\\underbrace{S, S, \\dots, S}_{k \\text{ times}}, \\underbrace{F, F, \\dots, F}_{n-k \\text{ times}}$$\n",
    "\n",
    "Since the tosses are independent (the result of this one doesn't affect the other), we multiply the probabilities of each toss together:\n",
    "\n",
    "- Probability of $S$ is $p$.\n",
    "    \n",
    "- Probability of $F$ is $(1-p)$.\n",
    "    \n",
    "\n",
    "So the probability of this specific sequence is:\n",
    "\n",
    "$$P(\\text{specific sequence}) = \\underbrace{p \\cdot p \\dots p}_{k \\text{ times}} \\cdot \\underbrace{(1-p) \\cdot (1-p) \\dots (1-p)}_{n-k \\text{ times}} = p^k (1-p)^{n-k}$$\n",
    "\n",
    "**Step 2: Counting the number of scenarios (Combinations)**\n",
    "\n",
    "The problem is that the $k$ heads don't necessarily have to be at the beginning. They can be scrambled anywhere in the sequence of $n$ trials.\n",
    "\n",
    "Example: $S, F, S, F \\dots$ or $F, F, S, S \\dots$\n",
    "\n",
    "The question becomes: How many ways are there to arrange $k$ letters $S$ into $n$ empty positions?\n",
    "\n",
    "This is the Combination problem in mathematics: Choose $k$ positions from $n$ positions (order of selection doesn't matter).\n",
    "\n",
    "The number of ways to choose is:\n",
    "\n",
    "$$\\binom{n}{k} = C_n^k = \\frac{n!}{k!(n-k)!}$$\n",
    "\n",
    "**Step 3: Putting it together**\n",
    "\n",
    "The overall probability $P(X=k)$ is the sum of probabilities of all possible scenarios.\n",
    "\n",
    "Since these scenarios are mutually exclusive (you can't roll sequence A and sequence B at the same time), and each scenario has the same probability of $p^k (1-p)^{n-k}$ (calculated in Step 1), we perform multiplication:\n",
    "\n",
    "$$\\text{Total Probability} = (\\text{Number of scenarios}) \\times (\\text{Probability of one scenario})$$\n",
    "\n",
    "$$P(X=k) = \\binom{n}{k} \\cdot p^k (1-p)^{n-k}$$\n",
    "\n",
    "**Cumulative Distribution Function (CDF)**\n",
    "\n",
    "Starting from the original definition of CDF:\n",
    "\n",
    "$$F(x) = P(X \\le x)$$\n",
    "\n",
    "The Binomial random variable $X$ only takes integer values belonging to the set $\\{0, 1, 2, \\dots, n\\}$.\n",
    "\n",
    "Formula to be proved:\n",
    "\n",
    "$$F(x) = \\begin{cases} 0 & \\text{if } x < 0 \\\\ \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1-p)^{n-k} & \\text{if } 0 \\le x < n \\\\ 1 & \\text{if } x \\ge n \\end{cases}$$\n",
    "\n",
    "We consider 3 cases of $x$ on the real number line:\n",
    "\n",
    "**Case 1: $x < 0$**\n",
    "\n",
    "Random variable $X$ counts the number of successes, so $X$ cannot be negative ($X \\ge 0$). Therefore, the event $\\{X \\le x\\}$ with $x < 0$ is an impossible event.\n",
    "\n",
    "$$F(x) = P(X \\le x) = 0$$\n",
    "\n",
    "**Case 2: $0 \\le x < n$**\n",
    "\n",
    "We need to calculate the sum of probabilities of all integer values $k$ that $X$ can take such that $k \\le x$. Since $k$ must be an integer, the condition $k \\le x$ is equivalent to $k \\le \\lfloor x \\rfloor$ (the largest integer not exceeding $x$).\n",
    "\n",
    "Example: If $x=2.7$, then the valid integer values are $0, 1, 2$. We see $\\lfloor 2.7 \\rfloor = 2$.\n",
    "\n",
    "Thus $F(x)$ is the sum of probabilities $P(X=k)$ running from $k=0$ to the upper limit $\\lfloor x \\rfloor$:\n",
    "\n",
    "$$F(x) = \\sum_{k=0}^{\\lfloor x \\rfloor} P(X=k)$$\n",
    "\n",
    "Substitute the Binomial PMF formula in:\n",
    "\n",
    "$$F(x) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "**Expectation ($\\mu_X$):**\n",
    "\n",
    "We have the definition $X$ is the sum of $X_i$:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\mathbb{E}\\left[ \\sum_{i=1}^n X_i \\right]$$\n",
    "\n",
    "Applying the linearity property of expectation (Expectation of a sum equals the sum of expectations), we have:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{i=1}^n \\mathbb{E}[X_i]$$\n",
    "\n",
    "Since each $X_i \\sim \\text{Bernoulli}(p)$ then $\\mathbb{E}[X_i] = p$ (as proved in the previous section). Substituting in, we get:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{i=1}^n p = n \\cdot p$$\n",
    "\n",
    "**Variance ($\\sigma_X^2$):**\n",
    "\n",
    "We have the definition $X$ is the sum of $X_i$:\n",
    "\n",
    "$$\\text{Var}(X) = \\text{Var}\\left( \\sum_{i=1}^n X_i \\right)$$\n",
    "\n",
    "Assuming the variables $X_i$ are independent of each other, applying the property that the variance of a sum equals the sum of variances:\n",
    "\n",
    "$$\\text{Var}(X) = \\sum_{i=1}^n \\text{Var}(X_i)$$\n",
    "\n",
    "Since each $X_i \\sim \\text{Bernoulli}(p)$ then $\\text{Var}(X_i) = p(1-p)$. Substituting in, we have:\n",
    "\n",
    "$$\\text{Var}(X) = \\sum_{i=1}^n p(1-p) = n \\cdot p(1-p)$$\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "### System Reliability\n",
    "\n",
    "You are operating a Data Center. You use a RAID 6 (Redundant Array of Independent Disks) storage system. This system consists of $n = 8$ hard drives.\n",
    "\n",
    "- According to technical specifications, the probability of a hard drive failing in 1 year is $p = 5\\%$ ($0.05$).\n",
    "    \n",
    "- The hard drives fail independently of each other.\n",
    "    \n",
    "- You want to calculate the risk probability to know whether to buy additional data insurance.\n",
    "    \n",
    "\n",
    "**Question:** What is the probability that exactly 2 hard drives fail together this year? ($k=2$).\n",
    "\n",
    "(Note: RAID 6 is still safe if 2 drives fail, but this is a red alert threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d2768",
   "metadata": {},
   "source": [
    "### Python Implementation\n",
    "\n",
    "This code segment simulates the calculation above using torch, and simultaneously illustrates calculating CDF (probability of failing at least how many drives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8f0d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Probability of having exactly 2 failed hard drives (PMF): 0.05146\n",
      "2. Probability system is still safe (failed <= 2 drives): 0.99421\n",
      "3. Risk of DATA LOSS (failed > 2 drives): 0.00579\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- PART 1: THEORETICAL CALCULATION ---\n",
    "# RAID problem configuration\n",
    "n_drives = 8 # Total number of hard drives\n",
    "p_fail = 0.05 # Failure probability of 1 drive (5%)\n",
    "\n",
    "# Create Binomial distribution\n",
    "# Note: PyTorch uses probs for p\n",
    "binom_dist = torch.distributions.binomial.Binomial(total_count=n_drives, probs=p_fail)\n",
    "\n",
    "# Calculate PMF at k=2 (Exactly 2 failed drives)\n",
    "k_fail = 2\n",
    "# log_prob returns logarithm of probability, so need exp() to get actual probability\n",
    "prob_exact_2 = torch.exp(binom_dist.log_prob(torch.tensor(float(k_fail))))\n",
    "\n",
    "print(f\"1. Probability of having exactly {k_fail} failed hard drives (PMF): {prob_exact_2.item():.5f}\")\n",
    "# Result should be approximately 0.05146 as calculated by hand\n",
    "\n",
    "# --- PART 2: REAL WORLD APPLICATION (CDF) ---\n",
    "# RAID 6 is safe if number of failed drives <= 2. System CRASHES (Data Loss) if > 2 drives fail (i.e. 3, 4... 8).\n",
    "# P(Crash) = P(X >= 3) = 1 - P(X <= 2) = 1 - CDF(2)\n",
    "\n",
    "# Calculate CDF manually by accumulating PMF from 0 to 2\n",
    "prob_safe = 0.0\n",
    "for k in range(3): # k = 0, 1, 2\n",
    "    prob_k = torch.exp(binom_dist.log_prob(torch.tensor(float(k))))\n",
    "    prob_safe += prob_k\n",
    "\n",
    "prob_crash = 1.0 - prob_safe\n",
    "\n",
    "print(f\"2. Probability system is still safe (failed <= 2 drives): {prob_safe.item():.5f}\")\n",
    "print(f\"3. Risk of DATA LOSS (failed > 2 drives): {prob_crash.item():.5f}\")\n",
    "# This result (Crash Risk) is an important figure to report to the boss!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d756a",
   "metadata": {},
   "source": [
    "## 2.5. Poisson Distribution\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Idea**\n",
    "\n",
    "Law of Rare Events. Imagine you are standing at a bus stop.\n",
    "\n",
    "If you divide 1 minute into 60 seconds. In each second, the probability of a bus arriving is very small (rare).\n",
    "\n",
    "However, if you accumulate a whole hour, surely a few buses will arrive.\n",
    "\n",
    "When we take the Binomial distribution $(n, p)$, keeping the expectation $\\lambda = np$ constant, but letting the number of trials $n \\to \\infty$ (dividing time infinitely small) and the probability of success $p \\to 0$ (event is very rare in that moment), we will obtain the Poisson distribution.\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The Poisson distribution is a discrete probability distribution used to describe the number of occurrences of a random event in a fixed interval of time or space, with the parameter λ representing the average number of times the event occurs.\n",
    "\n",
    "Notation: $X \\sim \\text{Poisson}(\\lambda)$.\n",
    "\n",
    "$\\lambda > 0$ is the average rate or the expected number of events occurring in a unit of time.\n",
    "\n",
    "**Probability Mass Function (PMF):**\n",
    "\n",
    "The probability that exactly $k$ events occur is:\n",
    "\n",
    "$$P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "\n",
    "(With $k \\in \\{0, 1, 2, ...\\}$)\n",
    "\n",
    "**Cumulative Distribution Function (CDF):**\n",
    "\n",
    "$$F(x) = \\begin{cases}\n",
    "\n",
    "0 & \\text{if } x < 0 \\\\\n",
    "\n",
    "e^{-\\lambda} \\sum_{k=0}^{\\lfloor x \\rfloor} \\frac{\\lambda^k}{k!} & \\text{if } x \\ge 0\n",
    "\n",
    "\\end{cases}$$\n",
    "\n",
    "Expectation:\n",
    "\n",
    "$$\\mu_X = \\lambda$$\n",
    "\n",
    "Variance:\n",
    "\n",
    "$$\\sigma_X^2 = \\lambda$$\n",
    "\n",
    "### Step-by-step Computation \n",
    "\n",
    "**Probability Mass Function (PMF):**\n",
    "\n",
    "Assumption:\n",
    "\n",
    "The probability of the event occurring follows the Binomial distribution $X \\sim \\text{Binomial}(n, p)$.\n",
    "\n",
    "We consider the limit when the number of trials $n \\to \\infty$, the probability of success each time $p \\to 0$, but the average expectation $\\lambda = n p$ is kept fixed (constant).\n",
    "\n",
    "Then $p = \\frac{\\lambda}{n}$.\n",
    "\n",
    "We need to prove $\\lim_{n \\to \\infty} P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$.\n",
    "\n",
    "Step 1: Write the Binomial PMF formula\n",
    "\n",
    "$$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "Substitute $p = \\frac{\\lambda}{n}$ into the formula:\n",
    "\n",
    "$$P(X=k) = \\frac{n!}{k!(n-k)!} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1 - \\frac{\\lambda}{n}\\right)^{n-k}$$\n",
    "\n",
    "Step 2: Expand factorial and power\n",
    "\n",
    "We know $\\frac{n!}{(n-k)!} = n(n-1)(n-2)\\dots(n-k+1)$.\n",
    "\n",
    "Separate the components in the expression:\n",
    "\n",
    "$$P(X=k) = \\frac{1}{k!} \\cdot \\underbrace{n(n-1)\\dots(n-k+1)}_{\\text{Factorial numerator}} \\cdot \\underbrace{\\frac{\\lambda^k}{n^k}}_{\\text{Separate } p^k} \\cdot \\underbrace{\\left(1 - \\frac{\\lambda}{n}\\right)^n \\left(1 - \\frac{\\lambda}{n}\\right)^{-k}}_{\\text{Separate } (1-p)^{n-k}}$$\n",
    "\n",
    "Step 3: Group variables dependent on $n$\n",
    "\n",
    "Rearrange to easily calculate the limit:\n",
    "\n",
    "$$P(X=k) = \\frac{\\lambda^k}{k!} \\cdot \\underbrace{\\frac{n(n-1)\\dots(n-k+1)}{n^k}}_{\\text{Group A}} \\cdot \\underbrace{\\left(1 - \\frac{\\lambda}{n}\\right)^n}_{\\text{Group B}} \\cdot \\underbrace{\\left(1 - \\frac{\\lambda}{n}\\right)^{-k}}_{\\text{Group C}}$$\n",
    "\n",
    "Step 4: Calculate the limit of each group as $n \\to \\infty$\n",
    "\n",
    "Group A: Divide $n^k$ (consisting of $k$ numbers of $n$) for each term in the numerator:\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\frac{n}{n} \\cdot \\frac{n-1}{n} \\cdots \\frac{n-k+1}{n} = 1 \\cdot (1-0) \\cdots (1-0) = 1$$\n",
    "\n",
    "Group B: This is the basic limit definition of the exponential function:\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\left(1 - \\frac{\\lambda}{n}\\right)^n = e^{-\\lambda}$$\n",
    "\n",
    "Group C: When $n \\to \\infty$ then $\\frac{\\lambda}{n} \\to 0$:\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\left(1 - 0\\right)^{-k} = 1$$\n",
    "\n",
    "Step 5: Conclusion\n",
    "\n",
    "Multiply the results together:\n",
    "\n",
    "$$P(X=k) = \\frac{\\lambda^k}{k!} \\cdot 1 \\cdot e^{-\\lambda} \\cdot 1$$\n",
    "\n",
    "$$\\Rightarrow P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\quad \\text{(Required to be proved)}$$\n",
    "\n",
    "**Cumulative Distribution Function (CDF):**\n",
    "\n",
    "Need to prove:\n",
    "\n",
    "$$F(x) = \\begin{cases} 0 & \\text{if } x \\< 0 e^{-\\lambda} \\sum\\_{k=0}^{\\lfloor x \\rfloor} \\frac{\\lambda^k}{k\\!} & \\text{if } x \\ge 0 \\end{cases}$$\n",
    "\n",
    "We start from the original definition of the cumulative distribution function:\n",
    "\n",
    "$$F(x) = P(X \\le x)$$\n",
    "\n",
    "The Poisson random variable $X$ only takes non-negative integer values: $\\{0, 1, 2, \\dots\\}$.\n",
    "\n",
    "Case 1: $x < 0$\n",
    "\n",
    "Since $X$ always takes non-negative values ($X \\ge 0$), the event $\\{X \\le x\\}$ with $x < 0$ is impossible.\n",
    "\n",
    "Therefore: $F(x) = 0$.\n",
    "\n",
    "Case 2: $x \\ge 0$\n",
    "\n",
    "To calculate $P(X \\le x)$, we need to sum the probabilities of all integer values $k$ that variable $X$ can take, provided $k$ does not exceed $x$.\n",
    "\n",
    "The condition \"$k$ is an integer and $k \\le x$\" is equivalent to \"$k$ is an integer and $k \\le \\lfloor x \\rfloor$\".\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "$$F(x) = \\sum_{k=0}^{\\lfloor x \\rfloor} P(X=k)$$\n",
    "\n",
    "Substitute the Poisson probability mass function (PMF) formula: $P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$.\n",
    "\n",
    "$$F(x) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "\n",
    "Since $e^{-\\lambda}$ is constant with respect to the running index $k$, we can take it out of the sum (distributive property of multiplication):\n",
    "\n",
    "$$F(x) = e^{-\\lambda} \\sum_{k=0}^{\\lfloor x \\rfloor} \\frac{\\lambda^k}{k!}$$\n",
    "\n",
    "**Expectation ($\\mu_X$):**\n",
    "\n",
    "According to the definition of expectation we have:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{k=0}^{\\infty} k \\cdot P(X=k)$$\n",
    "\n",
    "Substituting the Poisson probability mass function (PMF) formula $P(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}$, we get:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{k=0}^{\\infty} k \\cdot \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "\n",
    "At $k=0$, the first term equals $0$, so we can ignore it and start the sum from $k=1$:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{k=1}^{\\infty} k \\cdot \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "\n",
    "Proceed to expand the factorial $k! = k \\cdot (k-1)!$, simplify $k$ in the numerator and denominator, and simultaneously move the constant $e^{-\\lambda}$ outside the sum:\n",
    "\n",
    "$$\\mathbb{E}[X] = e^{-\\lambda} \\sum_{k=1}^{\\infty} \\frac{k \\cdot \\lambda^k}{k(k-1)!} = e^{-\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!}$$\n",
    "\n",
    "Separate one $\\lambda$ out so that $\\lambda^{k-1}$ remains inside the sum:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\lambda e^{-\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}$$\n",
    "\n",
    "Set the auxiliary variable $j = k - 1$. When $k=1$ then $j=0$. When $k \\to \\infty$ then $j \\to \\infty$. The sum in the brackets is the Taylor expansion of the function $e^\\lambda$:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\lambda e^{-\\lambda} \\underbrace{\\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!}}_{e^\\lambda}$$\n",
    "\n",
    "Finally, replace the sum with $e^\\lambda$ and simplify (since $e^{-\\lambda} \\cdot e^{\\lambda} = 1$):\n",
    "\n",
    "$$\\mathbb{E}[X] = \\lambda e^{-\\lambda} \\cdot e^{\\lambda} = \\lambda$$\n",
    "\n",
    "**Variance ($\\sigma_X^2$):**\n",
    "\n",
    "We have the formula for variance:\n",
    "\n",
    "$$\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
    "\n",
    "We already know the expectation $\\mathbb{E}[X] = \\lambda$. So the key is to calculate the 2nd Moment ($\\mathbb{E}[X^2]$).\n",
    "\n",
    "Using the identity $k^2 = k(k-1) + k$, we analyze:\n",
    "\n",
    "$$\\mathbb{E}[X^2] = \\mathbb{E}[X(X-1) + X] = \\mathbb{E}[X(X-1)] + \\mathbb{E}[X]$$\n",
    "\n",
    "$$\\mathbb{E}[X^2] = \\mathbb{E}[X(X-1)] + \\lambda$$\n",
    "\n",
    "Now we calculate the expectation of the factorial $\\mathbb{E}[X(X-1)]$ according to the definition (LOTUS):\n",
    "\n",
    "$$\\mathbb{E}[X(X-1)] = \\sum_{k=0}^{\\infty} k(k-1) \\cdot \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "\n",
    "At $k=0$ and $k=1$, the product $k(k-1) = 0$ so we ignore it and start the sum from $k=2$. Simplify the factorial $\\frac{k(k-1)}{k!} = \\frac{1}{(k-2)!}$, we have:\n",
    "\n",
    "$$\\mathbb{E}[X(X-1)] = e^{-\\lambda} \\sum_{k=2}^{\\infty} \\frac{\\lambda^k}{(k-2)!}$$\n",
    "\n",
    "Set the auxiliary variable $j = k - 2$. Then $\\lambda^k = \\lambda^{j+2} = \\lambda^2 \\cdot \\lambda^j$. Bring $\\lambda^2$ outside:\n",
    "\n",
    "$$\\mathbb{E}[X(X-1)] = \\lambda^2 e^{-\\lambda} \\underbrace{\\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!}}_{e^\\lambda}$$\n",
    "\n",
    "The sum in the brackets is the Taylor expansion of the function $e^\\lambda$. Substituting in we get:\n",
    "\n",
    "$$\\mathbb{E}[X(X-1)] = \\lambda^2 e^{-\\lambda} \\cdot e^{\\lambda} = \\lambda^2$$\n",
    "\n",
    "Implies the 2nd moment is:\n",
    "\n",
    "$$\\mathbb{E}[X^2] = \\lambda^2 + \\lambda$$\n",
    "\n",
    "Finally, substitute into the initial variance formula:\n",
    "\n",
    "$$\\text{Var}(X) = (\\lambda^2 + \\lambda) - \\lambda^2 = \\lambda$$\n",
    "\n",
    "Thus with the Poisson distribution, both Expectation and Variance equal $\\lambda$.\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "**DDoS Detection (Denial of Service)**\n",
    "\n",
    "In cybersecurity, normal traffic to a website often follows a Poisson distribution.\n",
    "\n",
    "Users access independently of each other.\n",
    "\n",
    "Average access speed ($\\lambda$) is usually stable within a certain time frame.\n",
    "\n",
    "If the number of requests suddenly spikes far beyond the allowable threshold of the Poisson distribution (lying in the extreme tail of the graph), the system will flag it as an anomaly or a potential DDoS attack.\n",
    "\n",
    "Concrete Example:\n",
    "\n",
    "You manage a Login Server.\n",
    "\n",
    "Normal: Average $\\lambda = 5$ failed logins in 1 minute (due to users forgetting passwords).\n",
    "\n",
    "Event: In the past minute, the system recorded 12 failed logins.\n",
    "\n",
    "Question: What is the probability of this happening randomly? If the probability is too low (< 1%), we conclude this is a Brute-force attack and block the IP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56ae91",
   "metadata": {},
   "source": [
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8ab125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (Normal): 5.0 fails/minute\n",
      "Observed (Observed): 12 fails/minute\n",
      "P-value (Random probability): 0.005453\n",
      ">>> WARNING: Brute-force attack detected! (P-value < 1%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# 1. System configuration\n",
    "# Average rate of failed logins/minute\n",
    "lambda_normal = 5.0\n",
    "\n",
    "# Number of failures observed in the current minute\n",
    "observed_failures = 12\n",
    "\n",
    "# 2. Build Poisson model\n",
    "# This is the \"Baseline\" of the normal system\n",
    "poisson_model = torch.distributions.poisson.Poisson(rate=lambda_normal)\n",
    "\n",
    "# 3. Calculate Anomaly Score\n",
    "# We calculate P-value: Probability that X >= observed_failures happens randomly\n",
    "# P(X >= k) = 1 - P(X <= k-1) = 1 - CDF(k-1)\n",
    "\n",
    "# Calculate CDF at k-1 = 11\n",
    "cdf_val = 0.0\n",
    "for k in range(observed_failures): # runs from 0 to 11\n",
    "    # log_prob returns logarithm, need exp to get probability\n",
    "    prob_k = torch.exp(poisson_model.log_prob(torch.tensor(float(k))))\n",
    "    cdf_val += prob_k\n",
    "\n",
    "p_value = 1.0 - cdf_val\n",
    "\n",
    "# 4. Decision Making\n",
    "# Confidence threshold alpha = 0.01 (1%)\n",
    "alpha = 0.01\n",
    "\n",
    "print(f\"Average (Normal): {lambda_normal} fails/minute\")\n",
    "print(f\"Observed (Observed): {observed_failures} fails/minute\")\n",
    "print(f\"P-value (Random probability): {p_value.item():.6f}\")\n",
    "\n",
    "# This graph helps visualize the tail of the distribution\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\">>> WARNING: Brute-force attack detected! (P-value < 1%)\")\n",
    "else:\n",
    "    print(\">>> NORMAL: Could be due to users forgetting passwords.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e170a8d",
   "metadata": {},
   "source": [
    "## 2.6. Gaussian Distribution\n",
    "\n",
    "### Definition\n",
    "\n",
    "A random variable $X$ follows a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, denoted $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, having the probability density function (PDF):\n",
    "\n",
    "$$p_{X}(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}$$\n",
    "\n",
    "$\\mu$ (Mean): Determines the center position of the \"bell\".\n",
    "\n",
    "$\\sigma^2$ (Variance): Determines the width of the distribution.\n",
    "\n",
    "**Cumulative Distribution Function (CDF)**\n",
    "\n",
    "The document emphasizes that the Gaussian CDF does not have a closed-form using elementary functions. To calculate, we must use the error function erf:\n",
    "\n",
    "$$F(x) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x - \\mu}{\\sigma\\sqrt{2}} \\right) \\right]$$\n",
    "\n",
    "### Step-by-step Computation (this is math proof)\n",
    "\n",
    "**Origin: The Central Limit Theorem**\n",
    "\n",
    "Unlike Poisson (where we let $n \\to \\infty$ and $p \\to 0$), the Gaussian distribution appears from another limit problem of the Binomial distribution.\n",
    "\n",
    "Suppose we have a sum $X^{(n)}$ of $n$ independent Bernoulli random variables with fixed probability $p$. When $n \\to \\infty$, both the expectation ($\\mu = np$) and variance ($\\sigma^2 = np(1-p)$) approach infinity, causing the distribution to \"flatten\" out and become hard to define.\n",
    "\n",
    "To solve this, we standardize this random variable:\n",
    "\n",
    "$$Y^{(n)} = \\frac{X^{(n)} - \\mu_{X^{(n)}}}{\\sigma_{X^{(n)}}}$$\n",
    "\n",
    "This variable $Y^{(n)}$ has a mean of 0 and variance of 1.\n",
    "\n",
    "The Central Limit Theorem (CLT) states that when $n \\to \\infty$, the distribution of $Y^{(n)}$ will converge to the standard Gaussian distribution:\n",
    "\n",
    "$$\\lim_{n\\rightarrow\\infty}P(Y^{(n)}\\in[a,b])=P(\\mathcal{N}(0,1)\\in[a,b])$$\n",
    "\n",
    "This is the reason Gaussian is the foundation of Machine Learning: Whenever we measure the sum of many small independent contributions, the result will approximate a Gaussian.\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "**Maximum Entropy Property**\n",
    "\n",
    "This is the most important philosophical property of Gaussian mentioned in the document. Gaussian is the distribution with Maximum Entropy among distributions with the same mean and variance.\n",
    "\n",
    "Significance: It is the \"most random\" and \"most conservative\" choice.\n",
    "\n",
    "Application: If we only know the data has a fixed Mean and Variance without knowing anything else, assuming it is Gaussian is safest because we do not impose any additional hidden structures (like symmetry or boundary limits) on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf837761",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "046340b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian N(0.0, 1.0^2) at x = 0.0\n",
      "PDF (Theory ~ 0.3989): 0.3989\n",
      "CDF (Theory = 0.5):    0.5000\n",
      "\n",
      "Random samples:\n",
      "tensor([[-0.3814, -0.0732,  0.0805,  0.3312, -0.0094],\n",
      "        [ 1.2661, -0.9991, -0.8708,  0.2856,  1.1419]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# 1. Parameter configuration\n",
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "\n",
    "# 2. Probability Density Function (PDF) - Formula A.17\n",
    "def gaussian_pdf(x, mu, sigma):\n",
    "    # Normalization coefficient: 1 / sqrt(2 * pi * sigma^2)\n",
    "    coefficient = 1.0 / math.sqrt(2 * math.pi * sigma**2)\n",
    "    # Exponent part: -(x - mu)^2 / (2 * sigma^2)\n",
    "    exponent = torch.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    return coefficient * exponent\n",
    "\n",
    "# 3. Cumulative Distribution Function (CDF) - Using erf function\n",
    "# Book uses integral approximation formula using erf\n",
    "def gaussian_cdf(x, mu, sigma):\n",
    "    # Formula: 0.5 * (1 + erf((x - mu) / (sigma * sqrt(2))))\n",
    "    return 0.5 * (1 + torch.erf((x - mu) / (sigma * math.sqrt(2))))\n",
    "\n",
    "# --- Illustration ---\n",
    "x_val = torch.tensor(0.0) # At the bell peak\n",
    "\n",
    "pdf_val = gaussian_pdf(x_val, mu, sigma)\n",
    "cdf_val = gaussian_cdf(x_val, mu, sigma)\n",
    "\n",
    "print(f\"Gaussian N({mu}, {sigma}^2) at x = {x_val}\")\n",
    "print(f\"PDF (Theory ~ 0.3989): {pdf_val:.4f}\")\n",
    "print(f\"CDF (Theory = 0.5):    {cdf_val:.4f}\")\n",
    "\n",
    "# Random sampling as guided by the book\n",
    "samples = torch.normal(mean=mu, std=sigma, size=(2, 5))\n",
    "print(\"\\nRandom samples:\")\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55e548",
   "metadata": {},
   "source": [
    "## 2.6. Exponential Family Distribution\n",
    "\n",
    "### Definition\n",
    "\n",
    "A probability distribution belongs to the Exponential Family if its probability density function (PDF) or probability mass function (PMF) can be represented in the following canonical form:\n",
    "\n",
    "$$p(x|\\boldsymbol{\\eta}) = h(x) \\cdot \\exp\\left( \\boldsymbol{\\eta}^\\top \\cdot T(x) - A(\\boldsymbol{\\eta}) \\right)$$\n",
    "\n",
    "Where the components are defined in detail as follows:\n",
    "\n",
    "$\\boldsymbol{\\eta}$ (Natural Parameters): Also called canonical parameters. This is a vector $\\boldsymbol{\\eta} = (\\eta_1, ..., \\eta_l) \\in \\mathbb{R}^l$ that determines the shape of the distribution.\n",
    "\n",
    "$T(x)$ (Sufficient Statistics): This is a function of the data $T(x) = (T_1(x), ..., T_l(x))$. It is called \"sufficient\" because the information contained in $T(x)$ is sufficient to calculate the probability density without retaining the entire original data $x$.\n",
    "\n",
    "$h(x)$ (Underlying Measure): It represents the underlying structure of the data before being affected by the exponential parameters.\n",
    "\n",
    "$A(\\boldsymbol{\\eta})$ (Cumulant Function): Cumulative function (or Log-partition function). Its role is to ensure the integral (or sum) of the probability density function equals 1 (distribution normalization). Its formula is:\n",
    "\n",
    "$$A(\\boldsymbol{\\eta}) = \\log \\left[ \\int h(x) \\cdot \\exp(\\boldsymbol{\\eta}^\\top \\cdot T(x)) \\, dx \\right]$$\n",
    "\n",
    "### Step-by-step Computation \n",
    "\n",
    "Gaussian Analysis\n",
    "\n",
    "To demonstrate the power of this theory, we will transform the Univariate Gaussian distribution into the canonical form of the exponential family, adhering closely to the transformation steps in the document.\n",
    "\n",
    "Step 1: Starting from the standard Gaussian PDF\n",
    "\n",
    "$$p(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right\\}$$\n",
    "\n",
    "Step 2: Expanding the identity in the exponent\n",
    "\n",
    "We expand $-(x-\\mu)^2 = -(x^2 - 2\\mu x + \\mu^2) = -x^2 + 2\\mu x - \\mu^2$.\n",
    "\n",
    "Substitute into the exponential expression and separate the elements:\n",
    "\n",
    "$$p(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sigma} \\cdot \\exp\\left\\{ \\frac{2\\mu x - x^2 - \\mu^2}{2\\sigma^2} \\right\\}$$\n",
    "\n",
    "Logarithmic transformation to bring $\\frac{1}{\\sigma}$ into the exponential function (note $\\frac{1}{\\sigma} = \\exp(-\\log \\sigma)$):\n",
    "\n",
    "$$p(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}} \\cdot \\exp\\left\\{ \\frac{\\mu}{\\sigma^2}x - \\frac{1}{2\\sigma^2}x^2 - \\left( \\frac{\\mu^2}{2\\sigma^2} + \\log(\\sigma) \\right) \\right\\}$$\n",
    "\n",
    "Step 3: Mapping to the canonical form\n",
    "\n",
    "Based on the equation above, we identify each corresponding component:\n",
    "\n",
    "Underlying measure $h(x)$:\n",
    "\n",
    "$$h(x) = \\frac{1}{\\sqrt{2\\pi}}$$\n",
    "\n",
    "Natural parameters $\\boldsymbol{\\eta}$: Is a 2-dimensional vector:\n",
    "\n",
    "$$\\boldsymbol{\\eta} = \\begin{bmatrix} \\eta_1 \\\\ \\eta_2 \\end{bmatrix} = \\begin{bmatrix} \\frac{\\mu}{\\sigma^2} \\\\ \\frac{1}{2\\sigma^2} \\end{bmatrix}$$\n",
    "\n",
    "Sufficient statistics $T(x)$: (Note how the document handles the negative sign)\n",
    "\n",
    "$$T(x) = \\begin{bmatrix} x \\\\ -x^2 \\end{bmatrix}$$\n",
    "\n",
    "(Check: $\\boldsymbol{\\eta}^\\top T(x) = \\frac{\\mu}{\\sigma^2}x + \\frac{1}{2\\sigma^2}(-x^2)$, matching the expansion in Step 2).\n",
    "\n",
    "Cumulant function $A(\\boldsymbol{\\eta})$:\n",
    "\n",
    "$$A(\\boldsymbol{\\eta}) = \\frac{\\mu^2}{2\\sigma^2} + \\log(\\sigma) = \\frac{\\eta_1^2}{4\\eta_2} - \\frac{1}{2}\\log(2\\eta_2)$$\n",
    "\n",
    "\n",
    "## Significance in Computer Science\n",
    "\n",
    "Reducing to the Exponential Family allows us to build generalized machine learning models (Generalized Linear Models - GLM). Instead of having to write separate optimization algorithms for each type of distribution, we can build a general theoretical framework to train for binary data (Bernoulli), count data (Poisson), and real data (Gaussian).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
